# LLM高级能力测评套件 - 项目总览

## 📋 项目概述

本项目是一个全面的LLM高级能力测评框架，旨在评估大语言模型在多轮复杂交互中的高级能力，特别是其作为"认知引擎"的潜力。核心关注点在于模型处理超长上下文、任务分解与重组、状态追踪、主动澄清以及工具协同的能力。

## 🏗️ 项目结构

```
testLLM/
├── core/                  # 核心框架
├── tests/                 # 测试用例
├── docs/                  # 文档
├── config/                # 配置文件
├── independence/          # 角色独立性测试模块
├── cognitive_ecosystem/   # 认知生态系统模块
├── tools/                 # 工具脚本
└── utils.py               # 工具函数
```

## 🧪 测试体系

### 测试分层结构

#### 基础能力层 (Pillars 1-8)
- **逻辑推理**: 数学计算、逻辑判断
- **指令遵循**: 精确执行复杂指令
- **结构化操作**: JSON、XML等格式化输出
- **长上下文**: 超长文本理解和处理
- **领域知识**: 专业领域知识应用
- **工具使用**: 工具调用和API集成
- **任务规划**: 多步骤任务分解和执行
- **元认知**: 自我反思和错误修正

#### 高级能力层 (Pillars 9-19)
- **创意生成**: 创新思维和创意解决方案
- **角色扮演**: 深度角色扮演和身份一致性
- **多角色协作**: 多角色对话管理和协作
- **任务图谱生成**: 复杂任务分解和依赖关系分析
- **项目管理**: 复杂项目规划和执行
- **并行任务优化**: 并行任务调度和优化
- **多学科分解**: 跨学科知识融合和应用

#### 前沿能力层 (Pillars 20-24)
- **海量角色共识**: 50-200个角色的大规模协作
- **动态角色切换**: 角色间的动态切换和状态维护
- **项目管理集成**: 复杂项目的状态图建模
- **并行任务优化**: 复杂依赖关系的分析
- **多学科分解**: 6-7个学科的知识融合

## 🔧 核心模块

### 1. 角色独立性测试系统

评估LLM在多轮对话中保持角色身份一致性的能力，包含三大核心实验：

#### E1: 破功测试 (Breaking Stress Test)
- 通过压力测试检测角色信念体系的稳定性
- 测量模型在极端条件下的角色保持能力

#### E2: 隐式认知测量 (Implicit Cognition Test)
- 通过隐式提示探测模型的深层认知结构
- 分析模型在无明确指令下的行为模式

#### E3: 纵向演化分析 (Longitudinal Consistency Test)
- 追踪模型在长时间对话中的角色演化轨迹
- 评估角色记忆和状态的持续性

### 2. 认知生态系统

模拟多智能体协作的认知生态系统，包含：

- **角色生成**: 生成具有不同背景、观点、专业领域的角色
- **共识算法**: 实现PoW、PoS、PBFT等区块链共识算法
- **协同编辑**: 多角色协作编辑和冲突解决
- **状态管理**: 大规模角色的状态追踪和管理

## 🚀 快速开始

### 1. 环境准备
```bash
# 安装依赖
pip install ollama

# 启动Ollama服务
ollama serve

# 下载测试模型
ollama pull qwen2:7b
```

### 2. 配置模型
编辑 `config.py` 文件：
```python
MODEL_TO_TEST = 'qwen2:7b'  # 主要测试模型
```

### 3. 运行测试
```bash
# 运行所有高级能力测试 (Pillar 9-19)
python run_advanced_capability_tests.py

# 运行前沿能力测试 (Pillar 20-24)
python run_massive_consensus_test.py

# 运行角色独立性测试
python run_pillar_25_independence.py
```

### 4. 查看结果
```bash
# 查看测试输出
ls testout/

# 运行分析
python analyze_results.py
```

## 📊 评估标准

### 定量指标
- **成功率**: 测试完成的成功比例
- **响应时间**: 平均响应延迟
- **资源消耗**: 内存和CPU使用情况

### 定性指标
- **创新性**: 解决方案的创新程度
- **一致性**: 角色身份和认知的稳定性
- **专业性**: 专业领域的知识深度
- **实用性**: 解决方案的实际应用价值

## 🔮 未来发展

### 短期计划
- 完善基础能力测试 (Pillar 1-8)
- 优化测试性能和资源利用率
- 增强结果分析和可视化功能

### 中期计划
- 支持多模态输入（图像、音频）
- 实现自适应测试难度调整
- 扩展云服务支持

### 长期计划
- 构建开放的测试社区
- 实现自动化测试生成
- 开发企业级测试平台
