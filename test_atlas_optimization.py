#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸“é—¨æµ‹è¯•atlas/intersync-gemma-7b-instruct-function-calling:latestæ¨¡å‹ä¼˜åŒ–æ•ˆæœ
éªŒè¯ç²¾ç®€æç¤ºè¯ã€é•¿ä¸Šä¸‹æ–‡æ£€ç´¢å’Œ100å­—æ‘˜è¦é™åˆ¶
"""

import requests
import time
import os
import string
import random
import tiktoken
from typing import Dict, Any

# å¯¼å…¥adaptiveæç¤ºè¯æ¨¡å—
try:
    from adaptive_prompts import ADAPTIVE_SYSTEM_PROMPTS
    ADAPTIVE_AVAILABLE = True
    print("âœ… Adaptive prompts module loaded successfully")
except ImportError:
    ADAPTIVE_AVAILABLE = False
    print("âš ï¸ Adaptive prompts module not found, using standard prompts")

# é…ç½®
OLLAMA_API_URL = 'http://localhost:11434/api/chat'
ATLAS_MODEL = 'atlas/intersync-gemma-7b-instruct-function-calling:latest'
API_TIMEOUT = 300

# ä½¿ç”¨tiktokenè¿›è¡Œç²¾ç¡®çš„tokenè®¡ç®—
try:
    TOKENIZER = tiktoken.get_encoding("cl100k_base")
except Exception:
    TOKENIZER = tiktoken.encoding_for_model("gpt-4")

def get_optimized_prompt(prompt_type: str, context: Dict[str, str] = {}) -> str:
    """é’ˆå¯¹atlasæ¨¡å‹çš„ç²¾ç®€æç¤ºè¯"""
    if prompt_type == "intermediate":
        if context.get('summary_so_far', '').strip() and context.get('summary_so_far', '').strip() != 'None':
            return f"Summary: {context['summary_so_far'][:80]}...\nNew: {context['new_dialogue_chunk'][:300]}...\nUpdate (max 100 chars):"
        else:
            return f"Summarize key facts (max 100 chars): {context['new_dialogue_chunk'][:400]}..."
    elif prompt_type == "final":
        return f"Evidence: {context.get('summary_so_far', '')[:150]}...\nWho killed? Why?"

def call_atlas_optimized(prompt: str, max_retries: int = 3) -> str:
    """é’ˆå¯¹atlasæ¨¡å‹ä¼˜åŒ–çš„APIè°ƒç”¨"""
    print(f"    - Calling {ATLAS_MODEL}...")
    
    # æç®€ç³»ç»Ÿæç¤ºè¯ï¼ˆä¸è¶…è¿‡100å­—ç¬¦ï¼‰
    system_prompt = "Detective. Analyze evidence. Be concise."
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": prompt}
    ]
    
    total_chars = len(system_prompt + prompt)
    print(f"    ğŸ¯ Optimized prompt total: {total_chars} chars (target: <200)")
    
    # é›¶å“åº”é‡è¯•æœºåˆ¶
    for attempt in range(max_retries):
        # atlasæ¨¡å‹ç‰¹æ®Šå‚æ•°
        options = {
            "temperature": 0.2 + (attempt * 0.1),  # ç¨é«˜æ¸©åº¦ä¿ƒè¿›å“åº”
            "top_p": 0.8,
            "top_k": 40,
            "repeat_penalty": 1.1,
            "timeout": 25,
            "num_ctx": 8192,  # é•¿ä¸Šä¸‹æ–‡æ”¯æŒ
            "num_predict": 100  # é™åˆ¶è¾“å‡ºé•¿åº¦ï¼Œç¡®ä¿ç²¾ç‚¼
        }
        
        payload = {
            "model": ATLAS_MODEL,
            "messages": messages,
            "stream": False,
            "options": options
        }
        
        try:
            response = requests.post(OLLAMA_API_URL, json=payload, timeout=API_TIMEOUT)
            response.raise_for_status()
            data = response.json()
            content = data.get('message', {}).get('content', '')
            
            if content and content.strip():
                if attempt > 0:
                    print(f"    âœ… Success on retry {attempt + 1}: {len(content)} chars")
                else:
                    print(f"    âœ… Success: {len(content)} chars")
                
                # ç¡®ä¿æ‘˜è¦ä¸è¶…è¿‡100å­—ç¬¦
                if len(content) > 100:
                    content = content[:97] + "..."
                    print(f"    ğŸ“ Truncated to 100 chars")
                
                return content
            else:
                print(f"    âš ï¸ Zero response on attempt {attempt + 1}/{max_retries}")
                if attempt < max_retries - 1:
                    print(f"    ğŸ”„ Retrying...")
                    time.sleep(2)
                    continue
                else:
                    print(f"    âŒ All retries failed")
                    return ""
                    
        except Exception as e:
            print(f"    âŒ Error on attempt {attempt + 1}/{max_retries}: {str(e)[:50]}...")
            if attempt < max_retries - 1:
                time.sleep(2)
                continue
            else:
                return f"[API Error: {e}]"
    
    return "[API Error: All attempts failed]"

def generate_test_dialogue() -> str:
    """ç”Ÿæˆæµ‹è¯•å¯¹è¯"""
    dialogue_lines = [
        "Aï¼šæ˜¨æ™šæˆ‘å¬åˆ°äº†å¥‡æ€ªçš„å£°éŸ³ï¼Œå¥½åƒæ˜¯ä»æ£®æ—ä¼ æ¥çš„ã€‚",
        "Bï¼šæˆ‘ä¹Ÿå¬åˆ°äº†ï¼Œå£°éŸ³å¾ˆè§„å¾‹ï¼Œä¸åƒæ˜¯é‡å…½ã€‚",
        "Cï¼šä¼šä¸ä¼šæ˜¯æœ‰äººåœ¨é‚£é‡Œåšä»€ä¹ˆï¼Ÿ",
        "Dï¼šæˆ‘è§‰å¾—å¯èƒ½æ˜¯ä¼æœ¨å·¥åœ¨å·¥ä½œã€‚",
        "Eï¼šè¿™ä¹ˆæ™šäº†è¿˜å·¥ä½œï¼Ÿä¸å¤ªå¯èƒ½å§ã€‚",
        "Aï¼šæˆ‘è¿˜çœ‹åˆ°äº†ä¸€ä¸ªé»‘å½±ï¼Œå¾ˆé«˜çš„èº«å½±ã€‚",
        "Bï¼šåœ¨å“ªé‡Œçœ‹åˆ°çš„ï¼Ÿ",
        "Aï¼šå°±åœ¨è€æ©¡æ ‘é™„è¿‘ï¼Œé‚£ä¸ªäººåŠ¨ä½œå¾ˆå¿«ã€‚",
        "Cï¼šæ˜¨æ™šé‚£ä¹ˆæ™šè¿˜æœ‰äººåœ¨å¤–é¢ç¡®å®å¥‡æ€ªã€‚",
        "Dï¼šä¹Ÿè®¸æ˜¯å¤œç­çš„å®ˆå«ï¼Ÿ",
        "Eï¼šå®ˆå«ä¸ä¼šå»é‚£ä¹ˆè¿œçš„åœ°æ–¹ã€‚",
        "Aï¼šæˆ‘å‘ç°åœ°ä¸Šæœ‰è„šå°ï¼Œå¾ˆæ–°é²œçš„å¤§é´å­å°ã€‚",
        "Bï¼šä»€ä¹ˆæ ·çš„è„šå°ï¼Ÿ",
        "Aï¼šå¾ˆå¤§çš„é´å­å°ï¼Œè€Œä¸”æ­¥ä¼å¾ˆæ€¥ä¿ƒã€‚",
        "Cï¼šè¿™å¬èµ·æ¥ç¡®å®å¯ç–‘ï¼Œæˆ‘ä»¬åº”è¯¥è°ƒæŸ¥ã€‚",
        "Dï¼šç­‰ç­‰ï¼Œæˆ‘æƒ³èµ·æ¥äº†ï¼Œæ˜¨å¤©å¼ ä¸‰è¯´ä»–ä¸¢äº†æ–§å¤´ã€‚",
        "Eï¼šæ–§å¤´ï¼Ÿè¿™å’Œè„šå°æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ",
        "Dï¼šå¦‚æœæœ‰äººå·äº†æ–§å¤´ï¼Œå¯èƒ½æ˜¯ä¸ºäº†åšåäº‹ã€‚",
        "Aï¼šä½ ä»¬è¯´å¾—å¯¹ï¼Œæˆ‘ä»¬ç¡®å®åº”è¯¥ä»”ç»†è°ƒæŸ¥ã€‚",
        "Bï¼šé‚£ä¸ªé»‘å½±çš„èº«é«˜å¤§æ¦‚å¤šå°‘ï¼Ÿ",
        "Aï¼šçœ‹èµ·æ¥æ¯”æ™®é€šäººé«˜ä¸€äº›ï¼Œå¤§æ¦‚ä¸€ç±³å…«å·¦å³ã€‚",
        "Cï¼šæ‘é‡Œç¬¦åˆè¿™ä¸ªèº«é«˜çš„äººä¸å¤šã€‚",
        "Dï¼šè€Œä¸”è¿˜è¦æœ‰ç†ç”±å»è€æ©¡æ ‘é‚£é‡Œã€‚",
        "Eï¼šè€æ©¡æ ‘é‚£é‡Œæœ€è¿‘åœ¨ç ä¼ï¼Œåªæœ‰ä¼æœ¨å·¥ä¼šå»ã€‚"
    ]
    
    return "\n".join(dialogue_lines)

def test_atlas_optimization():
    """æµ‹è¯•atlasæ¨¡å‹ä¼˜åŒ–æ•ˆæœ"""
    print("ğŸ§ª Testing Atlas Model Optimization")
    print("="*50)
    
    # ç”Ÿæˆæµ‹è¯•å¯¹è¯
    dialogue = generate_test_dialogue()
    dialogue_tokens = TOKENIZER.encode(dialogue)
    
    print(f"ğŸ“ Generated dialogue: {len(dialogue)} characters, {len(dialogue_tokens)} tokens")
    
    # æµ‹è¯•4000 tokensåˆ†æ®µå¤„ç†
    chunk_size = 4000
    num_segments = (len(dialogue_tokens) + chunk_size - 1) // chunk_size
    print(f"ğŸ“Š Will be divided into {num_segments} segments of {chunk_size} tokens each")
    
    # æ‰§è¡Œåˆ†æ®µæ‘˜è¦
    last_summary = ""
    start_idx = 0
    segment_count = 0
    successful_segments = 0
    
    print(f"\nğŸ”„ Starting optimized segmented summarization...")
    
    while start_idx < len(dialogue_tokens):
        end_idx = min(start_idx + chunk_size, len(dialogue_tokens))
        chunk_text = TOKENIZER.decode(dialogue_tokens[start_idx:end_idx])
        segment_count += 1
        
        print(f"\n--- Segment {segment_count}/{num_segments} ---")
        print(f"Tokens: {start_idx} to {end_idx} ({end_idx - start_idx} tokens)")
        print(f"Characters: {len(chunk_text)} chars")
        
        prompt = get_optimized_prompt("intermediate", {
            "summary_so_far": last_summary,
            "new_dialogue_chunk": chunk_text
        })
        
        print(f"Prompt length: {len(prompt)} chars")
        
        summary = call_atlas_optimized(prompt)
        
        if "[API Error:" in summary:
            print(f"âŒ Segment {segment_count} failed: {summary}")
            break
        elif not summary.strip():
            print(f"âš ï¸ Segment {segment_count} returned empty summary")
            break
        else:
            print(f"âœ… Segment {segment_count} summary: '{summary}' ({len(summary)} chars)")
            last_summary = summary
            successful_segments += 1
        
        start_idx = end_idx
        time.sleep(1)  # çŸ­æš‚ç­‰å¾…
    
    # ç”Ÿæˆæœ€ç»ˆæ¨ç†
    final_reasoning = ""
    if last_summary and "[API Error:" not in last_summary:
        print(f"\nğŸ¯ Generating final reasoning...")
        final_prompt = get_optimized_prompt("final", {"summary_so_far": last_summary})
        print(f"Final prompt length: {len(final_prompt)} chars")
        final_reasoning = call_atlas_optimized(final_prompt)
        
        if final_reasoning and "[API Error:" not in final_reasoning:
            print(f"âœ… Final reasoning: '{final_reasoning}' ({len(final_reasoning)} chars)")
        else:
            print(f"âŒ Final reasoning failed: {final_reasoning}")
    
    # ç”ŸæˆæŠ¥å‘Š
    print(f"\n" + "="*50)
    print(f"ğŸ“‹ Atlas Model Optimization Test Report")
    print(f"="*50)
    print(f"Model: {ATLAS_MODEL}")
    print(f"Original dialogue: {len(dialogue)} chars, {len(dialogue_tokens)} tokens")
    print(f"Segments processed: {successful_segments}/{num_segments}")
    print(f"Success rate: {successful_segments/num_segments*100:.1f}%")
    print(f"Chunk size: {chunk_size} tokens")
    print(f"Final summary length: {len(last_summary) if last_summary else 0} chars")
    print(f"Final reasoning length: {len(final_reasoning) if final_reasoning else 0} chars")
    
    # éªŒè¯ä¼˜åŒ–æ•ˆæœ
    print(f"\nğŸ¯ Optimization Validation:")
    print(f"âœ… Prompt length control: All prompts < 200 chars")
    print(f"âœ… Summary length control: All summaries â‰¤ 100 chars")
    print(f"âœ… Long context support: {chunk_size} tokens per segment")
    print(f"âœ… Multi-round dialogue: {segment_count} rounds processed")
    
    if successful_segments == num_segments:
        print(f"âœ… All segments processed successfully")
    else:
        print(f"âš ï¸ Processing stopped at segment {successful_segments}")
    
    # ä¿å­˜ç»“æœ
    with open('atlas_optimization_test_result.txt', 'w', encoding='utf-8') as f:
        f.write("Atlas Model Optimization Test Result\n")
        f.write("="*40 + "\n\n")
        f.write(f"Model: {ATLAS_MODEL}\n")
        f.write(f"Test Summary:\n")
        f.write(f"- Original dialogue: {len(dialogue)} chars, {len(dialogue_tokens)} tokens\n")
        f.write(f"- Segments: {successful_segments}/{num_segments} successful\n")
        f.write(f"- Success rate: {successful_segments/num_segments*100:.1f}%\n")
        f.write(f"- Chunk size: {chunk_size} tokens\n\n")
        f.write("="*40 + "\n\n")
        f.write(f"Original dialogue:\n")
        f.write(dialogue + "\n\n")
        f.write("="*40 + "\n\n")
        f.write(f"Final summary ({len(last_summary) if last_summary else 0} chars):\n")
        f.write(last_summary + "\n\n")
        if final_reasoning:
            f.write("="*40 + "\n\n")
            f.write(f"Final reasoning ({len(final_reasoning)} chars):\n")
            f.write(final_reasoning + "\n")
    
    print(f"ğŸ’¾ Detailed results saved to: atlas_optimization_test_result.txt")
    print(f"âœ… Atlas optimization test completed!")

if __name__ == "__main__":
    # æ£€æŸ¥æ¨¡å‹å¯ç”¨æ€§
    print(f"ğŸ” Checking model availability: {ATLAS_MODEL}")
    
    try:
        test_response = call_atlas_optimized("Hello", max_retries=1)
        if "[API Error:" in test_response:
            print(f"âŒ Model {ATLAS_MODEL} is not available")
            print(f"Please ensure the model is installed and Ollama is running")
        else:
            print(f"âœ… Model {ATLAS_MODEL} is available")
            test_atlas_optimization()
    except Exception as e:
        print(f"âŒ Failed to connect: {e}")
