#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Complete Web LLM Testing System
å®Œæ•´çš„Web LLMæµ‹è¯•ç³»ç»Ÿ - æ”¯æŒçœŸå®æ¨¡å‹é€‰æ‹©ã€æ‰¹é‡æµ‹è¯•ã€ç»“æœä¸‹è½½
"""

import streamlit as st
import subprocess
import sys
import os
import json
import time
import pandas as pd
from pathlib import Path
from datetime import datetime
import threading
import queue
import asyncio

# Add project root to path
sys.path.append(str(Path(__file__).parent))
sys.path.append(str(Path(__file__).parent / "scripts" / "utils"))

try:
    import cloud_services
    CLOUD_SERVICES_AVAILABLE = True
except:
    CLOUD_SERVICES_AVAILABLE = False

# å¯¼å…¥å¢å¼ºçš„æµ‹è¯•æ‰§è¡Œå™¨
try:
    from enhanced_test_executor import TestExecutor
    ENHANCED_EXECUTOR_AVAILABLE = True
except:
    ENHANCED_EXECUTOR_AVAILABLE = False

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="LLM Complete Testing System",
    page_icon="ğŸ§ª",
    layout="wide",
    initial_sidebar_state="expanded"
)

# åˆå§‹åŒ–session state
def init_session_state():
    """åˆå§‹åŒ–session state"""
    if 'testing_running' not in st.session_state:
        st.session_state.testing_running = False
    if 'test_results' not in st.session_state:
        st.session_state.test_results = []
    if 'current_test_index' not in st.session_state:
        st.session_state.current_test_index = 0
    if 'test_queue' not in st.session_state:
        st.session_state.test_queue = queue.Queue()
    if 'progress_data' not in st.session_state:
        st.session_state.progress_data = {}

# è·å–å¯ç”¨æ¨¡å‹
def get_available_models():
    """è·å–æ‰€æœ‰å¯ç”¨çš„çœŸå®æ¨¡å‹"""
    if not CLOUD_SERVICES_AVAILABLE:
        return []
    
    try:
        models = cloud_services.get_all_models()
        return models
    except:
        return []

# è·å–æµ‹è¯•æ–‡ä»¶åˆ—è¡¨
def get_test_files():
    """è·å–æ‰€æœ‰æµ‹è¯•æ–‡ä»¶"""
    if ENHANCED_EXECUTOR_AVAILABLE:
        executor = TestExecutor()
        return executor.get_test_files()
    else:
        # é™çº§åˆ°åŸºæœ¬ç‰ˆæœ¬
        test_files = []
        tests_dir = Path("tests")
        
        if tests_dir.exists():
            for file in tests_dir.glob("test_pillar_*.py"):
                # æå–pillarç¼–å·
                if file.name.startswith("test_pillar_25"):
                    pillar = 25
                else:
                    try:
                        pillar_num = int(file.name.split("_")[2].split(".")[0])
                        pillar = pillar_num
                    except:
                        continue
                
                # åˆ†ç±»æµ‹è¯•ç±»å‹
                if 1 <= pillar <= 8:
                    category = "åŸºç¡€èƒ½åŠ›"
                elif 9 <= pillar <= 19:
                    category = "é«˜çº§èƒ½åŠ›"
                elif 20 <= pillar <= 24:
                    category = "å‰æ²¿èƒ½åŠ›"
                elif pillar == 25:
                    category = "ä¸“é¡¹æµ‹è¯•"
                else:
                    category = "å…¶ä»–"
                
                test_files.append({
                    "file": file.name,
                    "pillar": pillar,
                    "category": category,
                    "path": str(file),
                    "title": file.name,
                    "description": "æµ‹è¯•æ–‡ä»¶"
                })
        
        return sorted(test_files, key=lambda x: x["pillar"])

# æ‰§è¡Œå•ä¸ªæµ‹è¯•
def run_single_test(test_info, model_key):
    """æ‰§è¡Œå•ä¸ªæµ‹è¯•"""
    if ENHANCED_EXECUTOR_AVAILABLE:
        executor = TestExecutor()
        return executor.execute_test(test_info, model_key)
    else:
        # é™çº§åˆ°åŸºæœ¬ç‰ˆæœ¬
        try:
            # è§£ææ¨¡å‹ä¿¡æ¯
            if '-' in model_key:
                service, model = model_key.split('-', 1)
            else:
                service = model_key
                model = model_key
            
            # ä½¿ç”¨æå–çš„promptæˆ–é»˜è®¤prompt
            prompt = test_info.get("prompt", f"è¯·å®Œæˆç¬¬{test_info['pillar']}é¡¹èƒ½åŠ›æµ‹è¯•")
            
            # è°ƒç”¨çœŸå®LLM
            response = cloud_services.call_cloud_service(service, model, prompt)
            
            return {
                "test_file": test_info["file"],
                "test_title": test_info.get("title", test_info["file"]),
                "pillar": test_info["pillar"],
                "category": test_info["category"],
                "model": model_key,
                "service": service,
                "prompt": prompt,
                "response": response,
                "timestamp": datetime.now().isoformat(),
                "status": "completed",
                "response_length": len(response) if response else 0
            }
            
        except Exception as e:
            return {
                "test_file": test_info["file"],
                "test_title": test_info.get("title", test_info["file"]),
                "pillar": test_info["pillar"],
                "category": test_info["category"],
                "model": model_key,
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
                "status": "failed"
            }

# ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
def generate_test_report(results):
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
    if not results:
        return None
    
    if ENHANCED_EXECUTOR_AVAILABLE:
        executor = TestExecutor()
        executor.results = results
        return executor.generate_comprehensive_report()
    else:
        # é™çº§åˆ°åŸºæœ¬ç‰ˆæœ¬
        # ç»Ÿè®¡ä¿¡æ¯
        total_tests = len(results)
        successful_tests = len([r for r in results if r["status"] == "completed"])
        failed_tests = total_tests - successful_tests
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        category_stats = {}
        for result in results:
            category = result["category"]
            if category not in category_stats:
                category_stats[category] = {"total": 0, "success": 0}
            category_stats[category]["total"] += 1
            if result["status"] == "completed":
                category_stats[category]["success"] += 1
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            "test_summary": {
                "total_tests": total_tests,
                "successful_tests": successful_tests,
                "failed_tests": failed_tests,
                "success_rate": (successful_tests / total_tests * 100) if total_tests > 0 else 0,
                "test_date": datetime.now().isoformat()
            },
            "category_statistics": category_stats,
            "detailed_results": results
        }
        
        return report

# ä¸»ç•Œé¢
def main():
    init_session_state()
    
    st.title("ğŸ§ª LLM Complete Testing System")
    st.markdown("---")
    
    # ä¾§è¾¹æ  - é…ç½®
    with st.sidebar:
        st.header("âš™ï¸ æµ‹è¯•é…ç½®")
        
        # æ¨¡å‹é€‰æ‹©
        st.subheader("1. é€‰æ‹©LLMæ¨¡å‹")
        available_models = get_available_models()
        
        if available_models:
            model_options = [f"{m['model']} ({m['service']})" for m in available_models]
            model_keys = [m['key'] for m in available_models]
            
            selected_model_index = st.selectbox(
                "é€‰æ‹©æ¨¡å‹:",
                range(len(model_options)),
                format_func=lambda x: model_options[x]
            )
            selected_model = model_keys[selected_model_index]
            
            st.info(f"å·²é€‰æ‹©: {model_options[selected_model_index]}")
        else:
            st.error("æ— æ³•åŠ è½½æ¨¡å‹åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥é…ç½®")
            selected_model = None
        
        # æµ‹è¯•é€‰æ‹©
        st.subheader("2. é€‰æ‹©æµ‹è¯•")
        test_files = get_test_files()
        
        if test_files:
            # æŒ‰ç±»åˆ«åˆ†ç»„
            categories = {}
            for test in test_files:
                cat = test["category"]
                if cat not in categories:
                    categories[cat] = []
                categories[cat].append(test)
            
            selected_tests = []
            for category, tests in categories.items():
                with st.expander(f"{category} ({len(tests)} ä¸ªæµ‹è¯•)"):
                    select_all = st.checkbox(f"å…¨é€‰ {category}", key=f"select_all_{category}")
                    
                    for test in tests:
                        test_key = f"{test['pillar']}_{test['file']}"
                        selected = st.checkbox(
                            f"Pillar {test['pillar']}: {test['file']}",
                            value=select_all,
                            key=test_key
                        )
                        if selected:
                            selected_tests.append(test)
            
            st.info(f"å·²é€‰æ‹© {len(selected_tests)} ä¸ªæµ‹è¯•")
        else:
            st.error("æ— æ³•åŠ è½½æµ‹è¯•æ–‡ä»¶")
            selected_tests = []
        
        # æµ‹è¯•è®¾ç½®
        st.subheader("3. æµ‹è¯•è®¾ç½®")
        test_mode = st.radio(
            "æµ‹è¯•æ¨¡å¼:",
            ["å•ä¸ªæ‰§è¡Œ", "æ‰¹é‡æ‰§è¡Œ", "è„šæœ¬æ¨¡å¼"]
        )
        
        batch_size = st.slider(
            "å¹¶å‘æ•°é‡:",
            min_value=1,
            max_value=10,
            value=3,
            help="åŒæ—¶æ‰§è¡Œçš„æµ‹è¯•æ•°é‡"
        )
        
        # å¼€å§‹æµ‹è¯•æŒ‰é’®
        start_button = st.button(
            "ğŸš€ å¼€å§‹æµ‹è¯•",
            disabled=st.session_state.testing_running or not selected_model or not selected_tests,
            type="primary"
        )
    
    # ä¸»ç•Œé¢å†…å®¹
    if start_button and selected_model and selected_tests:
        st.session_state.testing_running = True
        st.session_state.test_results = []
        st.session_state.current_test_index = 0
        
        # æ˜¾ç¤ºæµ‹è¯•è¿›åº¦
        progress_container = st.container()
        with progress_container:
            st.subheader("ğŸ“Š æµ‹è¯•è¿›åº¦")
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            results_container = st.container()
            
        # æ‰§è¡Œæµ‹è¯•
        total_tests = len(selected_tests)
        
        for i, test_info in enumerate(selected_tests):
            if not st.session_state.testing_running:
                break
            
            # æ›´æ–°è¿›åº¦
            progress = (i + 1) / total_tests
            progress_bar.progress(progress)
            status_text.text(f"æ­£åœ¨æ‰§è¡Œæµ‹è¯• {i+1}/{total_tests}: {test_info['file']}")
            
            # æ‰§è¡Œæµ‹è¯•
            with st.spinner(f"æ‰§è¡Œ {test_info['file']}..."):
                result = run_single_test(test_info, selected_model)
                st.session_state.test_results.append(result)
                
                # æ˜¾ç¤ºå®æ—¶ç»“æœ
                with results_container:
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric(f"æµ‹è¯• {i+1}", test_info['file'])
                    with col2:
                        status_icon = "âœ…" if result["status"] == "completed" else "âŒ"
                        st.markdown(f"**çŠ¶æ€**: {status_icon} {result['status']}")
                    with col3:
                        if result["status"] == "completed":
                            st.metric("å“åº”é•¿åº¦", f"{result['response_length']} å­—ç¬¦")
                
                # çŸ­æš‚å»¶è¿Ÿé¿å…APIé™åˆ¶
                time.sleep(1)
        
        # æµ‹è¯•å®Œæˆ
        st.session_state.testing_running = False
        progress_bar.progress(1.0)
        status_text.text("ğŸ‰ æµ‹è¯•å®Œæˆï¼")
        
        # æ˜¾ç¤ºç»“æœæ‘˜è¦
        st.subheader("ğŸ“ˆ æµ‹è¯•ç»“æœæ‘˜è¦")
        
        if st.session_state.test_results:
            # ç”ŸæˆæŠ¥å‘Š
            report = generate_test_report(st.session_state.test_results)
            
            if report:
                # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("æ€»æµ‹è¯•æ•°", report["test_summary"]["total_tests"])
                with col2:
                    st.metric("æˆåŠŸæ•°", report["test_summary"]["successful_tests"])
                with col3:
                    st.metric("å¤±è´¥æ•°", report["test_summary"]["failed_tests"])
                with col4:
                    st.metric("æˆåŠŸç‡", f"{report['test_summary']['success_rate']:.1f}%")
                
                # æŒ‰ç±»åˆ«æ˜¾ç¤ºç»“æœ
                st.subheader("ğŸ“Š åˆ†ç±»ç»Ÿè®¡")
                for category, stats in report["category_statistics"].items():
                    col1, col2 = st.columns(2)
                    with col1:
                        st.write(f"**{category}**")
                    with col2:
                        success_rate = (stats["success"] / stats["total"] * 100) if stats["total"] > 0 else 0
                        st.write(f"{stats['success']}/{stats['total']} ({success_rate:.1f}%)")
                
                # è¯¦ç»†ç»“æœè¡¨æ ¼
                st.subheader("ğŸ“‹ è¯¦ç»†ç»“æœ")
                
                # å‡†å¤‡è¡¨æ ¼æ•°æ®
                table_data = []
                for result in st.session_state.test_results:
                    table_data.append({
                        "æµ‹è¯•æ–‡ä»¶": result["test_file"],
                        "ç±»åˆ«": result["category"],
                        "çŠ¶æ€": result["status"],
                        "å“åº”é•¿åº¦": result.get("response_length", 0) if result["status"] == "completed" else 0,
                        "æ—¶é—´": result["timestamp"][:19]
                    })
                
                df = pd.DataFrame(table_data)
                st.dataframe(df, use_container_width=True)
                
                # æ˜¾ç¤ºè¯¦ç»†å“åº”
                st.subheader("ğŸ” è¯¦ç»†å“åº”")
                
                selected_result_index = st.selectbox(
                    "é€‰æ‹©æµ‹è¯•æŸ¥çœ‹è¯¦ç»†ç»“æœ:",
                    range(len(st.session_state.test_results)),
                    format_func=lambda x: f"{st.session_state.test_results[x]['test_file']}"
                )
                
                if selected_result_index is not None:
                    result = st.session_state.test_results[selected_result_index]
                    
                    st.markdown(f"**æµ‹è¯•æ–‡ä»¶**: {result['test_file']}")
                    st.markdown(f"**æ¨¡å‹**: {result['model']}")
                    st.markdown(f"**çŠ¶æ€**: {result['status']}")
                    
                    if result["status"] == "completed":
                        with st.expander("æŸ¥çœ‹Prompt"):
                            st.text_area("Prompt", result["prompt"], height=100)
                        
                        with st.expander("æŸ¥çœ‹å“åº”"):
                            st.text_area("Model Response", result["response"], height=200)
                    else:
                        st.error(f"é”™è¯¯: {result.get('error', 'Unknown error')}")
                
                # ä¸‹è½½æŠ¥å‘Š
                st.subheader("ğŸ’¾ ä¸‹è½½æµ‹è¯•æŠ¥å‘Š")
                
                # JSONæ ¼å¼æŠ¥å‘Š
                json_report = json.dumps(report, indent=2, ensure_ascii=False)
                st.download_button(
                    label="ä¸‹è½½JSONæŠ¥å‘Š",
                    data=json_report,
                    file_name=f"llm_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                    mime="application/json"
                )
                
                # CSVæ ¼å¼æŠ¥å‘Š
                csv_data = pd.DataFrame(table_data).to_csv(index=False)
                st.download_button(
                    label="ä¸‹è½½CSVæŠ¥å‘Š",
                    data=csv_data,
                    file_name=f"llm_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                    mime="text/csv"
                )
                
                # è„šæœ¬æ¨¡å¼æ”¯æŒ
                if test_mode == "è„šæœ¬æ¨¡å¼":
                    st.subheader("ğŸ“œ æ‰¹é‡æµ‹è¯•è„šæœ¬")
                    
                    script_content = f'''#!/usr/bin/env python3
# è‡ªåŠ¨ç”Ÿæˆçš„æ‰¹é‡æµ‹è¯•è„šæœ¬
# ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

import sys
import os
sys.path.append(os.path.dirname(__file__))

from scripts.utils.cloud_services import call_cloud_service

# æµ‹è¯•é…ç½®
MODEL_SERVICE = "{selected_model.split('-')[0]}"
MODEL_NAME = "{selected_model.split('-')[1] if '-' in selected_model else selected_model}"

# æµ‹è¯•åˆ—è¡¨
TESTS = {[
    {
        "file": test["file"],
        "pillar": test["pillar"],
        "prompt": "è¯·å®Œæˆç›¸å…³æµ‹è¯•"  # å®é™…ä½¿ç”¨æ—¶éœ€è¦æå–çœŸå®çš„prompt
    }
    for test in selected_tests
]}

def run_batch_tests():
    """æ‰§è¡Œæ‰¹é‡æµ‹è¯•"""
    results = []
    
    for i, test in enumerate(TESTS):
        print(f"æ‰§è¡Œæµ‹è¯• {{i+1}}/{{len(TESTS)}}: {{test['file']}}")
        
        try:
            response = call_cloud_service(MODEL_SERVICE, MODEL_NAME, test["prompt"])
            
            result = {{
                "test_file": test["file"],
                "pillar": test["pillar"],
                "response": response,
                "status": "completed"
            }}
        except Exception as e:
            result = {{
                "test_file": test["file"],
                "pillar": test["pillar"],
                "error": str(e),
                "status": "failed"
            }}
        
        results.append(result)
        print(f"å®Œæˆ: {{result['status']}}")
        
        # é¿å…APIé™åˆ¶
        import time
        time.sleep(1)
    
    # ä¿å­˜ç»“æœ
    import json
    with open("batch_test_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"æ‰¹é‡æµ‹è¯•å®Œæˆï¼Œå…±æ‰§è¡Œ {{len(results)}} ä¸ªæµ‹è¯•")

if __name__ == "__main__":
    run_batch_tests()
'''
                    
                    st.download_button(
                        label="ä¸‹è½½æ‰¹é‡æµ‹è¯•è„šæœ¬",
                        data=script_content,
                        file_name=f"batch_test_script_{datetime.now().strftime('%Y%m%d_%H%M%S')}.py",
                        mime="text/plain"
                    )
                    
                    st.info("ä¸‹è½½è„šæœ¬åï¼Œå¯ä»¥åœ¨æœ¬åœ°ç¯å¢ƒä¸­è¿è¡Œæ‰¹é‡æµ‹è¯•")
    
    else:
        st.info("è¯·é€‰æ‹©æ¨¡å‹å’Œæµ‹è¯•ï¼Œç„¶åç‚¹å‡»'å¼€å§‹æµ‹è¯•'")

if __name__ == "__main__":
    main()