# Requirements: Visual Test Interface

## Introduction
This document outlines the requirements for a visual test interface, a standalone web application that allows researchers and non-technical users to interact with the LLM evaluation suite. The interface will enable users to select and run tests, monitor execution in real-time, and visualize results through charts and graphs, all without using the command line.

## Requirements

1.  **As a researcher, I want to see a list of available LLM models, so that I can select which model to test.**
    1.1.  **When** the user opens the application, **then** the interface SHALL fetch the list of available models from the `scripts/utils/cloud_services.py` file.
    1.2.  **When** the list is successfully fetched, **then** the interface SHALL display the models in a dropdown menu.
    1.3.  **When** the list cannot be fetched, **then** the interface SHALL display an error message and allow the user to retry.

2.  **As a researcher, I want to see a list of all available test suites and individual tests, so that I can choose which ones to run.**
    2.  **When** the user opens the test selection panel, **then** the interface SHALL discover all Python test files in the `tests/` directory.
    2.2.  **When** discovering tests, **then** the interface SHALL group them into logical categories (e.g., "基础能力测试", "高级能力测试", "前沿能力测试").
    2.3.  **When** the tests are discovered, **then** the interface SHALL display them as a hierarchical list with checkboxes.
    2.4.  **When** the user checks the top-level category, **then** the interface SHALL automatically check all tests within that category.
    2.5.  **When** the user unchecks a test, **then** the interface SHALL uncheck its parent category if no other tests in that category are selected.

3.  **As a researcher, I want to run a selected set of tests on a chosen model, so that I can evaluate the model's performance.**
    3.1.  **When** the user clicks the "Run Tests" button, **and** a model is selected, **and** at least one test is selected, **then** the interface SHALL execute the corresponding test scripts (e.g., `run_comprehensive_tests.py` for a full suite, or individual `test_pillar_*.py` scripts).
    3.2.  **When** the user clicks "Run Tests" without selecting a model or any tests, **then** the interface SHALL display a validation error.
    3.3.  **When** the test execution starts, **then** the interface SHALL disable the "Run Tests" button to prevent duplicate runs.

4.  **As a researcher, I want to see the real-time progress and output of the running tests, so that I can monitor the execution and identify any issues.**
    4.1.  **When** a test is running, **then** the interface SHALL display a live log output panel.
    4.2.  **When** new output is generated by the test script, **then** the interface SHALL append it to the log panel and scroll to the bottom.
    4.3.  **When** the test is running, **then** the interface SHALL display a progress indicator (e.g., a spinner or progress bar).
    4.4.  **When** the test execution is completed (successfully or with errors), **then** the interface SHALL stop the progress indicator and re-enable the "Run Tests" button.

5.  **As a researcher, I want to see a summary of the test results in the form of charts and graphs, so that I can easily compare the performance of different models and tests.**
    5.1.  **When** a test run is completed, **then** the interface SHALL parse the JSON result file (saved in `testout/`).
    5.2.  **When** the results are parsed, **then** the interface SHALL display a dashboard with charts, including:
        -   A bar chart showing the success rate for each tested pillar.
        -   A pie chart showing the overall pass/fail ratio.
        -   A line chart showing the response time for different test cases (if available).
    5.3.  **When** the user selects a previous test run from a history list, **then** the interface SHALL load and display the corresponding results and charts.
    5.4.  **When** the result file is malformed or cannot be parsed, **then** the interface SHALL display an error message and show the raw JSON content as a fallback.

6.  **As a researcher, I want the visual interface to be a standalone module that does not interfere with the existing command-line tools, so that I can use both interfaces without conflict.**
    6.1.  **When** the visual interface is developed and deployed, **then** the existing command-line scripts (e.g., `run_comprehensive_tests.py`) SHALL remain fully functional.
    6.2.  **When** the visual interface runs a test, **then** it SHALL use the same underlying test scripts and save results to the same `testout/` directory as the command-line tools.
