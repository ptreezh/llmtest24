import requests
import random
import time
import os
import string
import csv
import tiktoken
from typing import Dict, Any

# å¯¼å…¥adaptiveæç¤ºè¯æ¨¡å—
try:
    from adaptive_prompts import ADAPTIVE_SYSTEM_PROMPTS, get_adaptive_messages
    ADAPTIVE_AVAILABLE = True
    print("âœ… Adaptive prompts module loaded successfully")
except ImportError:
    ADAPTIVE_AVAILABLE = False
    print("âš ï¸ Adaptive prompts module not found, using standard prompts")

# === Qiniu DeepSeek (OpenAIå…¼å®¹) API é…ç½® ===
# ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
from dotenv import load_dotenv
load_dotenv()

QINIU_API_URL = os.getenv("QINIU_API_URL", "https://api.qnaigc.com/v1/chat/completions")
QINIU_API_KEY = os.getenv("QINIU_API_KEY")
QINIU_GROUP = os.getenv("QINIU_GROUP", "DeepSeek")

# --- CONFIGURATION ---
# è¯·æ ¹æ®æ‚¨çš„æœ¬åœ°OllamaæœåŠ¡è¿›è¡Œé…ç½®
OLLAMA_API_URL = 'http://localhost:11434/api/chat'
# éœ€è¦è¿›è¡Œè¯„æµ‹çš„æ¨¡å‹åˆ—è¡¨
MODELS_TO_TEST = [
    'deepseek-v3-qiniu',  # ä¼˜å…ˆæµ‹è¯•ä¸ƒç‰›äº‘ DeepSeek å¤–éƒ¨APIæ¨¡å‹
    'qwen3:4b',
    'gemma3:latest'
    #  'atlas/intersync-gemma-7b-instruct-function-calling:latest', # å¦‚æœæ‚¨æœ‰è¿™ä¸ªæ¨¡å‹
    #  'mistral-nemo:latest',
   #  'cogito:latest',
    #  'yi:6b',
    #  'deepseek-coder:6.7b-instruct',
    #  'qwen:7b-chat'
    # 'exaone-deep:7.8b'
]
MAX_CONTEXT_TOKENS = 8192 # å‡è®¾æ‰€æœ‰æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£ä¸º8k
NUM_TEST_CASES = 5    # å¢åŠ ä¸º5è½®æµ‹è¯•
# å¢åŠ ä»»åŠ¡å¤æ‚åº¦ï¼Œç”Ÿæˆ6ä¸‡å­—ä»¥ä¸Šçš„å¯¹è¯
TOTAL_TURNS_PER_CASE = 2000 # æ¯è½®å¹³å‡çº¦30å­—ï¼Œ2000è½®çº¦6ä¸‡å­—
API_TIMEOUT = 3000 # APIè°ƒç”¨è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œå¯¹äºå¤§æ¨¡å‹æ¨ç†ï¼Œå¯èƒ½éœ€è¦è®¾ç½®é•¿ä¸€ç‚¹

# ä½¿ç”¨tiktokenè¿›è¡Œç²¾ç¡®çš„tokenè®¡ç®—
try:
    TOKENIZER = tiktoken.get_encoding("cl100k_base")
except Exception:
    TOKENIZER = tiktoken.encoding_for_model("gpt-4") # å¤‡ç”¨æ–¹æ¡ˆ

# --- 1. DATA GENERATION (IMPROVED) ---

def generate_god_view_script() -> Dict[str, Any]:
    """
    åŠ¨æ€ç”Ÿæˆä¸€ä¸ªç‹¬ç«‹çš„'ç‹¼äººæ€'æ¡ˆä»¶å‰§æœ¬ã€‚
    è¿”å›åŒ…å«å‡¶æ‰‹ã€åŠ¨æœºã€å¼ºçº¿ç´¢å’Œå¼±çº¿ç´¢(å¹²æ‰°é¡¹)çš„å­—å…¸
    å¢åŠ å¹²æ‰°é¡¹æ¯”ä¾‹ï¼Œå¹¶å¼•å…¥å¤šé‡æ¨ç†ï¼ˆä¼ªåŠ¨æœºã€ä¼ªå¼ºçº¿ç´¢ï¼‰
    """
    roles = list(string.ascii_uppercase)[:13] # A-M
    killer = random.choice(roles)
    # éšæœºé€‰ä¸€ä¸ªä¼ªå«Œç–‘äºº
    fake_suspect = random.choice([r for r in roles if r != killer])
    motives = {
        "lumberjack": {
            "motive_desc": "å› æœ¨æåˆ©æ¶¦çº çº·è€Œè¡Œå‡¶",
            "strong_clues": [
                f"æ¡ˆå‘ç°åœºå‘ç°äº†ç¨€æœ‰çš„æ¾æœ¨ç¢å±‘ï¼Œåªæœ‰ä¼æœ¨å·¥ {killer} ä¼šæ¥è§¦è¿™ç§æœ¨æ",
                f"{killer} çš„æ–§å¤´æœ€è¿‘è¢«å¼‚å¸¸ä»”ç»†åœ°æ‰“ç£¨å’Œæ¸…æ´—è¿‡",
                f"æœ‰æ‘æ°‘å¬åˆ° {killer} åœ¨æ¡ˆå‘å‰æ™šå¯¹å—å®³è€…å’†å“®è¯´'è¿™æ˜¯ä½ æœ€åä¸€æ¬¡äº¤è´§'"
            ],
            "red_herrings": [
                "å½“æ™šæœ‰äººå¬åˆ°äº†å¥‡æ€ªçš„é‡å…½åšå«å£°",
                "ä¸€ä¸ªå¸¸åœ¨æ²³è¾¹æ•£æ­¥çš„æ‘æ°‘çœ‹åˆ°ä¸€ä¸ªæ¨¡ç³Šçš„é»‘å½±è·³å…¥æ°´ä¸­",
                "å—å®³è€…æœ€è¿‘ä¼¼ä¹ä¸­äº†ä¸€ç¬”å°å½©ç¥¨ï¼Œä½†å¾ˆå¿«å°±èŠ±å…‰äº†"
            ]
        },
        "baker": {
            "motive_desc": "å› å•†ä¸šç«äº‰è€Œä¸‹æ¯’",
            "strong_clues": [
                f"æ³•åŒ»åœ¨å—å®³è€…çš„èŒ¶æ¯ä¸­æ£€æµ‹åˆ°å¾®é‡æä»å‘³æ¯’ç´ ",
                f"é¢åŒ…å¸ˆ {killer} æœ€è¿‘ä»é»‘å¸‚è´­ä¹°äº†ä¸€æ‰¹è¢«ç§°ä¸º'ç‰¹æ®Šå‘é…µç²‰'çš„åŒ–å­¦å“",
                f"åœ¨å—å®³è€…çš„åƒåœ¾æ¡¶é‡Œå‘ç°äº†ä¸€å¼ è¢«æ’•ç¢çš„ã€å†™æœ‰ {killer} å­—è¿¹çš„é…æ–¹çº¸æ¡"
            ],
            "red_herrings": [
                "å—å®³è€…çš„çª—æˆ·è¢«å‘ç°æ˜¯å¼€ç€çš„",
                f"å¦ä¸€ä½æ‘æ°‘ {random.choice([r for r in roles if r != killer])} å‰å‡ å¤©ä¹Ÿå’Œå—å®³è€…å‘ç”Ÿè¿‡æ¿€çƒˆäº‰åµ",
                "æ¡ˆå‘ç°åœºé™„è¿‘çš„ä¸€æ£µæ ‘ä¸ŠæŒ‚ç€ä¸€å—æ·±è‰²å¸ƒæ–™"
            ]
        }
    }
    motive_key = random.choice(list(motives.keys()))
    script = motives[motive_key]
    # å¢åŠ å¹²æ‰°é¡¹æ¯”ä¾‹ï¼šå°†red_herringsæ•°é‡ç¿»å€
    weak_clues = script["red_herrings"] * 2
    # å¼•å…¥å¤šé‡æ¨ç†ï¼šç”Ÿæˆä¼ªåŠ¨æœºå’Œä¼ªå¼ºçº¿ç´¢
    fake_motive = f"{fake_suspect} å› ä¸ºä¸å—å®³è€…æœ‰ç»æµçº çº·ï¼Œè¿‘æœŸè¡Œä¸ºå¼‚å¸¸ã€‚"
    fake_strong_clues = [
        f"æœ‰äººçœ‹åˆ° {fake_suspect} æ·±å¤œå‡ºç°åœ¨æ¡ˆå‘ç°åœºé™„è¿‘ï¼Œç¥è‰²æ…Œå¼ ",
        f"{fake_suspect} çš„è¡£æœä¸Šå‘ç°äº†ç–‘ä¼¼å—å®³è€…çš„è¡€è¿¹",
        f"æœ‰åŒ¿åä¿¡æŒ‡æ§ {fake_suspect} æ›¾å¨èƒå—å®³è€…"
    ]
    # å°†ä¼ªå¼ºçº¿ç´¢æ··å…¥all_clues
    all_clues = script["strong_clues"] + weak_clues + fake_strong_clues
    random.shuffle(all_clues)
    return {
        "true_killer": killer,
        "motive": script["motive_desc"],
        "strong_clues": script["strong_clues"],
        "weak_clues": weak_clues,
        "fake_motive": fake_motive,
        "fake_strong_clues": fake_strong_clues,
        "all_clues": all_clues
    }

def generate_dialogue(script: Dict[str, Any], total_turns: int) -> str:
    """
    æ ¹æ®å‰§æœ¬ï¼Œç”ŸæˆåŒ…å«å¤§é‡å™ªéŸ³å’Œå…³é”®çº¿ç´¢çš„å¯¹è¯æ–‡æœ¬ã€‚
    """
    dialogue_lines = []
    roles = list(string.ascii_uppercase)[:13]
    clues_to_inject = script['all_clues'].copy()
    random.shuffle(clues_to_inject)
    
    injection_points = sorted(random.sample(range(50, total_turns - 50), len(clues_to_inject)))
    
    common_templates = [
        "æœ€è¿‘æ‘é‡Œæ°”æ°›å¾ˆæ€ªã€‚", "æˆ‘ä¼šæ³¨æ„èº«è¾¹çš„åŠ¨é™ã€‚", "æˆ‘è§‰å¾—å¤§å®¶åº”è¯¥å›¢ç»“èµ·æ¥ã€‚",
        "è¯´å®è¯ï¼Œæˆ‘æœ‰ç‚¹å®³æ€•ï¼Œæ˜¨æ™šæ ¹æœ¬æ²¡æ•¢å‡ºé—¨ã€‚", "å¤§å®¶éƒ½åˆ«ä¹±çŒœäº†ï¼Œå’±ä»¬è¿˜æ˜¯æŠŠçŸ¥é“çš„éƒ½è¯´å‡ºæ¥å§ã€‚",
        "å”‰ï¼Œè¿™ç§äº‹æ€ä¹ˆä¼šå‘ç”Ÿåœ¨æˆ‘ä»¬æ‘å•Šâ€¦â€¦", "æˆ‘è§‰å¾—çº¿ç´¢å¤ªé›¶ç¢äº†ã€‚", "è¦ä¸æˆ‘ä»¬è½®æµè¯´è¯´æ˜¨æ™šéƒ½å¹²äº†å•¥ï¼Ÿ"
    ]

    clue_idx = 0
    for i in range(total_turns):
        speaker = random.choice(roles)
        if clue_idx < len(injection_points) and i == injection_points[clue_idx]:
            clue = clues_to_inject[clue_idx]
            line = f"{speaker}ï¼šæˆ‘å¥½åƒå‘ç°äº†ç‚¹ä»€ä¹ˆâ€¦â€¦ {clue}ã€‚ä¸è¿‡ä¹Ÿå¯èƒ½æ˜¯æˆ‘å¤šå¿ƒäº†ã€‚"
            clue_idx += 1
        else:
            line = f"{speaker}ï¼š{random.choice(common_templates)}"
        dialogue_lines.append(line)

    return "\n".join(dialogue_lines)


# --- 2. PROMPT ENGINEERING ---

def get_prompt(prompt_type: str, context: Dict[str, str] = {}, model: str = "") -> str:
    # ä¼˜åŒ–ç³»ç»Ÿæç¤ºè¯ï¼šå¼ºåŒ–ä¾¦æ¢è§’è‰²å’Œç ´æ¡ˆç›®æ ‡ï¼Œå¼ºè°ƒå› æœå…³ç³»å’Œæ’é™¤æ³•
    optimized_system_prompt = (
        "ä½ æ˜¯ä¸€åç»éªŒä¸°å¯Œçš„ä¾¦æ¢ï¼Œä½ çš„ä»»åŠ¡æ˜¯ç ´è§£è¿™èµ·è°‹æ€æ¡ˆï¼Œæ‰¾å‡ºçœŸå‡¶ï¼Œå¹¶ç”¨è¯æ®æ”¯æŒä½ çš„ç»“è®ºã€‚"
        "è¯·å»ºç«‹æ¸…æ™°çš„å› æœå…³ç³»ï¼ˆå“ªä¸ªè¯æ®æŒ‡å‘å“ªä¸ªå«Œç–‘äººï¼Œä»¥åŠä¸ºä»€ä¹ˆï¼‰ï¼Œ"
        "å¹¶è¯¦ç»†è¯´æ˜ä¸ºä»€ä¹ˆæ’é™¤å…¶ä»–å«Œç–‘äººï¼Œå¦‚ä½•è¯†åˆ«å’Œæ’é™¤ä¼ªé€ çº¿ç´¢ã€‚"
    )
    # é’ˆå¯¹atlas/intersync-gemmaæ¨¡å‹çš„è‹±æ–‡ç¼©å†™æ ¼å¼ï¼ˆæœ€é«˜æ•ˆï¼‰
    if "atlas/intersync-gemma" in model:
        if prompt_type == "intermediate":
            if context.get('summary_so_far', '').strip() and context.get('summary_so_far', '').strip() != 'None':
                summary = context['summary_so_far'][:60]
                new_content = context['new_dialogue_chunk'][:50]
                return f"E:{summary} N:{new_content} U:"
            else:
                content = context['new_dialogue_chunk'][:70]
                return f"S:{content}"
        elif prompt_type == "final":
            facts = context.get('summary_so_far', '')[:150]
            return f"E:{facts} K?"
    # æ ‡å‡†æç¤ºè¯ï¼ˆå…¶ä»–æ¨¡å‹ï¼‰
    if prompt_type == "intermediate":
        if context.get('summary_so_far', '').strip() and context.get('summary_so_far', '').strip() != 'None':
            return f"""System: {optimized_system_prompt}\n\nPrevious summary: {context['summary_so_far']}\n\nNew dialogue segment: {context['new_dialogue_chunk']}\n\nè¯·ç”¨ç®€æ˜ã€é€»è¾‘ç¼œå¯†çš„è¯­è¨€æ›´æ–°æ‘˜è¦ï¼Œçªå‡ºå› æœé“¾æ¡ï¼ˆå“ªä¸ªè¯æ®æŒ‡å‘å“ªä¸ªå«Œç–‘äººï¼Œä»¥åŠä¸ºä»€ä¹ˆï¼‰ï¼Œå¹¶è¯´æ˜å¦‚ä½•æ’é™¤å…¶ä»–å«Œç–‘äººå’Œä¼ªé€ çº¿ç´¢ï¼š"""
        else:
            return f"""System: {optimized_system_prompt}\n\nè¯·æ€»ç»“æœ¬æ®µå¯¹è¯ï¼Œçªå‡ºå…³é”®è¯æ®ã€å› æœå…³ç³»ï¼ˆå“ªä¸ªè¯æ®æŒ‡å‘å“ªä¸ªå«Œç–‘äººï¼Œä»¥åŠä¸ºä»€ä¹ˆï¼‰ï¼Œå¹¶è¯´æ˜å¦‚ä½•æ’é™¤å…¶ä»–å«Œç–‘äººå’Œä¼ªé€ çº¿ç´¢ï¼š\n\n{context['new_dialogue_chunk']}\n\næ‘˜è¦ï¼ˆç®€æ˜ã€é€»è¾‘ç¼œå¯†ï¼‰ï¼š"""
    elif prompt_type == "final":
        return f"""System: {optimized_system_prompt}\n\nè¯·åŸºäºæ‰€æœ‰å·²æ”¶é›†çš„è¯æ®å’Œä¿¡æ¯ï¼Œåˆ†æå¹¶ç¡®å®šè°æ˜¯çœŸæ­£çš„å‡¶æ‰‹ã€‚\n\nå®Œæ•´è¯æ®æ‘˜è¦: {context.get('summary_so_far', '')}\n\nè¯·ç»™å‡ºä½ çš„æœ€ç»ˆæ¨ç†å’Œç»“è®ºï¼Œå¿…é¡»å»ºç«‹æ¸…æ™°çš„å› æœå…³ç³»ï¼ˆå“ªä¸ªè¯æ®æŒ‡å‘å“ªä¸ªå«Œç–‘äººï¼Œä»¥åŠä¸ºä»€ä¹ˆï¼‰ï¼Œå¹¶è¯¦ç»†è¯´æ˜ä¸ºä»€ä¹ˆæ’é™¤å…¶ä»–å«Œç–‘äººï¼Œä»¥åŠå¦‚ä½•è¯†åˆ«å’Œæ’é™¤ä¼ªé€ çº¿ç´¢ï¼š"""
    return ""


# --- 3. EXECUTION & EVALUATION ---

def call_ollama(model: str, prompt: str, use_adaptive: bool = True, test_context: str = "detective_reasoning", max_retries: int = 10) -> str:
    """
    Calls the Ollama API and returns the content of the response.
    æ”¯æŒadaptiveæç¤ºè¯åŠŸèƒ½å’Œé›¶å“åº”é‡è¯•æœºåˆ¶ï¼Œé’ˆå¯¹atlasæ¨¡å‹è¿›è¡Œç‰¹æ®Šä¼˜åŒ–

    Args:
        model: æ¨¡å‹åç§°
        prompt: ç”¨æˆ·æç¤ºè¯
        use_adaptive: æ˜¯å¦ä½¿ç”¨adaptiveæç¤ºè¯
        test_context: æµ‹è¯•ä¸Šä¸‹æ–‡ï¼Œç”¨äºé€‰æ‹©åˆé€‚çš„adaptiveæç¤ºè¯
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    """
    print(f"    - Calling model: {model}...")

    # é’ˆå¯¹atlas/intersync-gemmaæ¨¡å‹çš„ç‰¹æ®Šå¤„ç†
    if "atlas/intersync-gemma" in model:
        # ä½¿ç”¨ç²¾ç®€ç³»ç»Ÿæç¤ºè¯ï¼ˆä¸è¶…è¿‡80å­—ç¬¦ï¼‰
        system_prompt = "Detective. Analyze murder case. Summarize key evidence concisely."
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt}
        ]
        print(f"    ğŸ¯ Using optimized prompt for atlas model (total: {len(system_prompt + prompt)} chars)")
    else:
        # æ ‡å‡†æ¨¡å‹çš„adaptiveæç¤ºè¯å¤„ç†
        if use_adaptive and ADAPTIVE_AVAILABLE:
            try:
                # ä¸ºTestLLMåˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„æµ‹è¯•è„šæœ¬åï¼ŒåŸºäºæµ‹è¯•ä¸Šä¸‹æ–‡
                test_script_name = f"test_pillar_{test_context}.py"

                # æ£€æŸ¥æ˜¯å¦æœ‰é’ˆå¯¹è¯¥æ¨¡å‹çš„adaptiveæç¤ºè¯
                if model in ADAPTIVE_SYSTEM_PROMPTS and test_script_name in ADAPTIVE_SYSTEM_PROMPTS[model]:
                    system_prompt = ADAPTIVE_SYSTEM_PROMPTS[model][test_script_name]
                    messages = [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": prompt}
                    ]
                    print(f"    ğŸ“ Using adaptive system prompt for {model}")
                else:
                    # å¦‚æœæ²¡æœ‰ç‰¹å®šçš„adaptiveæç¤ºè¯ï¼Œä½¿ç”¨é€šç”¨çš„detective reasoningæç¤ºè¯
                    if model in ADAPTIVE_SYSTEM_PROMPTS:
                        # ä½¿ç”¨è¯¥æ¨¡å‹çš„ä»»æ„ä¸€ä¸ªadaptiveæç¤ºè¯ä½œä¸ºåŸºç¡€
                        available_prompts = list(ADAPTIVE_SYSTEM_PROMPTS[model].keys())
                        if available_prompts:
                            # ä¿®æ”¹ä¸ºé€‚åˆdetective reasoningçš„æç¤ºè¯
                            detective_prompt = "You are an expert detective and logical reasoning engine. Your task is to analyze evidence, identify patterns, and draw logical conclusions from the provided information. Focus on clear, step-by-step reasoning."
                            messages = [
                                {"role": "system", "content": detective_prompt},
                                {"role": "user", "content": prompt}
                            ]
                            print(f"    ğŸ“ Using adapted detective reasoning prompt for {model}")
                        else:
                            messages = [{"role": "user", "content": prompt}]
                    else:
                        messages = [{"role": "user", "content": prompt}]
            except Exception as e:
                print(f"    âš ï¸ Adaptive prompts failed, using standard: {e}")
                messages = [{"role": "user", "content": prompt}]
        else:
            # ä½¿ç”¨æ ‡å‡†æ¶ˆæ¯æ ¼å¼
            messages = [{"role": "user", "content": prompt}]

    # é›¶å“åº”é‡è¯•æœºåˆ¶
    for attempt in range(max_retries):
        # é’ˆå¯¹atlasæ¨¡å‹çš„å¼ºåŒ–å‚æ•°ä¼˜åŒ–ï¼ˆç¡®ä¿é›¶å“åº”ï¼‰
        if "atlas/intersync-gemma" in model:
            # æ¸è¿›å¼å‚æ•°è°ƒæ•´ç­–ç•¥
            if attempt <= 2:
                # å‰3æ¬¡å°è¯•ï¼šæ ‡å‡†å‚æ•°
                temp = 0.6 + (attempt * 0.2)
                top_p = 0.95
                top_k = 60
            elif attempt <= 5:
                # ç¬¬4-6æ¬¡ï¼šæé«˜éšæœºæ€§
                temp = 0.9 + (attempt * 0.1)
                top_p = 0.98
                top_k = 80
            else:
                # ç¬¬7-10æ¬¡ï¼šæœ€å¤§éšæœºæ€§
                temp = 1.2 + (attempt * 0.1)
                top_p = 1.0
                top_k = 100

            options = {
                "temperature": min(temp, 2.0),  # é™åˆ¶æœ€å¤§æ¸©åº¦
                "top_p": top_p,
                "top_k": top_k,
                "repeat_penalty": max(1.0, 1.05 - (attempt * 0.01)),  # é€æ­¥é™ä½é‡å¤æƒ©ç½š
                "timeout": 40,
                "num_ctx": max(1024, 2048 - (attempt * 100)),  # é€æ­¥å‡å°‘ä¸Šä¸‹æ–‡
                "num_predict": 100 + (attempt * 10),  # é€æ­¥å¢åŠ è¾“å‡ºé•¿åº¦
                "seed": -1,  # éšæœºç§å­
                "mirostat": 2 if attempt > 3 else 0,  # åæœŸå¯ç”¨mirostat
                "mirostat_tau": 5.0 if attempt > 3 else 5.0
            }
        else:
            options = {
                "temperature": 0.1 + (attempt * 0.1),  # é€æ­¥å¢åŠ æ¸©åº¦
                "top_p": 0.9,
                "timeout": 30
            }

        payload = {
            "model": model,
            "messages": messages,
            "stream": False,
            "options": options
        }

        try:
            response = requests.post(OLLAMA_API_URL, json=payload, timeout=API_TIMEOUT)
            response.raise_for_status()
            data = response.json()
            content = data.get('message', {}).get('content', '')

            if content and content.strip():
                # æˆåŠŸè·å¾—éç©ºå“åº”
                if attempt > 0:
                    print(f"    âœ… Success on retry {attempt + 1}: {len(content)} chars")
                else:
                    print(f"    âœ… Success: {len(content)} chars")
                return content
            else:
                # é›¶å“åº”ï¼Œéœ€è¦é‡è¯•
                print(f"    âš ï¸ Zero response on attempt {attempt + 1}/{max_retries}")
                if attempt < max_retries - 1:
                    print(f"    ğŸ”„ Retrying with adjusted parameters...")
                    time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
                    continue
                else:
                    print(f"    âŒ All retries failed - returning empty response")
                    return ""

        except requests.exceptions.Timeout:
            print(f"    â° Timeout on attempt {attempt + 1}/{max_retries}")
            if attempt < max_retries - 1:
                print(f"    ğŸ”„ Retrying after timeout...")
                time.sleep(3)  # è¶…æ—¶åç­‰å¾…æ›´é•¿æ—¶é—´
                continue
            else:
                return f"[API Error: Timeout after {max_retries} attempts]"

        except requests.exceptions.RequestException as e:
            print(f"    âŒ Request error on attempt {attempt + 1}/{max_retries}: {e}")
            if attempt < max_retries - 1:
                print(f"    ğŸ”„ Retrying after error...")
                time.sleep(2)
                continue
            else:
                return f"[API Error: {e} after {max_retries} attempts]"

    return f"[API Error: All {max_retries} attempts failed]"

def call_qiniu_deepseek(prompt: str, max_retries: int = 5) -> str:
    """
    è°ƒç”¨ä¸ƒç‰›äº‘ DeepSeek (OpenAIå…¼å®¹) APIï¼Œè¿”å›å“åº”å†…å®¹ã€‚
    """
    headers = {
        "Authorization": f"Bearer {QINIU_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": "deepseek-v3",
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 1024,
        "temperature": 0.7,
        # "group": QINIU_GROUP  # å¦‚APIæ”¯æŒåˆ†ç»„å‚æ•°å¯åŠ ä¸Š
    }
    for attempt in range(max_retries):
        try:
            response = requests.post(QINIU_API_URL, headers=headers, json=payload, timeout=240)
            response.raise_for_status()
            data = response.json()
            content = data['choices'][0]['message']['content']
            if content and content.strip():
                print(f"    âœ… Qiniu DeepSeek success: {len(content)} chars")
                return content
        except Exception as e:
            print(f"    âŒ Qiniu DeepSeek API error: {e}")
            time.sleep(2)
    return "[API Error: Qiniu DeepSeek API failed]"

def save_detailed_test_data(case_num: int, model: str, script: Dict[str, Any], dialogue: str,
                           prompts_and_responses: list, final_reasoning: str):
    """
    ä¿å­˜æ¯æ¬¡æµ‹è¯•çš„è¯¦ç»†æ•°æ®ï¼ŒåŒ…æ‹¬å‰§æœ¬å…¨æ–‡ã€åˆ†æ®µå¯¹è¯ã€æç¤ºè¯å’Œå“åº”
    """
    # åˆ›å»ºæµ‹è¯•æ¡ˆä¾‹ä¸“ç”¨æ–‡ä»¶å¤¹
    safe_model_name = model.replace('/', '_').replace(':', '_')
    case_folder = f"case_{case_num}_{safe_model_name}"
    if not os.path.exists(case_folder):
        os.makedirs(case_folder)

    # 1. ä¿å­˜å®Œæ•´å‰§æœ¬ä¿¡æ¯
    script_content = f"""=== æ¡ˆä¾‹ {case_num} å®Œæ•´å‰§æœ¬ ===
æµ‹è¯•æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}
æµ‹è¯•æ¨¡å‹: {model}

--- æ¡ˆä»¶åŸºæœ¬ä¿¡æ¯ ---
çœŸæ­£å‡¶æ‰‹: {script['true_killer']}
ä½œæ¡ˆåŠ¨æœº: {script['motive']}

--- å…³é”®è¯æ® (å¼ºçº¿ç´¢) ---
{chr(10).join(f"{i+1}. {clue}" for i, clue in enumerate(script['strong_clues']))}

--- å¹²æ‰°ä¿¡æ¯ (å¼±çº¿ç´¢) ---
{chr(10).join(f"{i+1}. {clue}" for i, clue in enumerate(script['weak_clues']))}

--- ä¼ªåŠ¨æœº ---
{script['fake_motive']}

--- ä¼ªå¼ºçº¿ç´¢ ---
{chr(10).join(f"{i+1}. {clue}" for i, clue in enumerate(script['fake_strong_clues']))}

--- æ‰€æœ‰çº¿ç´¢æ··åˆ ---
{chr(10).join(f"{i+1}. {clue}" for i, clue in enumerate(script['all_clues']))}
"""

    with open(os.path.join(case_folder, "01_script.txt"), 'w', encoding='utf-8') as f:
        f.write(script_content)

    # 2. ä¿å­˜å®Œæ•´å¯¹è¯æ–‡æœ¬ï¼ˆåˆ†æ®µä¿å­˜ï¼‰
    dialogue_lines = dialogue.split('\n')
    lines_per_segment = 500  # æ¯æ®µ500è¡Œå¯¹è¯

    for i in range(0, len(dialogue_lines), lines_per_segment):
        segment_lines = dialogue_lines[i:i+lines_per_segment]
        segment_num = i // lines_per_segment + 1
        segment_content = f"""=== å¯¹è¯æ®µè½ {segment_num} ===
è¡Œæ•°èŒƒå›´: {i+1} - {min(i+lines_per_segment, len(dialogue_lines))}
æ€»è¡Œæ•°: {len(dialogue_lines)}

{chr(10).join(segment_lines)}
"""
        with open(os.path.join(case_folder, f"02_dialogue_segment_{segment_num:02d}.txt"), 'w', encoding='utf-8') as f:
            f.write(segment_content)

    # 3. ä¿å­˜æ‰€æœ‰æç¤ºè¯å’Œå“åº”
    prompts_content = f"""=== æ¡ˆä¾‹ {case_num} æ‰€æœ‰æç¤ºè¯å’Œå“åº” ===
æµ‹è¯•æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}
æµ‹è¯•æ¨¡å‹: {model}
æ€»äº¤äº’æ¬¡æ•°: {len(prompts_and_responses)}

"""

    for i, interaction in enumerate(prompts_and_responses, 1):
        prompts_content += f"""
--- äº¤äº’ {i} ---
ç±»å‹: {interaction['type']}
TokenèŒƒå›´: {interaction.get('token_range', 'N/A')}

ã€æç¤ºè¯ã€‘
{interaction['prompt']}

ã€æ¨¡å‹å“åº”ã€‘
{interaction['response']}

{'='*50}
"""

    with open(os.path.join(case_folder, "03_prompts_and_responses.txt"), 'w', encoding='utf-8') as f:
        f.write(prompts_content)

    # 4. ä¿å­˜æœ€ç»ˆæ¨ç†
    final_content = f"""=== æ¡ˆä¾‹ {case_num} æœ€ç»ˆæ¨ç† ===
æµ‹è¯•æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}
æµ‹è¯•æ¨¡å‹: {model}

--- æœ€ç»ˆæ¨ç†ç»“æœ ---
{final_reasoning}

--- æ­£ç¡®ç­”æ¡ˆå¯¹ç…§ ---
æ­£ç¡®å‡¶æ‰‹: {script['true_killer']}
ä½œæ¡ˆåŠ¨æœº: {script['motive']}

--- è¯„åˆ¤è¦ç‚¹ ---
1. æ˜¯å¦æ­£ç¡®è¯†åˆ«å‡¶æ‰‹ {script['true_killer']}
2. æ˜¯å¦æœ‰æ•ˆåˆ©ç”¨å…³é”®è¯æ®
3. æ˜¯å¦è¢«å¹²æ‰°ä¿¡æ¯è¯¯å¯¼
4. æ¨ç†é€»è¾‘æ˜¯å¦æ¸…æ™°
5. æ˜¯å¦è¯†åˆ«å¹¶æ’é™¤ä¼ªçº¿ç´¢
"""

    with open(os.path.join(case_folder, "04_final_reasoning.txt"), 'w', encoding='utf-8') as f:
        f.write(final_content)

    # 5. åˆ›å»ºæµ‹è¯•æ‘˜è¦
    summary_content = f"""=== æ¡ˆä¾‹ {case_num} æµ‹è¯•æ‘˜è¦ ===
æµ‹è¯•æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}
æµ‹è¯•æ¨¡å‹: {model}

--- æ–‡ä»¶è¯´æ˜ ---
01_script.txt - å®Œæ•´æ¡ˆä»¶å‰§æœ¬å’Œçº¿ç´¢ä¿¡æ¯
02_dialogue_segment_XX.txt - åˆ†æ®µå¯¹è¯å†…å®¹
03_prompts_and_responses.txt - æ‰€æœ‰æç¤ºè¯å’Œæ¨¡å‹å“åº”
04_final_reasoning.txt - æœ€ç»ˆæ¨ç†ç»“æœå’Œè¯„åˆ¤æ ‡å‡†

--- å¿«é€Ÿå¯¹æ¯” ---
æ­£ç¡®ç­”æ¡ˆ: {script['true_killer']}
æ¨¡å‹æ˜¯å¦æåŠæ­£ç¡®å‡¶æ‰‹: {'æ˜¯' if script['true_killer'] in final_reasoning else 'å¦'}
å“åº”é•¿åº¦: {len(final_reasoning)} å­—ç¬¦
å“åº”çŠ¶æ€: {'æ­£å¸¸' if final_reasoning and '[API Error:' not in final_reasoning else 'å¼‚å¸¸'}

--- äººå·¥è¯„åˆ¤æç¤º ---
è¯·æŸ¥çœ‹ 04_final_reasoning.txt ä¸­çš„æœ€ç»ˆæ¨ç†ï¼Œ
å¯¹æ¯”æ­£ç¡®ç­”æ¡ˆ {script['true_killer']}ï¼Œ
è¯„ä¼°æ¨¡å‹çš„æ¨ç†è´¨é‡å’Œå‡†ç¡®æ€§ã€‚
"""

    with open(os.path.join(case_folder, "00_README.txt"), 'w', encoding='utf-8') as f:
        f.write(summary_content)

    print(f"    âœ… è¯¦ç»†æµ‹è¯•æ•°æ®å·²ä¿å­˜åˆ°æ–‡ä»¶å¤¹: {case_folder}")
    return case_folder

def save_case_analysis(case_num: int, model: str, script: Dict[str, Any], final_reasoning: str):
    """
    ä¿å­˜æ¡ˆä¾‹åˆ†ææŠ¥å‘Šï¼ŒåŒ…å«æ­£ç¡®ç­”æ¡ˆå’Œè¯„åˆ¤æ ‡å‡†ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
    """
    # æ£€æŸ¥æ¨¡å‹å“åº”è´¨é‡
    if not final_reasoning or final_reasoning.strip() == "":
        reasoning_status = "âŒ æ¨¡å‹æœªæä¾›åˆ†æ (å¯èƒ½æ˜¯é›¶å“åº”é—®é¢˜)"
        reasoning_content = "æ— å“åº”å†…å®¹"
    elif "[API Error:" in final_reasoning:
        reasoning_status = "âŒ APIè°ƒç”¨é”™è¯¯"
        reasoning_content = final_reasoning
    else:
        reasoning_status = "âœ… æ¨¡å‹æä¾›äº†åˆ†æ"
        reasoning_content = final_reasoning

    analysis_report = f"""
=== æ¡ˆä¾‹ {case_num} åˆ†ææŠ¥å‘Š ===
æ¨¡å‹: {model}
æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}
çŠ¶æ€: {reasoning_status}

--- æ¨¡å‹åŸå§‹åˆ†æ ---
{reasoning_content}

--- æ­£ç¡®ç­”æ¡ˆä¸è¯„åˆ¤æ ‡å‡† ---
âœ… æ­£ç¡®å‡¶æ‰‹: {script['true_killer']}
âœ… ä½œæ¡ˆåŠ¨æœº: {script['motive']}

âœ… å…³é”®è¯æ® (å¼ºçº¿ç´¢):
{chr(10).join(f"  â€¢ {clue}" for clue in script['strong_clues'])}

âš ï¸ å¹²æ‰°ä¿¡æ¯ (å¼±çº¿ç´¢):
{chr(10).join(f"  â€¢ {clue}" for clue in script['weak_clues'])}

â— ä¼ªåŠ¨æœº:
  â€¢ {script['fake_motive']}
â— ä¼ªå¼ºçº¿ç´¢:
{chr(10).join(f"  â€¢ {clue}" for clue in script['fake_strong_clues'])}

ğŸ“‹ è¯„åˆ¤æ ‡å‡†:
1. å‡¶æ‰‹è¯†åˆ« (æ˜¯å¦æ­£ç¡®æŒ‡å‡º {script['true_killer']})
2. è¯æ®ä½¿ç”¨ (æ˜¯å¦æœ‰æ•ˆåˆ©ç”¨å…³é”®è¯æ®)
3. é€»è¾‘æ¨ç† (æ¨ç†é“¾æ˜¯å¦æ¸…æ™°è¿è´¯)
4. å¹²æ‰°æ’é™¤ (æ˜¯å¦è¢«å¼±çº¿ç´¢æˆ–ä¼ªçº¿ç´¢è¯¯å¯¼)
5. å¤šé‡æ¨ç† (æ˜¯å¦èƒ½è¯†åˆ«å¹¶æ’é™¤ä¼ªåŠ¨æœº/ä¼ªå¼ºçº¿ç´¢)

--- æ¨ç†è¦ç‚¹ ---
æ­£ç¡®çš„æ¨ç†åº”è¯¥:
â€¢ é‡ç‚¹å…³æ³¨å¼ºçº¿ç´¢ï¼Œå®ƒä»¬ç›´æ¥æŒ‡å‘çœŸå‡¶
â€¢ è¯†åˆ«å¹¶æ’é™¤å¹²æ‰°ä¿¡æ¯å’Œä¼ªçº¿ç´¢
â€¢ å»ºç«‹æ¸…æ™°çš„å› æœå…³ç³»é“¾
â€¢ å¾—å‡ºæ˜ç¡®çš„ç»“è®º
â€¢ èƒ½è¯†åˆ«ä¼ªåŠ¨æœºå’Œä¼ªå¼ºçº¿ç´¢çš„è¯¯å¯¼

--- æ‰‹åŠ¨è¯„åˆ¤æŒ‡å— ---
è¯·æ ¹æ®ä»¥ä¸Šæ ‡å‡†å¯¹æ¨¡å‹åˆ†æè¿›è¡Œè¯„åˆ† (1-5åˆ†):
â–¡ å‡¶æ‰‹è¯†åˆ«: ___/5 (æ˜¯å¦æ­£ç¡®è¯†åˆ«å‡º {script['true_killer']})
â–¡ è¯æ®ä½¿ç”¨: ___/5 (æ˜¯å¦æœ‰æ•ˆä½¿ç”¨å¼ºçº¿ç´¢)
â–¡ é€»è¾‘æ¨ç†: ___/5 (æ¨ç†æ˜¯å¦æ¸…æ™°è¿è´¯)
â–¡ å¹²æ‰°æ’é™¤: ___/5 (æ˜¯å¦é¿å…è¢«å¼±çº¿ç´¢æˆ–ä¼ªçº¿ç´¢è¯¯å¯¼)
â–¡ å¤šé‡æ¨ç†: ___/5 (æ˜¯å¦èƒ½è¯†åˆ«ä¼ªåŠ¨æœº/ä¼ªå¼ºçº¿ç´¢)
â–¡ æ€»ä½“è¯„åˆ†: ___/5

===============================
"""

    # ä¿å­˜åˆ°æ–‡ä»¶ï¼Œè‹¥é‡ååˆ™è‡ªåŠ¨ç¼–å·
    base_filename = f"case_{case_num}_{model.replace('/', '_').replace(':', '_')}_analysis.txt"
    filename = base_filename
    file_index = 1
    while os.path.exists(filename):
        name_part, ext = os.path.splitext(base_filename)
        filename = f"{name_part}_{file_index}{ext}"
        file_index += 1
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(analysis_report)

    print(f"    âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {filename}")
    return analysis_report

def run_test_pipeline():
    """
    ä¸»æµ‹è¯•æµç¨‹å‡½æ•° - åªä½¿ç”¨4000 tokensåˆ†æ®µæ–¹æ¡ˆï¼Œè®°å½•åŸå§‹åˆ†ææŠ¥å‘Šï¼Œä¿å­˜è¯¦ç»†æµ‹è¯•æ•°æ®
    """
    results_filepath = 'model_analysis_reports.csv'
    all_results = []

    chunk_size = 4000  # åªç”¨4000 tokensåˆ†æ®µ
    strategy_name = f"Balanced-{chunk_size}tokens"
    breakpoints = [chunk_size]
    for i in range(NUM_TEST_CASES):
        print(f"\n--- Running Test Case {i+1}/{NUM_TEST_CASES} ---")
        script = generate_god_view_script()
        dialogue = generate_dialogue(script, TOTAL_TURNS_PER_CASE)
        dialogue_tokens = TOKENIZER.encode(dialogue)
        print(f"  - Case generated. Killer: {script['true_killer']}. Total tokens: {len(dialogue_tokens)}")
        for model in MODELS_TO_TEST:
            print(f"\n  Testing Model: {model}, Strategy: {strategy_name}")
            last_summary = ""
            start_idx = 0
            segment_count = 0
            prompts_and_responses = []  # è®°å½•æ‰€æœ‰æç¤ºè¯å’Œå“åº”

            while start_idx < len(dialogue_tokens):
                end_idx = min(start_idx + chunk_size, len(dialogue_tokens))
                chunk_text = TOKENIZER.decode(dialogue_tokens[start_idx:end_idx])
                segment_count += 1
                print(f"    - Segment {segment_count}: Processing tokens {start_idx} to {end_idx} ({end_idx - start_idx} tokens)")
                prompt = get_prompt("intermediate", {
                    "summary_so_far": last_summary,
                    "new_dialogue_chunk": chunk_text
                }, model)

                if model == "deepseek-v3-qiniu":
                    intermediate_summary = call_qiniu_deepseek(prompt)
                else:
                    intermediate_summary = call_ollama(model, prompt, use_adaptive=False, test_context="summary_analysis")

                # è®°å½•æç¤ºè¯å’Œå“åº”
                prompts_and_responses.append({
                    "type": f"intermediate_segment_{segment_count}",
                    "token_range": f"{start_idx}-{end_idx}",
                    "prompt": prompt,
                    "response": intermediate_summary
                })

                if not intermediate_summary or intermediate_summary.strip() == "":
                    print("    ğŸ”„ Zero response, trying fallback prompt...")
                    if "atlas/intersync-gemma" in model:
                        if last_summary.strip():
                            fallback_prompt = f"Update:{last_summary[:30]}"
                        else:
                            fallback_prompt = f"Sum:{chunk_text[:40]}"
                        intermediate_summary = call_ollama(model, fallback_prompt, use_adaptive=False, test_context="summary_analysis")
                        print(f"    ğŸ†˜ Fallback prompt result: {len(intermediate_summary) if intermediate_summary else 0} chars")

                        # è®°å½•fallbackæç¤ºè¯å’Œå“åº”
                        prompts_and_responses.append({
                            "type": f"fallback_segment_{segment_count}",
                            "token_range": f"{start_idx}-{end_idx}",
                            "prompt": fallback_prompt,
                            "response": intermediate_summary
                        })

                if "[API Error:" in intermediate_summary:
                    print("    - Halting strategy due to API error.")
                    last_summary = intermediate_summary
                    break
                if not intermediate_summary or intermediate_summary.strip() == "":
                    print("    ğŸ†˜ Using default summary to continue...")
                    if last_summary.strip():
                        intermediate_summary = last_summary[:100] + " [continued]"
                    else:
                        intermediate_summary = "Evidence found, investigation continues."
                if "atlas/intersync-gemma" in model and intermediate_summary:
                    if len(intermediate_summary) > 150:
                        intermediate_summary = intermediate_summary[:147] + "..."
                        print(f"    ğŸ“ Truncated summary to 150 chars for atlas model")
                last_summary = intermediate_summary
                start_idx = end_idx
                time.sleep(2)

            if "[API Error:" in last_summary:
                final_reasoning = last_summary
            else:
                print("    - Generating final reasoning...")
                final_prompt = get_prompt("final", {"summary_so_far": last_summary}, model)
                if model == "deepseek-v3-qiniu":
                    final_reasoning = call_qiniu_deepseek(final_prompt)
                else:
                    final_reasoning = call_ollama(model, final_prompt, use_adaptive=False, test_context="final_reasoning")

                # è®°å½•æœ€ç»ˆæ¨ç†æç¤ºè¯å’Œå“åº”
                prompts_and_responses.append({
                    "type": "final_reasoning",
                    "token_range": "final",
                    "prompt": final_prompt,
                    "response": final_reasoning
                })

                if not final_reasoning or final_reasoning.strip() == "":
                    print("    ğŸ”„ Final reasoning zero response, trying fallback...")
                    if "atlas/intersync-gemma" in model:
                        fallback_final = f"Who killed? {last_summary[:50]}"
                        final_reasoning = call_ollama(model, fallback_final, use_adaptive=False, test_context="final_reasoning")
                        print(f"    ğŸ†˜ Fallback final reasoning: {len(final_reasoning) if final_reasoning else 0} chars")

                        # è®°å½•fallbackæœ€ç»ˆæ¨ç†
                        prompts_and_responses.append({
                            "type": "fallback_final_reasoning",
                            "token_range": "final",
                            "prompt": fallback_final,
                            "response": final_reasoning
                        })

                if not final_reasoning or final_reasoning.strip() == "":
                    print("    ğŸ†˜ Using default final reasoning...")
                    final_reasoning = f"Based on the evidence: {last_summary[:100]}, further investigation needed to determine the killer."

            # ä¿å­˜è¯¦ç»†æµ‹è¯•æ•°æ®ï¼ˆæ–°åŠŸèƒ½ï¼‰
            print("    - Saving detailed test data...")
            save_detailed_test_data(i + 1, model, script, dialogue, prompts_and_responses, final_reasoning)

            # ä¿å­˜ä¼ ç»Ÿåˆ†ææŠ¥å‘Šï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
            if "[API Error:" not in final_reasoning:
                print("    - Saving analysis report with correct answers...")
                save_case_analysis(i + 1, model, script, final_reasoning)

            if not final_reasoning or final_reasoning.strip() == "":
                response_status = "zero_response"
            elif "[API Error:" in final_reasoning:
                response_status = "api_error"
            else:
                response_status = "success"
            result = {
                "test_case": i + 1,
                "model": model,
                "strategy": strategy_name,
                "true_killer": script['true_killer'],
                "motive": script['motive'],
                "strong_clues": "; ".join(script['strong_clues']),
                "weak_clues": "; ".join(script['weak_clues']),
                "final_reasoning": final_reasoning,
                "response_status": response_status,
                "reasoning_length": len(final_reasoning) if final_reasoning else 0,
                "timestamp": time.strftime('%Y-%m-%d %H:%M:%S')
            }
            all_results.append(result)
            if all_results:
                with open(results_filepath, 'w', newline='', encoding='utf-8') as f:
                    writer = csv.DictWriter(f, fieldnames=all_results[0].keys())
                    writer.writeheader()
                    writer.writerows(all_results)
    print(f"\n--- Test Suite Complete. Full report saved to {results_filepath} ---")

# ====== æœ€å°åŒ–å¤–éƒ¨APIè¿é€šæ€§æµ‹è¯• ======
if __name__ == "__main__":
    if not os.path.exists('recursive_summary_results'):
        os.makedirs('recursive_summary_results')
    os.chdir('recursive_summary_results')
    
    run_test_pipeline()

    print(f"\n--- æµ‹è¯•å®Œæˆ ---")
    print(f"æ‰€æœ‰æµ‹è¯•æ•°æ®å·²ä¿å­˜åˆ°å½“å‰ç›®å½•")
    print(f"")
    print(f"ğŸ“Š æ±‡æ€»æŠ¥å‘Š:")
    print(f"  â€¢ CSVæ±‡æ€»æŠ¥å‘Š: model_analysis_reports.csv")
    print(f"  â€¢ ä¼ ç»Ÿåˆ†ææŠ¥å‘Š: case_X_MODEL_analysis.txt")
    print(f"")
    print(f"ğŸ“ è¯¦ç»†æµ‹è¯•æ•°æ® (æ¯ä¸ªæ¡ˆä¾‹ä¸€ä¸ªæ–‡ä»¶å¤¹):")
    print(f"  â€¢ case_X_MODEL/ - åŒ…å«å®Œæ•´æµ‹è¯•æ•°æ®")
    print(f"    â”œâ”€â”€ 00_README.txt - æµ‹è¯•æ‘˜è¦å’Œå¿«é€Ÿå¯¹æ¯”")
    print(f"    â”œâ”€â”€ 01_script.txt - å®Œæ•´æ¡ˆä»¶å‰§æœ¬å’Œçº¿ç´¢")
    print(f"    â”œâ”€â”€ 02_dialogue_segment_XX.txt - åˆ†æ®µå¯¹è¯å†…å®¹")
    print(f"    â”œâ”€â”€ 03_prompts_and_responses.txt - æ‰€æœ‰æç¤ºè¯å’Œå“åº”")
    print(f"    â””â”€â”€ 04_final_reasoning.txt - æœ€ç»ˆæ¨ç†å’Œè¯„åˆ¤æ ‡å‡†")
    print(f"")
    print(f"ğŸ” äººå·¥å¯¹æ¯”æµ‹è¯•:")
    print(f"  â€¢ æŸ¥çœ‹å„æ–‡ä»¶å¤¹ä¸­çš„ 03_prompts_and_responses.txt")
    print(f"  â€¢ å¤åˆ¶æç¤ºè¯åˆ°ç½‘é¡µAIæœåŠ¡è¿›è¡Œå¯¹æ¯”æµ‹è¯•")
    print(f"  â€¢ å‚è€ƒ 04_final_reasoning.txt ä¸­çš„æ­£ç¡®ç­”æ¡ˆ")
    print(f"  â€¢ ä½¿ç”¨ 00_README.txt å¿«é€Ÿäº†è§£æµ‹è¯•ç»“æœ")

# ====== æœ€å°åŒ–å¤–éƒ¨APIè¿é€šæ€§æµ‹è¯•ï¼ˆå¦‚éœ€å•ç‹¬æµ‹è¯•è¯·å–æ¶ˆæ³¨é‡Šï¼‰ ======
# if __name__ == "__main__":
#     print("\n--- Qiniu DeepSeek API æœ€å°åŒ–è¿é€šæ€§æµ‹è¯• ---")
#     test_prompt = "è¯·ç”¨ä¸€å¥è¯ä»‹ç»ä¸ƒç‰›äº‘çš„AIæ¨ç†èƒ½åŠ›ã€‚"
#     result = call_qiniu_deepseek(test_prompt)
#     print("APIè¿”å›ï¼š", result)

import sys
import io

# ä¿®æ”¹æ ‡å‡†è¾“å‡ºç¼–ç ä¸ºUTF-8
# å‡½æ•°è¯´æ˜ï¼šåˆå§‹åŒ–ç³»ç»Ÿæ ‡å‡†è¾“å‡ºæµï¼Œè®¾ç½®ç¼–ç ä¸ºUTF-8ä»¥æ”¯æŒUnicodeå­—ç¬¦
def init_console_encoding():
    if sys.stdout.encoding != 'utf-8':
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')

# è°ƒç”¨åˆå§‹åŒ–å‡½æ•°
init_console_encoding()

# æµ‹è¯•Unicodeå­—ç¬¦è¾“å‡º
print("âœ“ Adaptive prompts module loaded successfully")
