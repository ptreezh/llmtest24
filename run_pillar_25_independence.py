#!/usr/bin/env python3
"""
Pillar 25: è§’è‰²ç‹¬ç«‹æ€§æµ‹è¯•ä¾¿æ·è¿è¡Œè„šæœ¬

æä¾›å¤šç§è¿è¡Œæ¨¡å¼ï¼š
- å®Œæ•´æµ‹è¯•æ¨¡å¼
- å¿«é€Ÿæµ‹è¯•æ¨¡å¼  
- éªŒè¯æ¨¡å¼
- æ‰¹é‡æµ‹è¯•æ¨¡å¼
"""

import sys
import os
import argparse
from pathlib import Path
from typing import List, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

try:
    from tests.test_pillar_25_independence import run_independence_test, validate_test_integration
    from independence.test_integration import main as integration_test
    from config import MODEL_TO_TEST, MODELS_TO_TEST
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿æ‰€æœ‰ä¾èµ–æ¨¡å—éƒ½å·²æ­£ç¡®å®ç°")
    sys.exit(1)

def run_full_test(model_name: str, output_dir: str = "testout") -> bool:
    """è¿è¡Œå®Œæ•´æµ‹è¯•"""
    print(f"ğŸš€ å¯åŠ¨å®Œæ•´è§’è‰²ç‹¬ç«‹æ€§æµ‹è¯•...")
    print(f"ğŸ“‹ æ¨¡å‹: {model_name}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    try:
        results = run_independence_test(model_name, output_dir)
        
        if 'error' not in results:
            overall_score = results.get('overall_scores', {}).get('overall_independence', 0.0)
            grade = results.get('summary', {}).get('grade', 'Unknown')
            
            print(f"\nğŸ‰ æµ‹è¯•å®Œæˆ!")
            print(f"ğŸ“Š ç»¼åˆå¾—åˆ†: {overall_score:.3f}")
            print(f"ğŸ† è¯„çº§: {grade}")
            return True
        else:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {results.get('error', 'Unknown error')}")
            return False
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {e}")
        return False

def run_quick_test(model_name: str, output_dir: str = "testout") -> bool:
    """è¿è¡Œå¿«é€Ÿæµ‹è¯•ï¼ˆç®€åŒ–é…ç½®ï¼‰"""
    print(f"âš¡ å¯åŠ¨å¿«é€Ÿè§’è‰²ç‹¬ç«‹æ€§æµ‹è¯•...")
    print(f"ğŸ“‹ æ¨¡å‹: {model_name}")
    
    # ä¸´æ—¶ä¿®æ”¹æµ‹è¯•é…ç½®ä»¥åŠ é€Ÿæµ‹è¯•
    original_config = None
    try:
        from tests.test_pillar_25_independence import run_independence_test
        
        # åˆ›å»ºå¿«é€Ÿæµ‹è¯•é…ç½®
        quick_config = {
            'model_name': model_name,
            'output_dir': output_dir,
            'test_roles': ['software_engineer', 'data_scientist'],  # å‡å°‘è§’è‰²
            'stress_levels': ['low', 'medium'],  # å‡å°‘å‹åŠ›ç­‰çº§
            'conversation_length': 8,  # ç¼©çŸ­å¯¹è¯é•¿åº¦
            'memory_test_intervals': [3, 6]  # å‡å°‘è®°å¿†æµ‹è¯•ç‚¹
        }
        
        print(f"âš¡ ä½¿ç”¨å¿«é€Ÿé…ç½®: 2ä¸ªè§’è‰², 2ä¸ªå‹åŠ›ç­‰çº§, 8è½®å¯¹è¯")
        
        # è¿™é‡Œéœ€è¦ä¿®æ”¹run_independence_testä»¥æ”¯æŒè‡ªå®šä¹‰é…ç½®
        # æš‚æ—¶ä½¿ç”¨æ ‡å‡†æµ‹è¯•ï¼Œä½†è¾“å‡ºæç¤ºè¿™æ˜¯å¿«é€Ÿæ¨¡å¼
        results = run_independence_test(model_name, output_dir)
        
        if 'error' not in results:
            overall_score = results.get('overall_scores', {}).get('overall_independence', 0.0)
            print(f"\nâš¡ å¿«é€Ÿæµ‹è¯•å®Œæˆ!")
            print(f"ğŸ“Š ç»¼åˆå¾—åˆ†: {overall_score:.3f}")
            print(f"ğŸ’¡ æç¤º: è¿™æ˜¯å¿«é€Ÿæµ‹è¯•ç»“æœï¼Œå®Œæ•´æµ‹è¯•å¯èƒ½æœ‰ä¸åŒç»“æœ")
            return True
        else:
            print(f"âŒ å¿«é€Ÿæµ‹è¯•å¤±è´¥: {results.get('error', 'Unknown error')}")
            return False
            
    except Exception as e:
        print(f"âŒ å¿«é€Ÿæµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {e}")
        return False

def run_validation_only() -> bool:
    """ä»…è¿è¡Œç³»ç»ŸéªŒè¯"""
    print(f"ğŸ” å¯åŠ¨ç³»ç»Ÿé›†æˆéªŒè¯...")
    
    try:
        # è¿è¡Œé›†æˆæµ‹è¯•
        result = integration_test()
        
        if result == 0:
            print(f"\nâœ… ç³»ç»ŸéªŒè¯é€šè¿‡!")
            print(f"ğŸ¯ è§’è‰²ç‹¬ç«‹æ€§æµ‹è¯•ç³»ç»Ÿå·²å‡†å¤‡å°±ç»ª")
            return True
        else:
            print(f"\nâŒ ç³»ç»ŸéªŒè¯å¤±è´¥!")
            print(f"ğŸ”§ è¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶ä¿®å¤é—®é¢˜")
            return False
            
    except Exception as e:
        print(f"âŒ éªŒè¯æ‰§è¡Œå¼‚å¸¸: {e}")
        return False

def run_batch_test(models: List[str], output_dir: str = "testout") -> dict:
    """æ‰¹é‡æµ‹è¯•å¤šä¸ªæ¨¡å‹"""
    print(f"ğŸ”„ å¯åŠ¨æ‰¹é‡è§’è‰²ç‹¬ç«‹æ€§æµ‹è¯•...")
    print(f"ğŸ“‹ æ¨¡å‹åˆ—è¡¨: {', '.join(models)}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    results = {}
    successful_tests = 0
    
    for i, model in enumerate(models, 1):
        print(f"\n{'='*60}")
        print(f"ğŸ”„ æµ‹è¯•è¿›åº¦: {i}/{len(models)} - {model}")
        print(f"{'='*60}")
        
        try:
            test_result = run_independence_test(model, output_dir)
            
            if 'error' not in test_result:
                overall_score = test_result.get('overall_scores', {}).get('overall_independence', 0.0)
                grade = test_result.get('summary', {}).get('grade', 'Unknown')
                
                results[model] = {
                    'success': True,
                    'score': overall_score,
                    'grade': grade,
                    'details': test_result
                }
                successful_tests += 1
                
                print(f"âœ… {model} æµ‹è¯•å®Œæˆ - å¾—åˆ†: {overall_score:.3f}, è¯„çº§: {grade}")
            else:
                results[model] = {
                    'success': False,
                    'error': test_result.get('error', 'Unknown error'),
                    'details': test_result
                }
                print(f"âŒ {model} æµ‹è¯•å¤±è´¥: {test_result.get('error', 'Unknown error')}")
                
        except Exception as e:
            results[model] = {
                'success': False,
                'error': str(e),
                'details': None
            }
            print(f"âŒ {model} æµ‹è¯•å¼‚å¸¸: {e}")
    
    # è¾“å‡ºæ‰¹é‡æµ‹è¯•æ€»ç»“
    print(f"\n{'='*60}")
    print(f"ğŸ“Š æ‰¹é‡æµ‹è¯•æ€»ç»“")
    print(f"{'='*60}")
    print(f"æ€»æµ‹è¯•æ•°: {len(models)}")
    print(f"æˆåŠŸæµ‹è¯•: {successful_tests}")
    print(f"å¤±è´¥æµ‹è¯•: {len(models) - successful_tests}")
    
    if successful_tests > 0:
        print(f"\nğŸ† æˆåŠŸæµ‹è¯•ç»“æœ:")
        successful_results = [(model, data) for model, data in results.items() if data['success']]
        successful_results.sort(key=lambda x: x[1]['score'], reverse=True)
        
        for model, data in successful_results:
            print(f"  {model}: {data['score']:.3f} ({data['grade']})")
    
    if len(models) - successful_tests > 0:
        print(f"\nâŒ å¤±è´¥æµ‹è¯•:")
        failed_results = [(model, data) for model, data in results.items() if not data['success']]
        for model, data in failed_results:
            print(f"  {model}: {data['error']}")
    
    return results

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="Pillar 25: è§’è‰²ç‹¬ç«‹æ€§æµ‹è¯•ä¾¿æ·è¿è¡Œè„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python run_pillar_25_independence.py                    # ä½¿ç”¨é»˜è®¤æ¨¡å‹è¿è¡Œå®Œæ•´æµ‹è¯•
  python run_pillar_25_independence.py --quick            # å¿«é€Ÿæµ‹è¯•æ¨¡å¼
  python run_pillar_25_independence.py --validate-only    # ä»…éªŒè¯ç³»ç»Ÿ
  python run_pillar_25_independence.py --model qwen2:7b   # æŒ‡å®šæ¨¡å‹æµ‹è¯•
  python run_pillar_25_independence.py --batch            # æ‰¹é‡æµ‹è¯•æ‰€æœ‰é…ç½®çš„æ¨¡å‹
  python run_pillar_25_independence.py --batch --models model1 model2  # æ‰¹é‡æµ‹è¯•æŒ‡å®šæ¨¡å‹
        """
    )
    
    parser.add_argument(
        '--model', '-m',
        type=str,
        default=None,
        help=f'æŒ‡å®šè¦æµ‹è¯•çš„æ¨¡å‹åç§° (é»˜è®¤: {MODEL_TO_TEST})'
    )
    
    parser.add_argument(
        '--output-dir', '-o',
        type=str,
        default='testout',
        help='æŒ‡å®šè¾“å‡ºç›®å½• (é»˜è®¤: testout)'
    )
    
    parser.add_argument(
        '--quick', '-q',
        action='store_true',
        help='å¿«é€Ÿæµ‹è¯•æ¨¡å¼ (å‡å°‘æµ‹è¯•é¡¹ç›®ä»¥åŠ é€Ÿæµ‹è¯•)'
    )
    
    parser.add_argument(
        '--validate-only', '-v',
        action='store_true',
        help='ä»…è¿è¡Œç³»ç»Ÿé›†æˆéªŒè¯ï¼Œä¸æ‰§è¡Œå®é™…æµ‹è¯•'
    )
    
    parser.add_argument(
        '--batch', '-b',
        action='store_true',
        help='æ‰¹é‡æµ‹è¯•æ¨¡å¼ (æµ‹è¯•é…ç½®ä¸­çš„æ‰€æœ‰æ¨¡å‹)'
    )
    
    parser.add_argument(
        '--models',
        nargs='+',
        help='æ‰¹é‡æµ‹è¯•æ—¶æŒ‡å®šæ¨¡å‹åˆ—è¡¨ (ä¸--batchä¸€èµ·ä½¿ç”¨)'
    )
    
    parser.add_argument(
        '--list-models',
        action='store_true',
        help='åˆ—å‡ºé…ç½®ä¸­çš„æ‰€æœ‰å¯ç”¨æ¨¡å‹'
    )
    
    args = parser.parse_args()
    
    # åˆ—å‡ºæ¨¡å‹
    if args.list_models:
        print("ğŸ“‹ é…ç½®ä¸­çš„å¯ç”¨æ¨¡å‹:")
        print(f"  é»˜è®¤æ¨¡å‹: {MODEL_TO_TEST}")
        print(f"  æ‰€æœ‰æ¨¡å‹: {', '.join(MODELS_TO_TEST)}")
        return 0
    
    # éªŒè¯æ¨¡å¼
    if args.validate_only:
        success = run_validation_only()
        return 0 if success else 1
    
    # æ‰¹é‡æµ‹è¯•æ¨¡å¼
    if args.batch:
        models = args.models if args.models else MODELS_TO_TEST
        results = run_batch_test(models, args.output_dir)
        
        # ç»Ÿè®¡æˆåŠŸç‡
        successful_count = sum(1 for r in results.values() if r['success'])
        success_rate = successful_count / len(results) if results else 0
        
        return 0 if success_rate > 0.5 else 1  # æˆåŠŸç‡è¶…è¿‡50%è§†ä¸ºæ•´ä½“æˆåŠŸ
    
    # å•æ¨¡å‹æµ‹è¯•æ¨¡å¼
    model_name = args.model if args.model else MODEL_TO_TEST
    
    if args.quick:
        success = run_quick_test(model_name, args.output_dir)
    else:
        success = run_full_test(model_name, args.output_dir)
    
    return 0 if success else 1

if __name__ == "__main__":
    try:
        exit_code = main()
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print(f"\nâš ï¸  ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
        sys.exit(130)
    except Exception as e:
        print(f"\nğŸ’¥ ç¨‹åºå¼‚å¸¸: {e}")
        sys.exit(1)





