#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•atlasæ¨¡å‹çš„çœŸæ­£çŸ­æç¤ºè¯é…ç½®
éªŒè¯åˆå§‹æç¤ºè¯æ˜¯å¦çœŸçš„åœ¨80å­—ç¬¦ä»¥å†…
"""

def get_prompt(prompt_type: str, context: dict = {}, model: str = "") -> str:
    """å¤åˆ¶TestLLM.pyä¸­çš„æœ€æ–°get_promptå‡½æ•°"""
    # é’ˆå¯¹atlas/intersync-gemmaæ¨¡å‹çš„æç®€æç¤ºè¯ï¼ˆæ€»é•¿åº¦<300å­—ç¬¦ï¼‰
    # ç³»ç»Ÿæç¤ºè¯(65) + ç”¨æˆ·æç¤ºè¯(<235) = 300å­—ç¬¦ï¼Œæ€»ç»“å†…å®¹ä¸¥æ ¼é™åˆ¶åœ¨150å­—ç¬¦
    if "atlas/intersync-gemma" in model:
        if prompt_type == "intermediate":
            if context.get('summary_so_far', '').strip() and context.get('summary_so_far', '').strip() != 'None':
                # æç®€æ›´æ–°æç¤ºè¯ï¼Œæ€»è®¡<150å­—ç¬¦ï¼ˆç³»ç»Ÿæç¤ºè¯65å­—ç¬¦ï¼‰
                summary = context['summary_so_far'][:60]  # 60å­—ç¬¦æ¦‚æ‹¬
                new_content = context['new_dialogue_chunk'][:50]  # 50å­—ç¬¦æ–°å†…å®¹
                return f"Old:{summary} New:{new_content} Update:"
            else:
                # æç®€åˆå§‹æç¤ºè¯ - ä¸¥æ ¼æ§åˆ¶åœ¨80å­—ç¬¦ä»¥å†…
                content = context['new_dialogue_chunk'][:70]  # 70å­—ç¬¦å†…å®¹
                return f"Summarize:{content}"
        elif prompt_type == "final":
            # æç®€æœ€ç»ˆæ¨ç†
            facts = context.get('summary_so_far', '')[:200]  # 200å­—ç¬¦äº‹å®
            return f"Evidence:{facts} Killer?"
    
    return "Standard prompt for other models"

def test_short_prompts():
    """æµ‹è¯•çœŸæ­£çš„çŸ­æç¤ºè¯"""
    print("ğŸ§ª Testing Atlas Model Short Prompts")
    print("="*50)
    
    model = "atlas/intersync-gemma-7b-instruct-function-calling:latest"
    system_prompt = "Detective. Analyze murder case. Summarize key evidence concisely."
    
    print(f"System prompt: '{system_prompt}' ({len(system_prompt)} chars)")
    
    # æ¨¡æ‹Ÿ2000 tokensçš„é•¿å¯¹è¯å†…å®¹
    long_dialogue = """Aï¼šæ˜¨æ™šæˆ‘å¬åˆ°äº†å¥‡æ€ªçš„å£°éŸ³ï¼Œå¥½åƒæ˜¯ä»æ£®æ—é‚£è¾¹ä¼ æ¥çš„ï¼Œå£°éŸ³å¾ˆè§„å¾‹ï¼Œä¸åƒæ˜¯é‡å…½å‘å‡ºçš„ã€‚Bï¼šæˆ‘ä¹Ÿå¬åˆ°äº†ï¼Œè€Œä¸”æˆ‘è¿˜çœ‹åˆ°äº†ä¸€ä¸ªé»‘å½±åœ¨è€æ©¡æ ‘é™„è¿‘ç§»åŠ¨ï¼Œé‚£ä¸ªäººåŠ¨ä½œå¾ˆå¿«ã€‚Cï¼šè¿™ç¡®å®å¾ˆå¥‡æ€ªï¼Œä¼šä¸ä¼šæ˜¯æœ‰äººåœ¨é‚£é‡Œåšä»€ä¹ˆåäº‹ï¼Ÿæ˜¨æ™šé‚£ä¹ˆæ™šäº†è¿˜æœ‰äººåœ¨å¤–é¢ç¡®å®ä¸æ­£å¸¸ã€‚Dï¼šæˆ‘è§‰å¾—å¯èƒ½æ˜¯ä¼æœ¨å·¥åœ¨å·¥ä½œï¼Œä½†è¿™ä¹ˆæ™šäº†è¿˜å·¥ä½œç¡®å®å¾ˆå¥‡æ€ªã€‚Eï¼šè€Œä¸”æˆ‘å¬è¯´å¼ ä¸‰æ˜¨å¤©ä¸¢äº†ä¸€æŠŠæ–§å¤´ï¼Œè¿™ä¼šä¸ä¼šæœ‰å…³ç³»ï¼ŸFï¼šæ–§å¤´ï¼Ÿè¿™å’Œè„šå°æœ‰ä»€ä¹ˆå…³ç³»ï¼ŸGï¼šå¦‚æœæœ‰äººå·äº†æ–§å¤´ï¼Œå¯èƒ½æ˜¯ä¸ºäº†åšåäº‹ã€‚Hï¼šä½ ä»¬è¯´å¾—å¯¹ï¼Œæˆ‘ä»¬ç¡®å®åº”è¯¥ä»”ç»†è°ƒæŸ¥ä¸€ä¸‹ã€‚Iï¼šé‚£ä¸ªé»‘å½±çš„èº«é«˜å¤§æ¦‚å¤šå°‘ï¼ŸJï¼šçœ‹èµ·æ¥æ¯”æ™®é€šäººé«˜ä¸€äº›ï¼Œå¤§æ¦‚ä¸€ç±³å…«å·¦å³ã€‚Kï¼šæ‘é‡Œç¬¦åˆè¿™ä¸ªèº«é«˜çš„äººä¸å¤šã€‚Lï¼šè€Œä¸”è¿˜è¦æœ‰ç†ç”±å»è€æ©¡æ ‘é‚£é‡Œã€‚Mï¼šè€æ©¡æ ‘é‚£é‡Œæœ€è¿‘åœ¨ç ä¼ï¼Œåªæœ‰ä¼æœ¨å·¥ä¼šå»ã€‚Nï¼šä½†æ˜¯ä¼æœ¨å·¥ä¸ä¼šåœ¨æ·±å¤œå·¥ä½œã€‚Oï¼šé™¤éæœ‰ç‰¹æ®ŠåŸå› ã€‚Pï¼šæˆ‘ä»¬éœ€è¦è°ƒæŸ¥ä¸€ä¸‹æœ€è¿‘æœ‰å“ªäº›äººå»è¿‡é‚£é‡Œã€‚Qï¼šè¿˜è¦çœ‹çœ‹è°å’Œå¼ ä¸‰æœ‰çŸ›ç›¾ã€‚Rï¼šè¿™æ ·æˆ‘ä»¬å°±èƒ½ç¼©å°å«Œç–‘äººèŒƒå›´äº†ã€‚"""
    
    print(f"Long dialogue length: {len(long_dialogue)} chars")
    
    # æµ‹è¯•åœºæ™¯1ï¼šåˆå§‹æç¤ºè¯ï¼ˆåº”è¯¥å¾ˆçŸ­ï¼‰
    context1 = {
        "new_dialogue_chunk": long_dialogue
    }
    
    prompt1 = get_prompt("intermediate", context1, model)
    total1 = len(system_prompt) + len(prompt1)
    
    print(f"\n--- åœºæ™¯1ï¼šåˆå§‹æç¤ºè¯ï¼ˆä»é•¿å¯¹è¯ä¸­æˆªå–ï¼‰ ---")
    print(f"User prompt: '{prompt1}' ({len(prompt1)} chars)")
    print(f"Total length: {total1} chars")
    print(f"âœ… User prompt â‰¤ 80 chars: {len(prompt1) <= 80}")
    print(f"âœ… Total â‰¤ 150 chars: {total1 <= 150}")
    
    # æµ‹è¯•åœºæ™¯2ï¼šæ›´æ–°æç¤ºè¯ï¼ˆä¹Ÿåº”è¯¥å¾ˆçŸ­ï¼‰
    previous_summary = "æ˜¨æ™šå¬åˆ°è§„å¾‹å£°éŸ³æ¥è‡ªæ£®æ—ï¼Œçœ‹åˆ°é»‘å½±åœ¨è€æ©¡æ ‘é™„è¿‘ç§»åŠ¨ï¼Œæ€€ç–‘æœ‰äººåšåäº‹ã€‚å¼ ä¸‰ä¸¢äº†æ–§å¤´ã€‚"  # çº¦50å­—ç¬¦
    
    context2 = {
        "summary_so_far": previous_summary,
        "new_dialogue_chunk": long_dialogue
    }
    
    prompt2 = get_prompt("intermediate", context2, model)
    total2 = len(system_prompt) + len(prompt2)
    
    print(f"\n--- åœºæ™¯2ï¼šæ›´æ–°æç¤ºè¯ï¼ˆä»é•¿å¯¹è¯ä¸­æˆªå–ï¼‰ ---")
    print(f"Previous summary: '{previous_summary}' ({len(previous_summary)} chars)")
    print(f"User prompt: '{prompt2}' ({len(prompt2)} chars)")
    print(f"Total length: {total2} chars")
    print(f"âœ… User prompt â‰¤ 150 chars: {len(prompt2) <= 150}")
    print(f"âœ… Total â‰¤ 220 chars: {total2 <= 220}")
    
    # æµ‹è¯•åœºæ™¯3ï¼šæœ€ç»ˆæ¨ç†
    final_summary = "ç»è¿‡å¤šè½®åˆ†æï¼šæ˜¨æ™šæ£®æ—ä¼ æ¥è§„å¾‹å£°éŸ³ï¼Œè€æ©¡æ ‘é™„è¿‘å‘ç°é»‘å½±ç§»åŠ¨ã€‚å¼ ä¸‰ä¸¢å¤±æ–§å¤´ï¼Œåœ°é¢æœ‰å¤§é´å­è„šå°ã€‚é»‘å½±èº«é«˜ä¸€ç±³å…«å·¦å³ã€‚æå››æ·±å¤œå¤–å‡ºï¼Œä¸å¼ ä¸‰æœ‰çŸ›ç›¾ï¼Œèº«é«˜ç¬¦åˆã€‚"  # çº¦90å­—ç¬¦
    
    context3 = {
        "summary_so_far": final_summary
    }
    
    prompt3 = get_prompt("final", context3, model)
    total3 = len(system_prompt) + len(prompt3)
    
    print(f"\n--- åœºæ™¯3ï¼šæœ€ç»ˆæ¨ç† ---")
    print(f"Final summary: '{final_summary}' ({len(final_summary)} chars)")
    print(f"User prompt: '{prompt3}' ({len(prompt3)} chars)")
    print(f"Total length: {total3} chars")
    print(f"âœ… Total â‰¤ 300 chars: {total3 <= 300}")
    
    # æ±‡æ€»æŠ¥å‘Š
    print(f"\n" + "="*50)
    print(f"ğŸ“‹ Atlas Model Short Prompts Test Summary")
    print(f"="*50)
    print(f"System prompt length: {len(system_prompt)} chars")
    print(f"")
    print(f"Prompt length targets:")
    print(f"  åˆå§‹æç¤ºè¯: â‰¤ 80 chars (user prompt)")
    print(f"  æ›´æ–°æç¤ºè¯: â‰¤ 150 chars (user prompt)")
    print(f"  æœ€ç»ˆæ¨ç†: â‰¤ 300 chars (total)")
    print(f"")
    print(f"Test results:")
    print(f"  åœºæ™¯1 (åˆå§‹): {len(prompt1)} chars user, {total1} chars total - {'âœ… PASS' if len(prompt1) <= 80 and total1 <= 150 else 'âŒ FAIL'}")
    print(f"  åœºæ™¯2 (æ›´æ–°): {len(prompt2)} chars user, {total2} chars total - {'âœ… PASS' if len(prompt2) <= 150 and total2 <= 220 else 'âŒ FAIL'}")
    print(f"  åœºæ™¯3 (æœ€ç»ˆ): {len(prompt3)} chars user, {total3} chars total - {'âœ… PASS' if total3 <= 300 else 'âŒ FAIL'}")
    
    all_pass = all([
        len(prompt1) <= 80 and total1 <= 150,
        len(prompt2) <= 150 and total2 <= 220,
        total3 <= 300
    ])
    
    print(f"")
    print(f"Overall result: {'âœ… ALL TESTS PASS' if all_pass else 'âŒ SOME TESTS FAIL'}")
    
    if all_pass:
        print(f"ğŸ¯ Atlas model short prompts optimized!")
        print(f"âœ… åˆå§‹æç¤ºè¯çœŸæ­£ç®€çŸ­ (â‰¤80å­—ç¬¦)")
        print(f"âœ… æ›´æ–°æç¤ºè¯æ§åˆ¶åˆç† (â‰¤150å­—ç¬¦)")
        print(f"âœ… é€šè¿‡æˆªæ–­é•¿å¯¹è¯å†…å®¹å®ç°çŸ­æç¤ºè¯")
        print(f"âœ… æ¯2000 tokensæ€»ç»“ä¸€æ¬¡")
        print(f"")
        print(f"ğŸ”§ Key optimizations:")
        print(f"â€¢ åˆå§‹å¯¹è¯å†…å®¹æˆªæ–­åˆ°70å­—ç¬¦")
        print(f"â€¢ æ›´æ–°æ—¶æ‘˜è¦60å­—ç¬¦+æ–°å†…å®¹50å­—ç¬¦")
        print(f"â€¢ æç®€åŒ–æç¤ºè¯æ ¼å¼")
        print(f"â€¢ é¿å…ä¸å¿…è¦çš„è¯´æ˜æ–‡å­—")
    else:
        print(f"âš ï¸ Some tests failed, need further optimization")

if __name__ == "__main__":
    test_short_prompts()
