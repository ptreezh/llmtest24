#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤±è´¥æµ‹è¯•é‡è·‘è„šæœ¬
è‡ªåŠ¨æ£€æµ‹æ— å“åº”æˆ–å¤±è´¥çš„æµ‹è¯•ï¼Œå¹¶è¿›è¡Œé‡æ–°æµ‹è¯•
"""

import os
import sys
import time
import json
from datetime import datetime
from typing import List, Dict, Tuple
import ollama

class FailedTestRetrier:
    def __init__(self):
        self.testout_dir = "testout"
        self.retry_results = {}
        
        # åŠ è½½é…ç½®
        try:
            sys.path.append(os.path.abspath('.'))
            from config import MODEL_TO_TEST
            self.model = MODEL_TO_TEST
        except ImportError:
            print("é”™è¯¯: æ— æ³•å¯¼å…¥é…ç½®æ–‡ä»¶")
            sys.exit(1)
    
    def detect_failed_tests(self) -> List[Dict]:
        """æ£€æµ‹å¤±è´¥çš„æµ‹è¯•"""
        failed_tests = []
        
        if not os.path.exists(self.testout_dir):
            print("testoutç›®å½•ä¸å­˜åœ¨")
            return failed_tests
        
        for filename in os.listdir(self.testout_dir):
            if not filename.endswith('.txt'):
                continue
                
            filepath = os.path.join(self.testout_dir, filename)
            
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆå“åº”
                lines = content.split('\n')
                response_started = False
                response_content = ""
                
                for line in lines:
                    if line.startswith("MODEL RESPONSE:"):
                        response_started = True
                        continue
                    if response_started:
                        response_content += line + "\n"
                
                response_content = response_content.strip()
                
                # åˆ¤æ–­æ˜¯å¦å¤±è´¥
                is_failed = False
                failure_reason = ""
                
                if not response_content:
                    is_failed = True
                    failure_reason = "æ— å“åº”å†…å®¹"
                elif len(response_content) < 10:
                    is_failed = True
                    failure_reason = f"å“åº”è¿‡çŸ­ ({len(response_content)}å­—ç¬¦)"
                elif "error" in response_content.lower() or "é”™è¯¯" in response_content:
                    is_failed = True
                    failure_reason = "å“åº”åŒ…å«é”™è¯¯ä¿¡æ¯"
                
                if is_failed:
                    # æå–æµ‹è¯•ä¿¡æ¯
                    case_id = ""
                    test_type = ""
                    prompt = ""
                    
                    for line in lines:
                        if line.startswith("ç”¨ä¾‹ç¼–å·:"):
                            case_id = line.split(":", 1)[1].strip()
                        elif line.startswith("ç±»å‹:"):
                            test_type = line.split(":", 1)[1].strip()
                        elif line.startswith("PROMPT:"):
                            # æå–promptå†…å®¹
                            prompt_started = False
                            for l in lines:
                                if l.startswith("PROMPT:"):
                                    prompt_started = True
                                    continue
                                elif l.startswith("MODEL RESPONSE:"):
                                    break
                                elif prompt_started:
                                    prompt += l + "\n"
                            prompt = prompt.strip()
                            break
                    
                    failed_tests.append({
                        'filename': filename,
                        'filepath': filepath,
                        'case_id': case_id,
                        'test_type': test_type,
                        'prompt': prompt,
                        'failure_reason': failure_reason,
                        'original_response': response_content
                    })
                    
            except Exception as e:
                print(f"æ£€æŸ¥æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
        
        return failed_tests
    
    def retry_single_test(self, test_info: Dict, max_retries: int = 3) -> Tuple[bool, str]:
        """é‡è¯•å•ä¸ªæµ‹è¯•"""
        print(f"\nğŸ”„ é‡è¯•æµ‹è¯•: {test_info['test_type']} ({test_info['case_id']})")
        print(f"   åŸå› : {test_info['failure_reason']}")
        
        for attempt in range(max_retries):
            try:
                print(f"   å°è¯• {attempt + 1}/{max_retries}...")
                
                # è°ƒç”¨æ¨¡å‹
                response = ollama.chat(
                    model=self.model,
                    messages=[{'role': 'user', 'content': test_info['prompt']}],
                    options={
                        'timeout': 90,  # å¢åŠ è¶…æ—¶æ—¶é—´
                        'temperature': 0.8,  # ç¨å¾®å¢åŠ éšæœºæ€§
                        'top_p': 0.9
                    }
                )
                
                content = response['message']['content']
                
                # æ£€æŸ¥å“åº”è´¨é‡
                if content and len(content.strip()) > 20:
                    print(f"   âœ… é‡è¯•æˆåŠŸ! ({len(content)}å­—ç¬¦)")
                    return True, content
                else:
                    print(f"   âš ï¸ å“åº”ä»ç„¶è¿‡çŸ­: {len(content) if content else 0}å­—ç¬¦")
                    if attempt < max_retries - 1:
                        time.sleep(3)  # çŸ­æš‚ç­‰å¾…
                        
            except Exception as e:
                print(f"   âŒ å°è¯• {attempt + 1} å¤±è´¥: {e}")
                if attempt < max_retries - 1:
                    time.sleep(5)  # ç­‰å¾…æ›´é•¿æ—¶é—´
        
        print(f"   âŒ æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†")
        return False, ""
    
    def save_retry_result(self, test_info: Dict, new_response: str):
        """ä¿å­˜é‡è¯•ç»“æœ"""
        # åˆ›å»ºå¤‡ä»½
        backup_path = test_info['filepath'] + '.backup'
        if not os.path.exists(backup_path):
            with open(test_info['filepath'], 'r', encoding='utf-8') as f:
                backup_content = f.read()
            with open(backup_path, 'w', encoding='utf-8') as f:
                f.write(backup_content)
        
        # ä¿å­˜æ–°ç»“æœ
        with open(test_info['filepath'], 'w', encoding='utf-8') as f:
            f.write(f"ç”¨ä¾‹ç¼–å·: {test_info['case_id']}\n")
            f.write(f"ç±»å‹: {test_info['test_type']}\n")
            f.write(f"PROMPT:\n{test_info['prompt']}\n\n")
            f.write(f"MODEL RESPONSE:\n{new_response}\n\n")
            f.write(f"# é‡è¯•ä¿¡æ¯\n")
            f.write(f"# é‡è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"# åŸå¤±è´¥åŸå› : {test_info['failure_reason']}\n")
        
        print(f"   ğŸ’¾ ç»“æœå·²æ›´æ–°: {test_info['filepath']}")
    
    def run_retry_process(self):
        """è¿è¡Œé‡è¯•æµç¨‹"""
        print("ğŸ” æ£€æµ‹å¤±è´¥çš„æµ‹è¯•...")
        
        failed_tests = self.detect_failed_tests()
        
        if not failed_tests:
            print("âœ… æ²¡æœ‰å‘ç°å¤±è´¥çš„æµ‹è¯•!")
            return
        
        print(f"ğŸ“‹ å‘ç° {len(failed_tests)} ä¸ªå¤±è´¥çš„æµ‹è¯•:")
        for i, test in enumerate(failed_tests, 1):
            print(f"  {i}. {test['test_type']} - {test['failure_reason']}")
        
        # è¯¢é—®ç”¨æˆ·æ˜¯å¦ç»§ç»­
        user_input = input(f"\næ˜¯å¦é‡è¯•è¿™äº›å¤±è´¥çš„æµ‹è¯•? (y/N): ").strip().lower()
        if user_input != 'y':
            print("ç”¨æˆ·å–æ¶ˆé‡è¯•")
            return
        
        print(f"\nğŸš€ å¼€å§‹é‡è¯• {len(failed_tests)} ä¸ªå¤±è´¥çš„æµ‹è¯•...")
        
        success_count = 0
        
        for test_info in failed_tests:
            success, new_response = self.retry_single_test(test_info)
            
            if success:
                self.save_retry_result(test_info, new_response)
                success_count += 1
                
                self.retry_results[test_info['filename']] = {
                    'status': 'success',
                    'original_failure': test_info['failure_reason'],
                    'new_response_length': len(new_response),
                    'retry_time': datetime.now().isoformat()
                }
            else:
                self.retry_results[test_info['filename']] = {
                    'status': 'failed',
                    'original_failure': test_info['failure_reason'],
                    'retry_time': datetime.now().isoformat()
                }
        
        # ç”Ÿæˆé‡è¯•æŠ¥å‘Š
        print(f"\n{'='*60}")
        print(f"ğŸ“Š é‡è¯•å®ŒæˆæŠ¥å‘Š")
        print(f"{'='*60}")
        print(f"é‡è¯•æµ‹è¯•æ•°: {len(failed_tests)}")
        print(f"é‡è¯•æˆåŠŸæ•°: {success_count}")
        print(f"ä»ç„¶å¤±è´¥æ•°: {len(failed_tests) - success_count}")
        print(f"é‡è¯•æˆåŠŸç‡: {success_count/len(failed_tests)*100:.1f}%")
        
        # ä¿å­˜é‡è¯•ç»“æœ
        with open('retry_results.json', 'w', encoding='utf-8') as f:
            json.dump({
                'summary': {
                    'total_retries': len(failed_tests),
                    'successful_retries': success_count,
                    'failed_retries': len(failed_tests) - success_count,
                    'retry_success_rate': success_count/len(failed_tests)*100,
                    'timestamp': datetime.now().isoformat()
                },
                'detailed_results': self.retry_results
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ é‡è¯•ç»“æœå·²ä¿å­˜: retry_results.json")
        
        if success_count > 0:
            print(f"\nğŸ¯ å»ºè®®: é‡æ–°è¿è¡Œè¯„ä»·è„šæœ¬ä»¥æ›´æ–°åˆ†æç»“æœ")
            print(f"   python evaluate_results.py")

def main():
    retrier = FailedTestRetrier()
    retrier.run_retry_process()

if __name__ == "__main__":
    main()
