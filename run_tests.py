#!/usr/bin/env python3
"""
LLMè§’è‰²ç‹¬ç«‹æ€§æµ‹è¯•æ¡†æ¶ - ä¸»è¿è¡Œè„šæœ¬

ä½¿ç”¨æ–¹æ³•:
    python run_tests.py --help                    # æŸ¥çœ‹å¸®åŠ©
    python run_tests.py --basic                   # è¿è¡ŒåŸºç¡€æµ‹è¯•
    python run_tests.py --comprehensive           # è¿è¡Œç»¼åˆæµ‹è¯•
    python run_tests.py --model gpt-4 --role software_engineer  # æŒ‡å®šæ¨¡å‹å’Œè§’è‰²
"""

import argparse
import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from testLLM.core.test_runner import TestRunner
from testLLM.core.config_manager import ConfigManager

def setup_argument_parser():
    """è®¾ç½®å‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    
    parser = argparse.ArgumentParser(
        description="LLMè§’è‰²ç‹¬ç«‹æ€§æµ‹è¯•æ¡†æ¶",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  %(prog)s --basic                                    # è¿è¡ŒåŸºç¡€æµ‹è¯•
  %(prog)s --comprehensive                            # è¿è¡Œç»¼åˆæµ‹è¯•
  %(prog)s --model gpt-4 --role software_engineer    # æµ‹è¯•ç‰¹å®šæ¨¡å‹å’Œè§’è‰²
  %(prog)s --batch --models gpt-4,gpt-3.5-turbo      # æ‰¹é‡æµ‹è¯•å¤šä¸ªæ¨¡å‹
  %(prog)s --config custom_config.yaml               # ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
        """
    )
    
    # æµ‹è¯•æ¨¡å¼é€‰æ‹©
    mode_group = parser.add_mutually_exclusive_group()
    mode_group.add_argument(
        '--basic', 
        action='store_true',
        help='è¿è¡ŒåŸºç¡€æµ‹è¯•ï¼ˆå¿«é€ŸéªŒè¯ï¼‰'
    )
    mode_group.add_argument(
        '--comprehensive', 
        action='store_true',
        help='è¿è¡Œç»¼åˆæµ‹è¯•ï¼ˆå®Œæ•´æµ‹è¯•å¥—ä»¶ï¼‰'
    )
    mode_group.add_argument(
        '--batch', 
        action='store_true',
        help='æ‰¹é‡æµ‹è¯•æ¨¡å¼'
    )
    
    # æ¨¡å‹å’Œè§’è‰²é€‰æ‹©
    parser.add_argument(
        '--model', 
        type=str,
        help='æŒ‡å®šè¦æµ‹è¯•çš„æ¨¡å‹åç§°'
    )
    parser.add_argument(
        '--models', 
        type=str,
        help='æŒ‡å®šå¤šä¸ªæ¨¡å‹ï¼ˆé€—å·åˆ†éš”ï¼‰'
    )
    parser.add_argument(
        '--role', 
        type=str,
        help='æŒ‡å®šè¦æµ‹è¯•çš„è§’è‰²åç§°'
    )
    parser.add_argument(
        '--roles', 
        type=str,
        help='æŒ‡å®šå¤šä¸ªè§’è‰²ï¼ˆé€—å·åˆ†éš”ï¼‰'
    )
    
    # æµ‹è¯•ç±»å‹é€‰æ‹©
    parser.add_argument(
        '--test-types', 
        type=str,
        default='character_breaking,implicit_cognition,longitudinal_consistency',
        help='æŒ‡å®šæµ‹è¯•ç±»å‹ï¼ˆé€—å·åˆ†éš”ï¼‰ï¼Œé»˜è®¤: character_breaking,implicit_cognition,longitudinal_consistency'
    )
    
    # é…ç½®é€‰é¡¹
    parser.add_argument(
        '--config', 
        type=str,
        default='config/test_config.yaml',
        help='é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤: config/test_config.yaml'
    )
    parser.add_argument(
        '--output', 
        type=str,
        default='results',
        help='è¾“å‡ºç›®å½•ï¼Œé»˜è®¤: results'
    )
    
    # æŠ¥å‘Šé€‰é¡¹
    parser.add_argument(
        '--formats', 
        type=str,
        default='json,html',
        help='æŠ¥å‘Šæ ¼å¼ï¼ˆé€—å·åˆ†éš”ï¼‰ï¼Œé»˜è®¤: json,html'
    )
    parser.add_argument(
        '--no-report', 
        action='store_true',
        help='ä¸ç”ŸæˆæŠ¥å‘Šï¼Œä»…æ˜¾ç¤ºç»“æœ'
    )
    
    # å…¶ä»–é€‰é¡¹
    parser.add_argument(
        '--verbose', '-v', 
        action='store_true',
        help='è¯¦ç»†è¾“å‡º'
    )
    parser.add_argument(
        '--dry-run', 
        action='store_true',
        help='è¯•è¿è¡Œï¼Œä¸å®é™…æ‰§è¡Œæµ‹è¯•'
    )
    parser.add_argument(
        '--max-attempts', 
        type=int,
        default=5,
        help='æ¯ä¸ªæµ‹è¯•çš„æœ€å¤§å°è¯•æ¬¡æ•°ï¼Œé»˜è®¤: 5'
    )
    
    return parser

def validate_arguments(args):
    """éªŒè¯å‘½ä»¤è¡Œå‚æ•°"""
    
    errors = []
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    config_path = Path(args.config)
    if not config_path.exists():
        errors.append(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
    
    # æ£€æŸ¥è¾“å‡ºç›®å½•
    output_path = Path(args.output)
    try:
        output_path.mkdir(parents=True, exist_ok=True)
    except Exception as e:
        errors.append(f"æ— æ³•åˆ›å»ºè¾“å‡ºç›®å½• {args.output}: {str(e)}")
    
    # æ£€æŸ¥æµ‹è¯•å‚æ•°
    if not any([args.basic, args.comprehensive, args.batch, args.model, args.models]):
        errors.append("å¿…é¡»æŒ‡å®šæµ‹è¯•æ¨¡å¼æˆ–å…·ä½“çš„æ¨¡å‹")
    
    if errors:
        print("âŒ å‚æ•°éªŒè¯å¤±è´¥:")
        for error in errors:
            print(f"   {error}")
        return False
    
    return True

def run_basic_test(runner, args):
    """è¿è¡ŒåŸºç¡€æµ‹è¯•"""
    
    print("ğŸ§ª è¿è¡ŒåŸºç¡€æµ‹è¯•")
    print("=" * 40)
    
    # ä½¿ç”¨é»˜è®¤çš„æµ‹è¯•é…ç½®
    models = ["gpt-3.5-turbo"]
    roles = ["software_engineer"]
    
    if args.model:
        models = [args.model]
    if args.role:
        roles = [args.role]
    
    results = {}
    
    for model in models:
        for role in roles:
            print(f"\nğŸ” æµ‹è¯•: {model} + {role}")
            
            # è¿è¡Œè§’è‰²ç ´åæµ‹è¯•
            result = runner.run_character_breaking_test(
                model_name=model,
                role_name=role,
                max_attempts=min(args.max_attempts, 3)  # åŸºç¡€æµ‹è¯•é™åˆ¶å°è¯•æ¬¡æ•°
            )
            
            if result:
                score = result.get('overall_score', 0)
                status = "âœ… é€šè¿‡" if score >= 0.7 else "âŒ å¤±è´¥"
                print(f"   ç»“æœ: {score:.3f} {status}")
                
                if model not in results:
                    results[model] = {}
                results[model][role] = {'character_breaking': result}
            else:
                print("   âŒ æµ‹è¯•å¤±è´¥")
    
    return results

def run_comprehensive_test(runner, args):
    """è¿è¡Œç»¼åˆæµ‹è¯•"""
    
    print("ğŸ”¬ è¿è¡Œç»¼åˆæµ‹è¯•")
    print("=" * 40)
    
    # ç¡®å®šæµ‹è¯•æ¨¡å‹å’Œè§’è‰²
    models = ["gpt-3.5-turbo"]
    roles = ["software_engineer", "data_scientist"]
    
    if args.models:
        models = [m.strip() for m in args.models.split(',')]
    elif args.model:
        models = [args.model]
    
    if args.roles:
        roles = [r.strip() for r in args.roles.split(',')]
    elif args.role:
        roles = [args.role]
    
    # ç¡®å®šæµ‹è¯•ç±»å‹
    test_types = [t.strip() for t in args.test_types.split(',')]
    
    print(f"ğŸ“Š æµ‹è¯•é…ç½®:")
    print(f"   æ¨¡å‹: {', '.join(models)}")
    print(f"   è§’è‰²: {', '.join(roles)}")
    print(f"   æµ‹è¯•ç±»å‹: {', '.join(test_types)}")
    
    # è¿è¡Œç»¼åˆæµ‹è¯•
    results = runner.run_comprehensive_test(
        models=models,
        roles=roles,
        test_types=test_types
    )
    
    return results

def run_batch_test(runner, args):
    """è¿è¡Œæ‰¹é‡æµ‹è¯•"""
    
    print("ğŸ“¦ è¿è¡Œæ‰¹é‡æµ‹è¯•")
    print("=" * 40)
    
    # è·å–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹å’Œè§’è‰²
    available_models = ["gpt-3.5-turbo", "gpt-4"]  # å¯ä»¥ä»é…ç½®ä¸­è¯»å–
    available_roles = ["software_engineer", "data_scientist", "product_manager"]
    
    if args.models:
        models = [m.strip() for m in args.models.split(',')]
    else:
        models = available_models
    
    if args.roles:
        roles = [r.strip() for r in args.roles.split(',')]
    else:
        roles = available_roles
    
    print(f"ğŸ“Š æ‰¹é‡æµ‹è¯•é…ç½®:")
    print(f"   æ¨¡å‹æ•°é‡: {len(models)}")
    print(f"   è§’è‰²æ•°é‡: {len(roles)}")
    print(f"   æ€»æµ‹è¯•ç»„åˆ: {len(models) * len(roles)}")
    
    # è¿è¡Œæ‰¹é‡æµ‹è¯•
    batch_results = []
    
    for i, model in enumerate(models, 1):
        for j, role in enumerate(roles, 1):
            print(f"\nğŸ” [{i}/{len(models)}][{j}/{len(roles)}] æµ‹è¯•: {model} + {role}")
            
            try:
                result = runner.run_character_breaking_test(
                    model_name=model,
                    role_name=role,
                    max_attempts=args.max_attempts
                )
                
                if result:
                    score = result.get('overall_score', 0)
                    status = "âœ…" if score >= 0.7 else "âŒ"
                    print(f"   ç»“æœ: {score:.3f} {status}")
                    
                    batch_results.append({
                        'model': model,
                        'role': role,
                        'score': score,
                        'result': result
                    })
                else:
                    print("   âŒ æµ‹è¯•å¤±è´¥")
                    
            except Exception as e:
                print(f"   âŒ æµ‹è¯•å¼‚å¸¸: {str(e)}")
    
    return batch_results

def display_results_summary(results):
    """æ˜¾ç¤ºç»“æœæ‘˜è¦"""
    
    print("\nğŸ“Š æµ‹è¯•ç»“æœæ‘˜è¦")
    print("=" * 40)
    
    if isinstance(results, dict) and 'test_results' in results:
        # ç»¼åˆæµ‹è¯•ç»“æœæ ¼å¼
        total_tests = 0
        passed_tests = 0
        
        for model_name, model_results in results['test_results'].items():
            print(f"\nğŸ¤– æ¨¡å‹: {model_name}")
            
            for role_name, role_results in model_results.items():
                print(f"   ğŸ‘¤ è§’è‰²: {role_name}")
                
                for test_type, test_result in role_results.items():
                    if isinstance(test_result, dict) and 'overall_score' in test_result:
                        score = test_result['overall_score']
                        status = "âœ… é€šè¿‡" if score >= 0.7 else "âŒ å¤±è´¥"
                        print(f"      ğŸ§ª {test_type}: {score:.3f} {status}")
                        
                        total_tests += 1
                        if score >= 0.7:
                            passed_tests += 1
        
        if total_tests > 0:
            pass_rate = (passed_tests / total_tests) * 100
            print(f"\nğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
            print(f"   æ€»æµ‹è¯•æ•°: {total_tests}")
            print(f"   é€šè¿‡æ•°: {passed_tests}")
            print(f"   é€šè¿‡ç‡: {pass_rate:.1f}%")
    
    elif isinstance(results, list):
        # æ‰¹é‡æµ‹è¯•ç»“æœæ ¼å¼
        total_tests = len(results)
        passed_tests = sum(1 for r in results if r.get('score', 0) >= 0.7)
        
        if total_tests > 0:
            pass_rate = (passed_tests / total_tests) * 100
            avg_score = sum(r.get('score', 0) for r in results) / total_tests
            
            print(f"ğŸ“ˆ æ‰¹é‡æµ‹è¯•ç»Ÿè®¡:")
            print(f"   æ€»æµ‹è¯•æ•°: {total_tests}")
            print(f"   é€šè¿‡æ•°: {passed_tests}")
            print(f"   é€šè¿‡ç‡: {pass_rate:.1f}%")
            print(f"   å¹³å‡åˆ†æ•°: {avg_score:.3f}")

def main():
    """ä¸»å‡½æ•°"""
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = setup_argument_parser()
    args = parser.parse_args()
    
    # æ˜¾ç¤ºæ ‡é¢˜
    print("ğŸš€ LLMè§’è‰²ç‹¬ç«‹æ€§æµ‹è¯•æ¡†æ¶")
    print("=" * 50)
    
    # éªŒè¯å‚æ•°
    if not validate_arguments(args):
        return 1
    
    # è¯•è¿è¡Œæ¨¡å¼
    if args.dry_run:
        print("ğŸ” è¯•è¿è¡Œæ¨¡å¼ - ä¸ä¼šå®é™…æ‰§è¡Œæµ‹è¯•")
        print(f"   é…ç½®æ–‡ä»¶: {args.config}")
        print(f"   è¾“å‡ºç›®å½•: {args.output}")
        print(f"   æŠ¥å‘Šæ ¼å¼: {args.formats}")
        return 0
    
    try:
        # åˆå§‹åŒ–é…ç½®å’Œæµ‹è¯•è¿è¡Œå™¨
        print("ğŸ“‹ æ­£åœ¨åŠ è½½é…ç½®...")
        config = ConfigManager(args.config)
        
        print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–æµ‹è¯•è¿è¡Œå™¨...")
        runner = TestRunner(config)
        
        # æ ¹æ®æ¨¡å¼è¿è¡Œæµ‹è¯•
        results = None
        
        if args.basic:
            results = run_basic_test(runner, args)
        elif args.comprehensive:
            results = run_comprehensive_test(runner, args)
        elif args.batch:
            results = run_batch_test(runner, args)
        elif args.model or args.models:
            # å•ç‹¬æŒ‡å®šæ¨¡å‹çš„æƒ…å†µï¼Œè¿è¡Œç»¼åˆæµ‹è¯•
            results = run_comprehensive_test(runner, args)
        
        # æ˜¾ç¤ºç»“æœæ‘˜è¦
        if results:
            display_results_summary(results)
            
            # ç”ŸæˆæŠ¥å‘Š
            if not args.no_report:
                print("\nğŸ“„ æ­£åœ¨ç”ŸæˆæŠ¥å‘Š...")
                
                formats = [f.strip() for f in args.formats.split(',')]
                
                if isinstance(results, dict):
                    report_files = runner.generate_report(results, args.output, formats)
                else:
                    # æ‰¹é‡æµ‹è¯•ç»“æœéœ€è¦è½¬æ¢æ ¼å¼
                    formatted_results = {
                        'session_id': f"batch_test_{int(__import__('time').time())}",
                        'timestamp': __import__('time').strftime("%Y-%m-%d %H:%M:%S"),
                        'test_results': {},
                        'batch_results': results
                    }
                    report_files = runner.generate_report(formatted_results, args.output, formats)
                
                print("âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ:")
                for format_type, file_path in report_files.items():
                    print(f"   ğŸ“ {format_type.upper()}: {file_path}")
        
        print("\nğŸ‰ æµ‹è¯•å®Œæˆ!")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
        return 1
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())