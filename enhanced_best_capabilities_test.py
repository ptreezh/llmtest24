#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºæµ‹è¯•æ¡†æ¶ - ä¸“é—¨æµ‹è¯•è¡¨ç°æœ€ä½³çš„ä¸‰é¡¹èƒ½åŠ›
æ¶Œç°åˆ†æã€æ•°å­¦æ¨ç†ã€è§’è‰²æ‰®æ¼”
"""

import os
import sys
import time
import json
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import ollama

# å¯¼å…¥å¢å¼ºæµ‹è¯•æ¡†æ¶
sys.path.append(os.path.abspath('.'))
from enhanced_test_framework import EnhancedTestFramework

class BestCapabilitiesTestSuite(EnhancedTestFramework):
    def __init__(self):
        super().__init__()
        self.capability_stats = {
            'emergence': {'total': 0, 'success': 0, 'zero_responses': 0},
            'math': {'total': 0, 'success': 0, 'zero_responses': 0},
            'persona': {'total': 0, 'success': 0, 'zero_responses': 0}
        }
    
    def test_emergence_analysis(self):
        """æµ‹è¯•æ¶Œç°åˆ†æèƒ½åŠ›"""
        print("ğŸ§  æµ‹è¯•èƒ½åŠ›1: æ¶Œç°åˆ†æ (Emergence Analysis)")
        print("="*60)
        
        emergence_tests = [
            {
                "name": "ç”¨æˆ·å¼•å¯¼æµç¨‹åé¦ˆåˆ†æ",
                "prompt": """ä½œä¸ºAIé¡¹ç›®ç»ç†ï¼Œä½ æ”¶åˆ°äº†ä¸¤æ¡å…³äºäº§å“æ–°ç”¨æˆ·å¼•å¯¼æµç¨‹çš„åé¦ˆï¼š
                
åé¦ˆAï¼ˆæ•°æ®åˆ†æå¸ˆï¼‰ï¼š"æ–°ç”¨æˆ·å¼•å¯¼æµç¨‹çš„å®Œæˆç‡è¾¾åˆ°85%ï¼Œç”¨æˆ·å¹³å‡åœç•™æ—¶é—´å¢åŠ äº†30%ã€‚"
åé¦ˆBï¼ˆç”¨æˆ·è®¿è°ˆï¼‰ï¼š"å¤šä½æ–°ç”¨æˆ·åæ˜ å¼•å¯¼æµç¨‹è¿‡äºå¤æ‚ï¼Œæ­¥éª¤å¤ªå¤šï¼Œå¸Œæœ›èƒ½å¤Ÿç®€åŒ–ã€‚"

è¯·åˆ†æè¿™ä¸¤æ¡åé¦ˆä¹‹é—´çš„å†²çªï¼Œå¹¶æå‡ºåˆ›æ–°çš„è§£å†³æ–¹æ¡ˆã€‚""",
                "complexity_level": 3
            },
            {
                "name": "äº§å“åŠŸèƒ½å†²çªåˆ†æ",
                "prompt": """è¯·åˆ†æä»¥ä¸‹ä¸¤æ¡äº§å“åé¦ˆä¸­çš„æ·±å±‚çŸ›ç›¾ï¼š

åé¦ˆAï¼š"æ–°åŠŸèƒ½ä¸Šçº¿åï¼Œç”¨æˆ·æ´»è·ƒåº¦æå‡äº†25%ï¼Œæ—¥å‡ä½¿ç”¨æ—¶é•¿å¢åŠ ã€‚"
åé¦ˆBï¼š"éƒ¨åˆ†è€ç”¨æˆ·è¡¨ç¤ºæ–°åŠŸèƒ½éš¾ä»¥ä¸Šæ‰‹ï¼Œè€ƒè™‘è½¬å‘ç«äº‰å¯¹æ‰‹äº§å“ã€‚"

è¯·ä»ç”¨æˆ·ä½“éªŒã€äº§å“ç­–ç•¥ã€å•†ä¸šä»·å€¼ä¸‰ä¸ªç»´åº¦è¿›è¡Œæ¶Œç°åˆ†æï¼Œæ‰¾å‡ºæ½œåœ¨çš„åˆ›æ–°æœºä¼šã€‚""",
                "complexity_level": 4
            },
            {
                "name": "å¤šç»´åº¦å†²çªæ•´åˆ",
                "prompt": """ä½œä¸ºæˆ˜ç•¥é¡¾é—®ï¼Œè¯·åˆ†æä»¥ä¸‹ä¸‰ä¸ªç»´åº¦çš„å†²çªå¹¶æå‡ºæ•´åˆæ–¹æ¡ˆï¼š

æŠ€æœ¯ç»´åº¦ï¼š"AIç®—æ³•å‡†ç¡®ç‡è¾¾åˆ°95%ï¼Œä½†è®¡ç®—èµ„æºæ¶ˆè€—å¢åŠ 40%ã€‚"
å•†ä¸šç»´åº¦ï¼š"å®¢æˆ·æ„¿æ„ä¸ºé«˜å‡†ç¡®ç‡ä»˜è´¹ï¼Œä½†æˆæœ¬ä¸Šå‡å½±å“åˆ©æ¶¦ç‡ã€‚"
ç”¨æˆ·ç»´åº¦ï¼š"ç”¨æˆ·æœŸæœ›å¿«é€Ÿå“åº”ï¼Œä½†é«˜å‡†ç¡®ç‡éœ€è¦æ›´å¤šå¤„ç†æ—¶é—´ã€‚"

è¯·è¿›è¡Œæ¶Œç°åˆ†æï¼Œæ‰¾å‡ºä¸‰è€…å¹³è¡¡çš„åˆ›æ–°è§£å†³æ–¹æ¡ˆã€‚""",
                "complexity_level": 5
            }
        ]
        
        emergence_results = []
        
        for i, test in enumerate(emergence_tests, 1):
            print(f"\nğŸ” æ¶Œç°æµ‹è¯• {i}: {test['name']}")
            print(f"å¤æ‚åº¦çº§åˆ«: {test['complexity_level']}")
            
            self.capability_stats['emergence']['total'] += 1
            
            # æ ¹æ®å¤æ‚åº¦è°ƒæ•´ç­–ç•¥
            if test['complexity_level'] >= 4:
                # ä½¿ç”¨æ¸è¿›å¼æ–¹æ³•
                progressive_prompts = self._decompose_emergence_prompt(test['prompt'])
                result = self.progressive_complexity_test(test['name'], progressive_prompts)
                success = result['successful_levels'] == len(progressive_prompts)
            else:
                # ç›´æ¥æµ‹è¯•
                success, response, metadata = self.smart_chat(test['prompt'])
            
            if success:
                self.capability_stats['emergence']['success'] += 1
                print(f"  âœ… æˆåŠŸ")
            else:
                print(f"  âŒ å¤±è´¥")
                if 'zero_responses' in str(metadata):
                    self.capability_stats['emergence']['zero_responses'] += 1
            
            emergence_results.append({
                'test_name': test['name'],
                'complexity_level': test['complexity_level'],
                'success': success,
                'strategy_used': 'progressive' if test['complexity_level'] >= 4 else 'direct'
            })
        
        return emergence_results
    
    def test_math_reasoning(self):
        """æµ‹è¯•æ•°å­¦æ¨ç†èƒ½åŠ›"""
        print("\nğŸ”¢ æµ‹è¯•èƒ½åŠ›2: æ•°å­¦æ¨ç† (Mathematical Reasoning)")
        print("="*60)
        
        math_tests = [
            {
                "name": "åŸºç¡€å·¥ç¨‹é—®é¢˜",
                "prompt": """ä¸€ä¸ªæ°´æ± æœ‰ç”²ã€ä¹™ä¸¤ä¸ªè¿›æ°´ç®¡ã€‚å•å¼€ç”²ç®¡ï¼Œ3å°æ—¶å¯ä»¥æ³¨æ»¡æ°´æ± ï¼›å•å¼€ä¹™ç®¡ï¼Œ5å°æ—¶å¯ä»¥æ³¨æ»¡æ°´æ± ã€‚
ç°åœ¨ï¼Œä¸¤ä¸ªæ°´ç®¡åŒæ—¶å¼€å¯ï¼Œè¯·é—®éœ€è¦å¤šä¹…æ‰èƒ½å°†æ°´æ± æ³¨æ»¡ï¼Ÿ

è¯·ç»™å‡ºè¯¦ç»†çš„è®¡ç®—è¿‡ç¨‹ï¼ŒåŒ…æ‹¬ï¼š
1. é—®é¢˜åˆ†æ
2. è®¾å®šå˜é‡
3. å»ºç«‹æ–¹ç¨‹
4. æ±‚è§£è¿‡ç¨‹
5. éªŒè¯ç­”æ¡ˆ""",
                "expected_answer": "1.875å°æ—¶æˆ–15/8å°æ—¶"
            },
            {
                "name": "å¤æ‚å·¥ç¨‹é—®é¢˜",
                "prompt": """ä¸€ä¸ªæ°´æ± æœ‰ç”²ã€ä¹™ã€ä¸™ä¸‰ä¸ªè¿›æ°´ç®¡ï¼Œåˆ†åˆ«å•ç‹¬æ³¨æ»¡æ°´æ± éœ€2ã€3ã€6å°æ—¶ã€‚
åŒæ—¶è¿˜æœ‰ä¸€ä¸ªæ’æ°´ç®¡ï¼Œå•ç‹¬å¼€å¯å¯ä»¥åœ¨4å°æ—¶å†…æ’ç©ºæ»¡æ± çš„æ°´ã€‚

å¦‚æœå››ä¸ªç®¡é“åŒæ—¶å¼€å¯ï¼Œé—®å¤šä¹…å¯ä»¥æ³¨æ»¡æ°´æ± ï¼Ÿè¯·ç»™å‡ºå®Œæ•´çš„è§£é¢˜è¿‡ç¨‹ã€‚""",
                "expected_answer": "12/7å°æ—¶"
            },
            {
                "name": "åº”ç”¨æ•°å­¦é—®é¢˜",
                "prompt": """æŸå…¬å¸ç”Ÿäº§æ•ˆç‡åˆ†æï¼š
- ç”Ÿäº§çº¿Aï¼šæ¯å°æ—¶ç”Ÿäº§100ä¸ªäº§å“ï¼Œè¿è¡Œæˆæœ¬50å…ƒ/å°æ—¶
- ç”Ÿäº§çº¿Bï¼šæ¯å°æ—¶ç”Ÿäº§80ä¸ªäº§å“ï¼Œè¿è¡Œæˆæœ¬30å…ƒ/å°æ—¶
- ç”Ÿäº§çº¿Cï¼šæ¯å°æ—¶ç”Ÿäº§120ä¸ªäº§å“ï¼Œè¿è¡Œæˆæœ¬70å…ƒ/å°æ—¶

ç°åœ¨éœ€è¦åœ¨8å°æ—¶å†…ç”Ÿäº§2000ä¸ªäº§å“ï¼Œä¸”æ€»æˆæœ¬ä¸è¶…è¿‡400å…ƒã€‚
è¯·è®¾è®¡æœ€ä¼˜çš„ç”Ÿäº§æ–¹æ¡ˆï¼Œå¹¶è®¡ç®—å…·ä½“çš„è¿è¡Œæ—¶é—´åˆ†é…ã€‚""",
                "expected_answer": "éœ€è¦ä¼˜åŒ–è®¡ç®—"
            }
        ]
        
        math_results = []
        
        for i, test in enumerate(math_tests, 1):
            print(f"\nğŸ§® æ•°å­¦æµ‹è¯• {i}: {test['name']}")
            
            self.capability_stats['math']['total'] += 1
            
            success, response, metadata = self.smart_chat(test['prompt'])
            
            if success:
                # æ£€æŸ¥ç­”æ¡ˆè´¨é‡
                has_process = any(keyword in response for keyword in ['è®¡ç®—', 'è¿‡ç¨‹', 'æ­¥éª¤', 'åˆ†æ', 'è®¾å®š'])
                has_answer = len(response) > 100  # åŸºæœ¬é•¿åº¦æ£€æŸ¥
                
                if has_process and has_answer:
                    self.capability_stats['math']['success'] += 1
                    print(f"  âœ… æˆåŠŸ - åŒ…å«è¯¦ç»†è¿‡ç¨‹")
                    quality = "high"
                else:
                    print(f"  âš ï¸ éƒ¨åˆ†æˆåŠŸ - ç¼ºå°‘è¯¦ç»†è¿‡ç¨‹")
                    quality = "medium"
            else:
                print(f"  âŒ å¤±è´¥")
                if not response:
                    self.capability_stats['math']['zero_responses'] += 1
                quality = "low"
            
            math_results.append({
                'test_name': test['name'],
                'success': success,
                'quality': quality,
                'response_length': len(response) if response else 0,
                'metadata': metadata
            })
        
        return math_results
    
    def test_persona_consistency(self):
        """æµ‹è¯•è§’è‰²æ‰®æ¼”ä¸€è‡´æ€§"""
        print("\nğŸ­ æµ‹è¯•èƒ½åŠ›3: è§’è‰²æ‰®æ¼” (Persona Consistency)")
        print("="*60)
        
        # å¤šè½®è§’è‰²æ‰®æ¼”æµ‹è¯•
        persona_scenarios = [
            {
                "name": "èµ›åšæœ‹å…‹çŒ«å’ª",
                "setup_prompt": "ä»ç°åœ¨å¼€å§‹ï¼Œä½ æ˜¯ä¸€åªç”Ÿæ´»åœ¨èµ›åšæœ‹å…‹åŸå¸‚é‡Œçš„çŒ«ï¼Œæ‹¥æœ‰åŸºç¡€çš„ç”µå­è„‘æ¥å£ï¼Œèƒ½ç†è§£äººç±»è¯­è¨€ã€‚è¯·æè¿°ä¸€ä¸‹ä½ çœ¼ä¸­çš„ä¸–ç•Œã€‚",
                "test_prompts": [
                    "ä½ æœ€å–œæ¬¢åƒä»€ä¹ˆï¼Ÿ",
                    "ä½ ä¸€å¤©çš„ç”Ÿæ´»æ˜¯æ€æ ·çš„ï¼Ÿ",
                    "ä½ å¦‚ä½•çœ‹å¾…äººç±»ï¼Ÿ"
                ]
            },
            {
                "name": "æ•°æ®åˆ†æå¸ˆä¸“å®¶",
                "setup_prompt": "ä½ ç°åœ¨æ˜¯ä¸€åèµ„æ·±æ•°æ®åˆ†æå¸ˆï¼Œæœ‰10å¹´çš„è¡Œä¸šç»éªŒï¼Œä¸“ç²¾äºç”¨æˆ·è¡Œä¸ºåˆ†æå’Œå•†ä¸šæ™ºèƒ½ã€‚è¯·ä»‹ç»ä¸€ä¸‹ä½ çš„ä¸“ä¸šèƒŒæ™¯ã€‚",
                "test_prompts": [
                    "è¯·ä¸ºä¸€ä¸ªç”µå•†å¹³å°è®¾è®¡ç”¨æˆ·ç•™å­˜ç‡åˆ†ææ–¹æ¡ˆã€‚",
                    "å¦‚ä½•è¯„ä¼°A/Bæµ‹è¯•çš„æ•ˆæœï¼Ÿ",
                    "é¢å¯¹æ•°æ®è´¨é‡é—®é¢˜ï¼Œä½ é€šå¸¸å¦‚ä½•å¤„ç†ï¼Ÿ"
                ]
            }
        ]
        
        persona_results = []
        
        for scenario in persona_scenarios:
            print(f"\nğŸª è§’è‰²æµ‹è¯•: {scenario['name']}")
            
            # åˆå§‹åŒ–è§’è‰²
            context = []
            setup_success, setup_response, setup_metadata = self.smart_chat(scenario['setup_prompt'])
            
            if not setup_success:
                print(f"  âŒ è§’è‰²è®¾å®šå¤±è´¥")
                continue
            
            print(f"  âœ… è§’è‰²è®¾å®šæˆåŠŸ")
            context.append({'role': 'user', 'content': scenario['setup_prompt']})
            context.append({'role': 'assistant', 'content': setup_response})
            
            # æµ‹è¯•è§’è‰²ä¸€è‡´æ€§
            consistency_scores = []
            
            for i, test_prompt in enumerate(scenario['test_prompts'], 1):
                print(f"    è½®æ¬¡ {i}: ", end="")
                
                self.capability_stats['persona']['total'] += 1
                
                success, response, metadata = self.smart_chat(test_prompt, context)
                
                if success:
                    # æ£€æŸ¥è§’è‰²ä¸€è‡´æ€§
                    consistency_score = self._evaluate_persona_consistency(
                        scenario['name'], response, setup_response
                    )
                    consistency_scores.append(consistency_score)
                    
                    if consistency_score >= 0.7:
                        self.capability_stats['persona']['success'] += 1
                        print(f"âœ… ä¸€è‡´æ€§è‰¯å¥½ ({consistency_score:.2f})")
                    else:
                        print(f"âš ï¸ ä¸€è‡´æ€§ä¸€èˆ¬ ({consistency_score:.2f})")
                    
                    # æ›´æ–°ä¸Šä¸‹æ–‡
                    context.append({'role': 'user', 'content': test_prompt})
                    context.append({'role': 'assistant', 'content': response})
                else:
                    print(f"âŒ å¤±è´¥")
                    if not response:
                        self.capability_stats['persona']['zero_responses'] += 1
                    consistency_scores.append(0.0)
                    break
            
            avg_consistency = sum(consistency_scores) / len(consistency_scores) if consistency_scores else 0
            
            persona_results.append({
                'scenario_name': scenario['name'],
                'setup_success': setup_success,
                'rounds_completed': len(consistency_scores),
                'average_consistency': avg_consistency,
                'consistency_scores': consistency_scores
            })
        
        return persona_results
    
    def _decompose_emergence_prompt(self, complex_prompt: str) -> List[str]:
        """å°†å¤æ‚çš„æ¶Œç°åˆ†æé—®é¢˜åˆ†è§£ä¸ºæ¸è¿›å¼æç¤º"""
        return [
            "è¯·å…ˆè¯†åˆ«å’Œæ€»ç»“ç»™å®šåé¦ˆä¸­çš„å…³é”®ä¿¡æ¯ç‚¹ã€‚",
            "åŸºäºä¸Šè¿°ä¿¡æ¯ï¼Œè¯·åˆ†æå…¶ä¸­å­˜åœ¨çš„å†²çªæˆ–çŸ›ç›¾ã€‚",
            "ç»“åˆå‰é¢çš„åˆ†æï¼Œè¯·æå‡ºåˆ›æ–°çš„è§£å†³æ–¹æ¡ˆã€‚"
        ]
    
    def _evaluate_persona_consistency(self, persona_type: str, current_response: str, setup_response: str) -> float:
        """è¯„ä¼°è§’è‰²ä¸€è‡´æ€§å¾—åˆ†"""
        # ç®€åŒ–çš„ä¸€è‡´æ€§è¯„ä¼°
        if "èµ›åšæœ‹å…‹çŒ«å’ª" in persona_type:
            cat_keywords = ['çŒ«', 'å–µ', 'çˆªå­', 'å°¾å·´', 'ç”µå­', 'èµ›åš', 'åŸå¸‚']
            score = sum(1 for keyword in cat_keywords if keyword in current_response) / len(cat_keywords)
        elif "æ•°æ®åˆ†æå¸ˆ" in persona_type:
            analyst_keywords = ['æ•°æ®', 'åˆ†æ', 'æŒ‡æ ‡', 'ç”¨æˆ·', 'ä¸šåŠ¡', 'ç»Ÿè®¡', 'æ¨¡å‹']
            score = sum(1 for keyword in analyst_keywords if keyword in current_response) / len(analyst_keywords)
        else:
            score = 0.5  # é»˜è®¤åˆ†æ•°
        
        return min(score, 1.0)
    
    def run_best_capabilities_test(self):
        """è¿è¡Œæœ€ä½³èƒ½åŠ›æµ‹è¯•å¥—ä»¶"""
        print("ğŸŒŸ æœ€ä½³èƒ½åŠ›å¢å¼ºæµ‹è¯•å¥—ä»¶")
        print("="*80)
        print(f"æ¨¡å‹: {self.model}")
        print(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ä¼˜åŒ–å‚æ•°: é•¿åº¦é™åˆ¶{self.max_prompt_length}å­—ç¬¦, å¤æ‚åº¦é™åˆ¶Level{self.max_complexity_level}")
        print()
        
        # è¿è¡Œä¸‰é¡¹èƒ½åŠ›æµ‹è¯•
        emergence_results = self.test_emergence_analysis()
        math_results = self.test_math_reasoning()
        persona_results = self.test_persona_consistency()
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        self._generate_capabilities_report(emergence_results, math_results, persona_results)
    
    def _generate_capabilities_report(self, emergence_results, math_results, persona_results):
        """ç”Ÿæˆèƒ½åŠ›æµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ğŸ“Š æœ€ä½³èƒ½åŠ›æµ‹è¯•æŠ¥å‘Š")
        print("="*80)
        
        # æ€»ä½“ç»Ÿè®¡
        total_tests = sum(stats['total'] for stats in self.capability_stats.values())
        total_success = sum(stats['success'] for stats in self.capability_stats.values())
        total_zero = sum(stats['zero_responses'] for stats in self.capability_stats.values())
        
        overall_success_rate = (total_success / max(total_tests, 1)) * 100
        overall_zero_rate = (total_zero / max(total_tests, 1)) * 100
        
        print(f"ğŸ“ˆ æ€»ä½“è¡¨ç°:")
        print(f"  æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"  æˆåŠŸæµ‹è¯•: {total_success}")
        print(f"  é›¶å“åº”æ•°: {total_zero}")
        print(f"  æ€»æˆåŠŸç‡: {overall_success_rate:.1f}%")
        print(f"  é›¶å“åº”ç‡: {overall_zero_rate:.1f}%")
        
        # å„èƒ½åŠ›è¯¦ç»†è¡¨ç°
        print(f"\nğŸ¯ å„èƒ½åŠ›è¯¦ç»†è¡¨ç°:")
        
        capabilities = [
            ('æ¶Œç°åˆ†æ', 'emergence', emergence_results),
            ('æ•°å­¦æ¨ç†', 'math', math_results),
            ('è§’è‰²æ‰®æ¼”', 'persona', persona_results)
        ]
        
        for cap_name, cap_key, results in capabilities:
            stats = self.capability_stats[cap_key]
            success_rate = (stats['success'] / max(stats['total'], 1)) * 100
            zero_rate = (stats['zero_responses'] / max(stats['total'], 1)) * 100
            
            print(f"\n  {cap_name}:")
            print(f"    æµ‹è¯•æ•°é‡: {stats['total']}")
            print(f"    æˆåŠŸç‡: {success_rate:.1f}%")
            print(f"    é›¶å“åº”ç‡: {zero_rate:.1f}%")
            
            # ç‰¹æ®ŠæŒ‡æ ‡
            if cap_key == 'persona' and persona_results:
                avg_consistency = sum(r['average_consistency'] for r in persona_results) / len(persona_results)
                print(f"    å¹³å‡ä¸€è‡´æ€§: {avg_consistency:.2f}")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_data = {
            'timestamp': datetime.now().isoformat(),
            'model': self.model,
            'optimization_params': {
                'max_prompt_length': self.max_prompt_length,
                'max_complexity_level': self.max_complexity_level,
                'retry_attempts': self.retry_attempts
            },
            'overall_stats': {
                'total_tests': total_tests,
                'total_success': total_success,
                'total_zero_responses': total_zero,
                'overall_success_rate': overall_success_rate,
                'overall_zero_rate': overall_zero_rate
            },
            'capability_stats': self.capability_stats,
            'detailed_results': {
                'emergence': emergence_results,
                'math': math_results,
                'persona': persona_results
            }
        }
        
        with open('best_capabilities_test_report.json', 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ è¯¦ç»†æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: best_capabilities_test_report.json")
        print(f"âœ… æœ€ä½³èƒ½åŠ›æµ‹è¯•å®Œæˆï¼")

def main():
    test_suite = BestCapabilitiesTestSuite()
    test_suite.run_best_capabilities_test()

if __name__ == "__main__":
    main()
