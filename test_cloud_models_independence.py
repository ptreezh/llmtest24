#!/usr/bin/env python3
"""
äº‘æ¨¡å‹è§’è‰²ç‹¬ç«‹æ€§æµ‹è¯• - ä¼˜å…ˆæµ‹è¯•äº‘APIæ¨¡å‹
"""

import sys
import os
import time
import json
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from tests.test_pillar_25_independence import run_independence_test

# äº‘æ¨¡å‹åˆ—è¡¨ - æŒ‰ä¼˜å…ˆçº§æ’åº
CLOUD_MODELS_TO_TEST = [
    # SiliconFlow API æ¨¡å‹
    'siliconflow/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B',
    'siliconflow/THUDM/GLM-Z1-9B-0414', 
    'siliconflow/Qwen/Qwen2.5-7B-Instruct',
    'siliconflow/THUDM/GLM-4-9B-0414',
    'siliconflow/Qwen/Qwen3-8B',
    'siliconflow/internlm/internlm2_5-7b-chat',
    'siliconflow/THUDM/glm-4-9b-chat',
    
    # å¤šå¹³å°æ¨¡å‹ - ä½¿ç”¨autoå‰ç¼€è‡ªåŠ¨è½®è¯¢
    'auto/llama-3-8b-instruct',  # llama3:instruct å¯¹åº”
    'auto/phi-3-mini-instruct',   # phi3:mini å¯¹åº”
    'auto/yi-1.5-6b-chat',       # yi:6b å¯¹åº”
    'auto/gemma-7b-it',          # gemma å¯¹åº”
    'auto/granite-3b-code-instruct', # granite-code:3b å¯¹åº”
    'auto/Mistral-Nemo-12B-instruct', # mistral-nemo:latest å¯¹åº”
    
    # å¤‡ç”¨ï¼šç›´æ¥æŒ‡å®šå¹³å°
    'groq/llama-3-8b-instruct',
    'together/phi-3-mini-instruct',
    'openrouter/yi-1.5-6b-chat',
]

def test_cloud_model(model_name: str) -> dict:
    """æµ‹è¯•å•ä¸ªäº‘æ¨¡å‹"""
    print(f"\n{'='*80}")
    print(f"â˜ï¸  å¼€å§‹æµ‹è¯•äº‘æ¨¡å‹: {model_name}")
    print(f"{'='*80}")
    
    start_time = time.time()
    
    try:
        # è¿è¡Œè§’è‰²ç‹¬ç«‹æ€§æµ‹è¯•
        results = run_independence_test(model_name=model_name)
        
        end_time = time.time()
        test_duration = end_time - start_time
        
        # æ·»åŠ æµ‹è¯•æ—¶é•¿ä¿¡æ¯
        results['test_duration_seconds'] = test_duration
        results['test_duration_formatted'] = f"{test_duration/60:.1f}åˆ†é’Ÿ"
        
        # è·å–ç»¼åˆè¯„åˆ†
        overall_score = results.get('overall_scores', {}).get('overall_independence', 0.0)
        grade = results.get('summary', {}).get('grade', 'Unknown')
        
        print(f"\nâœ… äº‘æ¨¡å‹ {model_name} æµ‹è¯•å®Œæˆ!")
        print(f"ğŸ“Š ç»¼åˆå¾—åˆ†: {overall_score:.3f}")
        print(f"ğŸ† è¯„çº§: {grade}")
        print(f"â±ï¸  æµ‹è¯•æ—¶é•¿: {test_duration/60:.1f}åˆ†é’Ÿ")
        
        return {
            'model_name': model_name,
            'status': 'success',
            'overall_score': overall_score,
            'grade': grade,
            'test_duration': test_duration,
            'results': results
        }
        
    except Exception as e:
        end_time = time.time()
        test_duration = end_time - start_time
        
        error_msg = str(e)
        print(f"\nâŒ äº‘æ¨¡å‹ {model_name} æµ‹è¯•å¤±è´¥!")
        print(f"ğŸ”¥ é”™è¯¯ä¿¡æ¯: {error_msg}")
        print(f"â±ï¸  å¤±è´¥æ—¶é•¿: {test_duration/60:.1f}åˆ†é’Ÿ")
        
        return {
            'model_name': model_name,
            'status': 'failed',
            'error': error_msg,
            'test_duration': test_duration,
            'results': None
        }

def main():
    """ä¸»å‡½æ•° - æ‰¹é‡æµ‹è¯•äº‘æ¨¡å‹"""
    print(f"â˜ï¸  äº‘æ¨¡å‹è§’è‰²ç‹¬ç«‹æ€§æµ‹è¯•")
    print(f"ğŸ“‹ å¾…æµ‹è¯•äº‘æ¨¡å‹æ•°é‡: {len(CLOUD_MODELS_TO_TEST)}")
    print(f"â° é¢„è®¡æ€»æ—¶é•¿: {len(CLOUD_MODELS_TO_TEST) * 8}åˆ†é’Ÿ")
    
    # åˆ›å»ºç»“æœç›®å½•
    results_dir = Path("testout/cloud_results")
    results_dir.mkdir(parents=True, exist_ok=True)
    
    # æ‰¹é‡æµ‹è¯•ç»“æœ
    batch_results = {
        'test_type': 'cloud_models_independence',
        'start_time': time.strftime('%Y-%m-%d %H:%M:%S'),
        'total_models': len(CLOUD_MODELS_TO_TEST),
        'model_results': [],
        'summary': {}
    }
    
    successful_tests = 0
    failed_tests = 0
    total_start_time = time.time()
    
    # é€ä¸ªæµ‹è¯•äº‘æ¨¡å‹
    for i, model_name in enumerate(CLOUD_MODELS_TO_TEST, 1):
        print(f"\n{'â˜ï¸ ' * 15}")
        print(f"è¿›åº¦: {i}/{len(CLOUD_MODELS_TO_TEST)} - {model_name}")
        print(f"{'â˜ï¸ ' * 15}")
        
        # æµ‹è¯•å•ä¸ªäº‘æ¨¡å‹
        model_result = test_cloud_model(model_name)
        batch_results['model_results'].append(model_result)
        
        # ç»Ÿè®¡ç»“æœ
        if model_result['status'] == 'success':
            successful_tests += 1
        else:
            failed_tests += 1
        
        # ä¿å­˜ä¸­é—´ç»“æœ
        intermediate_file = results_dir / f"cloud_results_progress_{i}.json"
        with open(intermediate_file, 'w', encoding='utf-8') as f:
            json.dump(batch_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“ˆ å½“å‰è¿›åº¦ç»Ÿè®¡:")
        print(f"   âœ… æˆåŠŸ: {successful_tests}")
        print(f"   âŒ å¤±è´¥: {failed_tests}")
        print(f"   ğŸ“Š æˆåŠŸç‡: {successful_tests/(successful_tests+failed_tests)*100:.1f}%")
        
        # çŸ­æš‚ä¼‘æ¯ï¼Œé¿å…APIé™æµ
        if i < len(CLOUD_MODELS_TO_TEST):
            print(f"   â¸ï¸  ä¼‘æ¯10ç§’ï¼Œé¿å…APIé™æµ...")
            time.sleep(10)
    
    # è®¡ç®—æ€»ä½“ç»Ÿè®¡
    total_end_time = time.time()
    total_duration = total_end_time - total_start_time
    
    batch_results['end_time'] = time.strftime('%Y-%m-%d %H:%M:%S')
    batch_results['total_duration_seconds'] = total_duration
    batch_results['total_duration_formatted'] = f"{total_duration/3600:.1f}å°æ—¶"
    batch_results['summary'] = {
        'successful_tests': successful_tests,
        'failed_tests': failed_tests,
        'success_rate': successful_tests / len(CLOUD_MODELS_TO_TEST) * 100,
        'average_test_time': total_duration / len(CLOUD_MODELS_TO_TEST)
    }
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    final_results_file = results_dir / "cloud_results_final.json"
    with open(final_results_file, 'w', encoding='utf-8') as f:
        json.dump(batch_results, f, ensure_ascii=False, indent=2)
    
    # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
    print(f"\n{'ğŸ‰' * 30}")
    print(f"ğŸ äº‘æ¨¡å‹æµ‹è¯•å®Œæˆ!")
    print(f"{'ğŸ‰' * 30}")
    print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    print(f"   æ€»äº‘æ¨¡å‹æ•°: {len(CLOUD_MODELS_TO_TEST)}")
    print(f"   âœ… æˆåŠŸ: {successful_tests}")
    print(f"   âŒ å¤±è´¥: {failed_tests}")
    print(f"   ğŸ“ˆ æˆåŠŸç‡: {successful_tests/len(CLOUD_MODELS_TO_TEST)*100:.1f}%")
    print(f"   â±ï¸  æ€»æ—¶é•¿: {total_duration/3600:.1f}å°æ—¶")
    print(f"   ğŸ“ ç»“æœæ–‡ä»¶: {final_results_file}")
    
    # æ˜¾ç¤ºæœ€ä½³äº‘æ¨¡å‹
    successful_models = [r for r in batch_results['model_results'] if r['status'] == 'success']
    if successful_models:
        # æŒ‰å¾—åˆ†æ’åº
        successful_models.sort(key=lambda x: x.get('overall_score', 0), reverse=True)
        
        print(f"\nğŸ† äº‘æ¨¡å‹æ’è¡Œæ¦œ (Top 5):")
        for i, model in enumerate(successful_models[:5], 1):
            print(f"   {i}. {model['model_name']}")
            print(f"      ğŸ“Š å¾—åˆ†: {model.get('overall_score', 0):.3f}")
            print(f"      ğŸ… è¯„çº§: {model.get('grade', 'Unknown')}")
            print(f"      â±ï¸  æ—¶é•¿: {model.get('test_duration', 0)/60:.1f}åˆ†é’Ÿ")
    
    # æŒ‰APIæä¾›å•†åˆ†ç»„ç»Ÿè®¡
    api_stats = {}
    for result in batch_results['model_results']:
        if result['status'] == 'success':
            model_name = result['model_name']
            if model_name.startswith('siliconflow/'):
                api_provider = 'SiliconFlow'
            elif model_name.startswith('auto/'):
                api_provider = 'Auto-Multi-Cloud'
            elif model_name.startswith('groq/'):
                api_provider = 'Groq'
            elif model_name.startswith('together/'):
                api_provider = 'Together'
            elif model_name.startswith('openrouter/'):
                api_provider = 'OpenRouter'
            else:
                api_provider = 'Other'
            
            if api_provider not in api_stats:
                api_stats[api_provider] = {'count': 0, 'avg_score': 0, 'scores': []}
            
            api_stats[api_provider]['count'] += 1
            api_stats[api_provider]['scores'].append(result.get('overall_score', 0))
    
    # è®¡ç®—å¹³å‡åˆ†
    for provider in api_stats:
        scores = api_stats[provider]['scores']
        api_stats[provider]['avg_score'] = sum(scores) / len(scores) if scores else 0
    
    print(f"\nğŸ“Š APIæä¾›å•†è¡¨ç°:")
    for provider, stats in api_stats.items():
        print(f"   {provider}: {stats['count']}ä¸ªæ¨¡å‹, å¹³å‡åˆ†: {stats['avg_score']:.3f}")

if __name__ == "__main__":
    main()
