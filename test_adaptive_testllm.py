#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•å¸¦æœ‰adaptiveæç¤ºè¯çš„TestLLMåŠŸèƒ½
å¿«é€ŸéªŒè¯adaptiveæç¤ºè¯æ˜¯å¦æ­£ç¡®é›†æˆ
"""

import requests
import time
import os
from typing import Dict, Any

# å¯¼å…¥adaptiveæç¤ºè¯æ¨¡å—
try:
    from adaptive_prompts import ADAPTIVE_SYSTEM_PROMPTS
    ADAPTIVE_AVAILABLE = True
    print("âœ… Adaptive prompts module loaded successfully")
except ImportError:
    ADAPTIVE_AVAILABLE = False
    print("âš ï¸ Adaptive prompts module not found")

# é…ç½®
OLLAMA_API_URL = 'http://localhost:11434/api/chat'
TEST_MODEL = 'atlas/intersync-gemma-7b-instruct-function-calling:latest'
API_TIMEOUT = 60

def call_ollama_with_adaptive(model: str, prompt: str, test_context: str = "summary_analysis") -> str:
    """
    ä½¿ç”¨adaptiveæç¤ºè¯è°ƒç”¨Ollama API
    """
    print(f"ğŸ” Testing adaptive prompts for {model}")
    print(f"ğŸ“ Context: {test_context}")
    
    # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼Œæ”¯æŒadaptiveæç¤ºè¯
    if ADAPTIVE_AVAILABLE:
        try:
            test_script_name = f"test_pillar_{test_context}.py"
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é’ˆå¯¹è¯¥æ¨¡å‹çš„adaptiveæç¤ºè¯
            if model in ADAPTIVE_SYSTEM_PROMPTS and test_script_name in ADAPTIVE_SYSTEM_PROMPTS[model]:
                system_prompt = ADAPTIVE_SYSTEM_PROMPTS[model][test_script_name]
                messages = [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ]
                print(f"âœ… Using adaptive system prompt:")
                print(f"   {system_prompt[:100]}...")
            else:
                messages = [{"role": "user", "content": prompt}]
                print(f"âš ï¸ No adaptive prompt found for {model} + {test_script_name}")
        except Exception as e:
            print(f"âŒ Adaptive prompts failed: {e}")
            messages = [{"role": "user", "content": prompt}]
    else:
        messages = [{"role": "user", "content": prompt}]
    
    payload = {
        "model": model,
        "messages": messages,
        "stream": False,
        "options": {"temperature": 0.1}
    }
    
    try:
        print(f"ğŸš€ Calling {model}...")
        response = requests.post(OLLAMA_API_URL, json=payload, timeout=API_TIMEOUT)
        response.raise_for_status()
        data = response.json()
        content = data.get('message', {}).get('content', '')
        
        if content:
            print(f"âœ… Response received: {len(content)} characters")
            return content
        else:
            print(f"âš ï¸ Empty response")
            return ""
            
    except Exception as e:
        print(f"âŒ API call failed: {e}")
        return f"[API Error: {e}]"

def test_adaptive_prompts():
    """
    æµ‹è¯•adaptiveæç¤ºè¯åŠŸèƒ½
    """
    print("ğŸ§ª Testing Adaptive Prompts for TestLLM")
    print("="*60)
    
    # æµ‹è¯•æ¡ˆä¾‹
    test_cases = [
        {
            "context": "summary_analysis",
            "prompt": """
Please analyze the following dialogue segment and extract key information:

Detective: "We found fingerprints on the weapon."
Suspect A: "I never touched that knife!"
Witness: "I saw someone running from the scene around midnight."
Detective: "The victim was found at 11:45 PM."

Please summarize the key facts and evidence.
"""
        },
        {
            "context": "final_reasoning", 
            "prompt": """
Based on the following evidence, determine who is the most likely suspect:

Evidence Summary:
- Fingerprints found on weapon belong to Suspect B
- Suspect A has no alibi for the time of crime
- Suspect B was seen arguing with victim earlier
- Witness saw someone matching Suspect B's description fleeing
- Victim was killed between 11:30-11:45 PM

Who is the perpetrator and why?
"""
        }
    ]
    
    results = []
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n--- Test Case {i}: {test_case['context']} ---")
        
        # æµ‹è¯•å¸¦adaptiveæç¤ºè¯çš„ç‰ˆæœ¬
        print(f"\nğŸ”¬ Testing WITH adaptive prompts:")
        adaptive_response = call_ollama_with_adaptive(
            TEST_MODEL, 
            test_case['prompt'], 
            test_case['context']
        )
        
        # æµ‹è¯•ä¸å¸¦adaptiveæç¤ºè¯çš„ç‰ˆæœ¬
        print(f"\nğŸ”¬ Testing WITHOUT adaptive prompts:")
        standard_response = call_ollama_standard(TEST_MODEL, test_case['prompt'])
        
        # è®°å½•ç»“æœ
        result = {
            "test_case": i,
            "context": test_case['context'],
            "adaptive_response": adaptive_response,
            "standard_response": standard_response,
            "adaptive_length": len(adaptive_response) if adaptive_response else 0,
            "standard_length": len(standard_response) if standard_response else 0
        }
        results.append(result)
        
        print(f"\nğŸ“Š Comparison:")
        print(f"   Adaptive response: {result['adaptive_length']} chars")
        print(f"   Standard response: {result['standard_length']} chars")
        
        time.sleep(2)  # é¿å…APIè°ƒç”¨è¿‡å¿«
    
    # ç”ŸæˆæŠ¥å‘Š
    print(f"\n" + "="*60)
    print(f"ğŸ“‹ Adaptive Prompts Test Report")
    print(f"="*60)
    
    for result in results:
        print(f"\nTest Case {result['test_case']} ({result['context']}):")
        print(f"  Adaptive: {result['adaptive_length']} chars")
        print(f"  Standard: {result['standard_length']} chars")
        
        if result['adaptive_length'] > 0 and result['standard_length'] > 0:
            ratio = result['adaptive_length'] / result['standard_length']
            print(f"  Ratio: {ratio:.2f}x")
            if ratio > 1.2:
                print(f"  âœ… Adaptive prompts produced more detailed response")
            elif ratio < 0.8:
                print(f"  âš ï¸ Adaptive prompts produced shorter response")
            else:
                print(f"  ğŸ“Š Similar response lengths")
        elif result['adaptive_length'] > 0:
            print(f"  âœ… Only adaptive prompts produced response")
        elif result['standard_length'] > 0:
            print(f"  âŒ Only standard prompts produced response")
        else:
            print(f"  âŒ Both failed to produce response")
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    with open('adaptive_prompts_test_results.txt', 'w', encoding='utf-8') as f:
        f.write("Adaptive Prompts Test Results\n")
        f.write("="*50 + "\n\n")
        
        for result in results:
            f.write(f"Test Case {result['test_case']}: {result['context']}\n")
            f.write("-" * 40 + "\n")
            f.write(f"ADAPTIVE RESPONSE ({result['adaptive_length']} chars):\n")
            f.write(result['adaptive_response'] + "\n\n")
            f.write(f"STANDARD RESPONSE ({result['standard_length']} chars):\n")
            f.write(result['standard_response'] + "\n\n")
            f.write("="*50 + "\n\n")
    
    print(f"\nğŸ’¾ Detailed results saved to: adaptive_prompts_test_results.txt")
    print(f"âœ… Adaptive prompts test completed!")

def call_ollama_standard(model: str, prompt: str) -> str:
    """
    æ ‡å‡†æ–¹å¼è°ƒç”¨Ollama APIï¼ˆä¸ä½¿ç”¨adaptiveæç¤ºè¯ï¼‰
    """
    payload = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "stream": False,
        "options": {"temperature": 0.1}
    }
    
    try:
        response = requests.post(OLLAMA_API_URL, json=payload, timeout=API_TIMEOUT)
        response.raise_for_status()
        data = response.json()
        content = data.get('message', {}).get('content', '')
        
        if content:
            print(f"âœ… Standard response received: {len(content)} characters")
            return content
        else:
            print(f"âš ï¸ Empty standard response")
            return ""
            
    except Exception as e:
        print(f"âŒ Standard API call failed: {e}")
        return f"[API Error: {e}]"

if __name__ == "__main__":
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨
    print(f"ğŸ” Checking if model {TEST_MODEL} is available...")
    
    try:
        test_response = call_ollama_standard(TEST_MODEL, "Hello")
        if "[API Error:" in test_response:
            print(f"âŒ Model {TEST_MODEL} is not available")
            print(f"Please make sure the model is installed and Ollama is running")
        else:
            print(f"âœ… Model {TEST_MODEL} is available")
            test_adaptive_prompts()
    except Exception as e:
        print(f"âŒ Failed to connect to Ollama: {e}")
        print(f"Please make sure Ollama is running on {OLLAMA_API_URL}")
