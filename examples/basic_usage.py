#!/usr/bin/env python3
"""
LLMè§’è‰²ç‹¬ç«‹æ€§æµ‹è¯•æ¡†æ¶ - åŸºç¡€ä½¿ç”¨ç¤ºä¾‹

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨æµ‹è¯•æ¡†æ¶è¿›è¡ŒåŸºæœ¬çš„è§’è‰²ç‹¬ç«‹æ€§æµ‹è¯•
"""

import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from testLLM.core.test_runner import TestRunner
from testLLM.core.config_manager import ConfigManager

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºåŸºç¡€æµ‹è¯•æµç¨‹"""
    
    print("ğŸš€ LLMè§’è‰²ç‹¬ç«‹æ€§æµ‹è¯•æ¡†æ¶ - åŸºç¡€ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 50)
    
    try:
        # 1. åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
        print("ğŸ“‹ æ­£åœ¨åŠ è½½é…ç½®...")
        config_path = project_root / "config" / "test_config.yaml"
        config = ConfigManager(str(config_path))
        print("âœ… é…ç½®åŠ è½½æˆåŠŸ")
        
        # 2. åˆ›å»ºæµ‹è¯•è¿è¡Œå™¨
        print("\nğŸ”§ æ­£åœ¨åˆå§‹åŒ–æµ‹è¯•è¿è¡Œå™¨...")
        runner = TestRunner(config)
        print("âœ… æµ‹è¯•è¿è¡Œå™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # 3. å®šä¹‰æµ‹è¯•å‚æ•°
        test_models = ["gpt-3.5-turbo"]  # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ æ›´å¤šæ¨¡å‹
        test_roles = ["software_engineer", "data_scientist"]
        
        print(f"\nğŸ¯ æµ‹è¯•é…ç½®:")
        print(f"   æ¨¡å‹: {', '.join(test_models)}")
        print(f"   è§’è‰²: {', '.join(test_roles)}")
        
        # 4. è¿è¡Œå•ä¸ªæµ‹è¯•ç±»å‹ç¤ºä¾‹
        print("\n" + "="*50)
        print("ğŸ§ª è¿è¡Œè§’è‰²ç ´åæµ‹è¯•ç¤ºä¾‹")
        print("="*50)
        
        for model in test_models:
            for role in test_roles:
                print(f"\nğŸ” æµ‹è¯• {model} æ¨¡å‹çš„ {role} è§’è‰²...")
                
                # è¿è¡Œè§’è‰²ç ´åæµ‹è¯•
                breaking_results = runner.run_character_breaking_test(
                    model_name=model,
                    role_name=role,
                    max_attempts=3  # å‡å°‘æµ‹è¯•æ¬¡æ•°ä»¥åŠ å¿«æ¼”ç¤º
                )
                
                # æ˜¾ç¤ºç»“æœæ‘˜è¦
                if breaking_results:
                    score = breaking_results.get('overall_score', 0)
                    status = "é€šè¿‡" if score >= 0.7 else "å¤±è´¥"
                    print(f"   ğŸ“Š æµ‹è¯•ç»“æœ: {score:.2f} ({status})")
                    
                    # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
                    if 'test_details' in breaking_results:
                        details = breaking_results['test_details']
                        print(f"   ğŸ“ˆ æŠµæŠ—åŠ›è¯„åˆ†: {details.get('resistance_score', 0):.2f}")
                        print(f"   ğŸ“ˆ ä¸€è‡´æ€§è¯„åˆ†: {details.get('consistency_score', 0):.2f}")
                        print(f"   ğŸ“ˆ é€‚å½“æ€§è¯„åˆ†: {details.get('appropriateness_score', 0):.2f}")
                else:
                    print("   âŒ æµ‹è¯•å¤±è´¥")
        
        # 5. è¿è¡Œç»¼åˆæµ‹è¯•ç¤ºä¾‹
        print("\n" + "="*50)
        print("ğŸ”¬ è¿è¡Œç»¼åˆæµ‹è¯•ç¤ºä¾‹")
        print("="*50)
        
        comprehensive_results = runner.run_comprehensive_test(
            models=test_models,
            roles=test_roles[:1],  # åªæµ‹è¯•ä¸€ä¸ªè§’è‰²ä»¥åŠ å¿«æ¼”ç¤º
            test_types=['character_breaking', 'implicit_cognition']
        )
        
        # 6. ç”ŸæˆæŠ¥å‘Š
        print("\nğŸ“„ æ­£åœ¨ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")
        output_dir = project_root / "results" / "basic_example"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        report_files = runner.generate_report(
            comprehensive_results,
            str(output_dir),
            formats=['json', 'html']
        )
        
        print("âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ:")
        for format_type, file_path in report_files.items():
            print(f"   ğŸ“ {format_type.upper()}: {file_path}")
        
        # 7. æ˜¾ç¤ºæµ‹è¯•æ€»ç»“
        print("\n" + "="*50)
        print("ğŸ“Š æµ‹è¯•æ€»ç»“")
        print("="*50)
        
        if comprehensive_results and 'test_results' in comprehensive_results:
            total_tests = 0
            passed_tests = 0
            
            for model_results in comprehensive_results['test_results'].values():
                for test_results in model_results.values():
                    for test_result in test_results.values():
                        if isinstance(test_result, dict) and 'overall_score' in test_result:
                            total_tests += 1
                            if test_result['overall_score'] >= 0.7:
                                passed_tests += 1
            
            pass_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0
            print(f"æ€»æµ‹è¯•æ•°: {total_tests}")
            print(f"é€šè¿‡æµ‹è¯•æ•°: {passed_tests}")
            print(f"é€šè¿‡ç‡: {pass_rate:.1f}%")
        
        print("\nğŸ‰ åŸºç¡€æµ‹è¯•ç¤ºä¾‹å®Œæˆ!")
        print(f"ğŸ“ è¯¦ç»†ç»“æœè¯·æŸ¥çœ‹: {output_dir}")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())