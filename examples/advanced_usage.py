#!/usr/bin/env python3
"""
LLMè§’è‰²ç‹¬ç«‹æ€§æµ‹è¯•æ¡†æ¶ - é«˜çº§ä½¿ç”¨ç¤ºä¾‹

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†æ¡†æ¶çš„é«˜çº§åŠŸèƒ½ï¼ŒåŒ…æ‹¬:
- è‡ªå®šä¹‰æµ‹è¯•é…ç½®
- æ‰¹é‡æµ‹è¯•
- ç»“æœåˆ†æå’Œå¯è§†åŒ–
- æ€§èƒ½ç›‘æ§
"""

import os
import sys
import time
from pathlib import Path
from typing import Dict, List, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from testLLM.core.test_runner import TestRunner
from testLLM.core.config_manager import ConfigManager
from testLLM.results.report_generator import ReportGenerator
from testLLM.results.data_exporter import DataExporter

def run_performance_benchmark():
    """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    print("ğŸš€ æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 40)
    
    config_path = project_root / "config" / "test_config.yaml"
    config = ConfigManager(str(config_path))
    runner = TestRunner(config)
    
    # æµ‹è¯•ä¸åŒæ¨¡å‹çš„æ€§èƒ½
    models = ["gpt-3.5-turbo"]  # å¯ä»¥æ·»åŠ æ›´å¤šæ¨¡å‹
    roles = ["software_engineer", "data_scientist", "product_manager"]
    
    performance_results = {}
    
    for model in models:
        print(f"\nğŸ” æµ‹è¯•æ¨¡å‹: {model}")
        model_performance = {}
        
        for role in roles:
            print(f"  ğŸ“‹ æµ‹è¯•è§’è‰²: {role}")
            
            start_time = time.time()
            
            # è¿è¡Œå¿«é€Ÿæµ‹è¯•
            result = runner.run_character_breaking_test(
                model_name=model,
                role_name=role,
                max_attempts=2
            )
            
            end_time = time.time()
            duration = end_time - start_time
            
            model_performance[role] = {
                'duration': duration,
                'score': result.get('overall_score', 0) if result else 0,
                'success': result is not None
            }
            
            print(f"    â±ï¸  è€—æ—¶: {duration:.2f}ç§’")
            print(f"    ğŸ“Š è¯„åˆ†: {model_performance[role]['score']:.2f}")
        
        performance_results[model] = model_performance
    
    return performance_results

def run_comparative_analysis():
    """è¿è¡Œæ¯”è¾ƒåˆ†æ"""
    
    print("\nğŸ”¬ æ¯”è¾ƒåˆ†ææµ‹è¯•")
    print("=" * 40)
    
    config_path = project_root / "config" / "test_config.yaml"
    config = ConfigManager(str(config_path))
    runner = TestRunner(config)
    
    # å®šä¹‰æµ‹è¯•çŸ©é˜µ
    test_matrix = {
        "models": ["gpt-3.5-turbo"],
        "roles": ["software_engineer", "data_scientist"],
        "test_types": ["character_breaking", "implicit_cognition"]
    }
    
    print(f"ğŸ“Š æµ‹è¯•çŸ©é˜µ:")
    print(f"   æ¨¡å‹: {', '.join(test_matrix['models'])}")
    print(f"   è§’è‰²: {', '.join(test_matrix['roles'])}")
    print(f"   æµ‹è¯•ç±»å‹: {', '.join(test_matrix['test_types'])}")
    
    # è¿è¡Œæ¯”è¾ƒæµ‹è¯•
    comparative_results = []
    
    for model in test_matrix["models"]:
        for role in test_matrix["roles"]:
            print(f"\nğŸ§ª æµ‹è¯•ç»„åˆ: {model} + {role}")
            
            session_results = {
                'model': model,
                'role': role,
                'timestamp': time.time(),
                'results': {}
            }
            
            for test_type in test_matrix["test_types"]:
                print(f"  ğŸ” è¿è¡Œ {test_type} æµ‹è¯•...")
                
                if test_type == "character_breaking":
                    result = runner.run_character_breaking_test(model, role, max_attempts=2)
                elif test_type == "implicit_cognition":
                    result = runner.run_implicit_cognition_test(model, role)
                else:
                    result = None
                
                session_results['results'][test_type] = result
                
                if result:
                    score = result.get('overall_score', 0)
                    print(f"    ğŸ“ˆ {test_type}: {score:.2f}")
                else:
                    print(f"    âŒ {test_type}: å¤±è´¥")
            
            comparative_results.append(session_results)
    
    return comparative_results

def analyze_results(results: List[Dict[str, Any]]):
    """åˆ†ææµ‹è¯•ç»“æœ"""
    
    print("\nğŸ“Š ç»“æœåˆ†æ")
    print("=" * 40)
    
    # æŒ‰æ¨¡å‹åˆ†ç»„åˆ†æ
    model_stats = {}
    
    for session in results:
        model = session['model']
        if model not in model_stats:
            model_stats[model] = {
                'total_tests': 0,
                'passed_tests': 0,
                'scores': [],
                'test_types': {}
            }
        
        for test_type, result in session['results'].items():
            model_stats[model]['total_tests'] += 1
            
            if test_type not in model_stats[model]['test_types']:
                model_stats[model]['test_types'][test_type] = []
            
            if result and 'overall_score' in result:
                score = result['overall_score']
                model_stats[model]['scores'].append(score)
                model_stats[model]['test_types'][test_type].append(score)
                
                if score >= 0.7:
                    model_stats[model]['passed_tests'] += 1
    
    # è¾“å‡ºåˆ†æç»“æœ
    for model, stats in model_stats.items():
        print(f"\nğŸ¤– æ¨¡å‹: {model}")
        
        if stats['scores']:
            avg_score = sum(stats['scores']) / len(stats['scores'])
            pass_rate = (stats['passed_tests'] / stats['total_tests']) * 100
            
            print(f"   ğŸ“ˆ å¹³å‡åˆ†æ•°: {avg_score:.3f}")
            print(f"   âœ… é€šè¿‡ç‡: {pass_rate:.1f}%")
            print(f"   ğŸ“Š æµ‹è¯•æ€»æ•°: {stats['total_tests']}")
            
            # æŒ‰æµ‹è¯•ç±»å‹åˆ†æ
            for test_type, scores in stats['test_types'].items():
                if scores:
                    avg_type_score = sum(scores) / len(scores)
                    print(f"   ğŸ” {test_type}: {avg_type_score:.3f}")
        else:
            print("   âŒ æ— æœ‰æ•ˆæµ‹è¯•ç»“æœ")

def generate_advanced_reports(results: List[Dict[str, Any]]):
    """ç”Ÿæˆé«˜çº§æŠ¥å‘Š"""
    
    print("\nğŸ“„ ç”Ÿæˆé«˜çº§æŠ¥å‘Š")
    print("=" * 40)
    
    output_dir = project_root / "results" / "advanced_example"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆå§‹åŒ–æŠ¥å‘Šç”Ÿæˆå™¨å’Œæ•°æ®å¯¼å‡ºå™¨
    config_path = project_root / "config" / "test_config.yaml"
    config = ConfigManager(str(config_path))
    
    report_generator = ReportGenerator(config)
    data_exporter = DataExporter(config)
    
    # è½¬æ¢ç»“æœæ ¼å¼
    formatted_results = {
        'session_id': f"advanced_test_{int(time.time())}",
        'timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
        'test_results': {},
        'metadata': {
            'test_type': 'advanced_comparative',
            'total_sessions': len(results)
        }
    }
    
    for session in results:
        model = session['model']
        role = session['role']
        
        if model not in formatted_results['test_results']:
            formatted_results['test_results'][model] = {}
        
        if role not in formatted_results['test_results'][model]:
            formatted_results['test_results'][model][role] = {}
        
        formatted_results['test_results'][model][role] = session['results']
    
    # ç”Ÿæˆå¤šç§æ ¼å¼çš„æŠ¥å‘Š
    try:
        # JSONæŠ¥å‘Š
        json_file = data_exporter.export_results(
            formatted_results, 
            str(output_dir), 
            ['json']
        )
        print(f"âœ… JSONæŠ¥å‘Š: {json_file.get('json', 'N/A')}")
        
        # CSVæŠ¥å‘Š
        csv_file = data_exporter.export_results(
            formatted_results,
            str(output_dir),
            ['csv']
        )
        print(f"âœ… CSVæŠ¥å‘Š: {csv_file.get('csv', 'N/A')}")
        
        # HTMLæŠ¥å‘Š
        html_file = report_generator.generate_html_report(
            formatted_results,
            str(output_dir / "advanced_report.html")
        )
        print(f"âœ… HTMLæŠ¥å‘Š: {html_file}")
        
        # æ¯”è¾ƒåˆ†ææŠ¥å‘Š
        comparison_file = data_exporter.export_comparison_data(
            [formatted_results],
            str(output_dir)
        )
        print(f"âœ… æ¯”è¾ƒåˆ†æ: {comparison_file}")
        
    except Exception as e:
        print(f"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")

def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸš€ LLMè§’è‰²ç‹¬ç«‹æ€§æµ‹è¯•æ¡†æ¶ - é«˜çº§ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 60)
    
    try:
        # 1. è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
        performance_results = run_performance_benchmark()
        
        # 2. è¿è¡Œæ¯”è¾ƒåˆ†æ
        comparative_results = run_comparative_analysis()
        
        # 3. åˆ†æç»“æœ
        analyze_results(comparative_results)
        
        # 4. ç”Ÿæˆé«˜çº§æŠ¥å‘Š
        generate_advanced_reports(comparative_results)
        
        print("\nğŸ‰ é«˜çº§æµ‹è¯•ç¤ºä¾‹å®Œæˆ!")
        print(f"ğŸ“ è¯¦ç»†ç»“æœè¯·æŸ¥çœ‹: {project_root / 'results' / 'advanced_example'}")
        
        # 5. æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
        print("\nâš¡ æ€§èƒ½ç»Ÿè®¡:")
        for model, model_perf in performance_results.items():
            total_time = sum(role_perf['duration'] for role_perf in model_perf.values())
            avg_score = sum(role_perf['score'] for role_perf in model_perf.values()) / len(model_perf)
            print(f"   {model}: æ€»è€—æ—¶ {total_time:.2f}ç§’, å¹³å‡åˆ†æ•° {avg_score:.3f}")
        
    except Exception as e:
        print(f"\nâŒ é«˜çº§æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())