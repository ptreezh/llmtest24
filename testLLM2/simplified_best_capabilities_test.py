#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–ç‰ˆæœ€ä½³èƒ½åŠ›æµ‹è¯• - åŸºäºæ¨¡å‹å®é™…è¡¨ç°è°ƒæ•´æµ‹è¯•éš¾åº¦
é’ˆå¯¹æ¶Œç°åˆ†æã€æ•°å­¦æ¨ç†ã€è§’è‰²æ‰®æ¼”è¿›è¡Œé€‚åº¦ç®€åŒ–çš„æµ‹è¯•
"""

import os
import sys
import time
import json
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import ollama

# å¯¼å…¥å¢å¼ºæµ‹è¯•æ¡†æ¶
sys.path.append(os.path.abspath('.'))
from enhanced_test_framework import EnhancedTestFramework

class SimplifiedBestCapabilitiesTest(EnhancedTestFramework):
    def __init__(self):
        super().__init__()
        # æ ¹æ®é›¶å“åº”åˆ†æè°ƒæ•´å‚æ•°
        self.max_prompt_length = 300  # æ›´ä¿å®ˆçš„é•¿åº¦é™åˆ¶
        self.max_complexity_level = 2  # é™ä½å¤æ‚åº¦é™åˆ¶
        self.timeout_seconds = 20     # ç¼©çŸ­è¶…æ—¶æ—¶é—´
        
        self.capability_results = {
            'emergence': [],
            'math': [],
            'persona': []
        }
    
    def test_simplified_emergence(self):
        """ç®€åŒ–çš„æ¶Œç°åˆ†ææµ‹è¯•"""
        print("ğŸ§  ç®€åŒ–æ¶Œç°åˆ†ææµ‹è¯•")
        print("="*50)
        
        # ç®€åŒ–çš„æ¶Œç°åˆ†æé—®é¢˜
        simple_emergence_tests = [
            {
                "name": "åŸºç¡€åé¦ˆåˆ†æ",
                "prompt": "åˆ†æä¸¤æ¡åé¦ˆï¼šA. ç”¨æˆ·æ´»è·ƒåº¦æå‡äº†ã€‚B. éƒ¨åˆ†ç”¨æˆ·è§‰å¾—éš¾ç”¨ã€‚è¯·æ‰¾å‡ºçŸ›ç›¾å¹¶ç»™å»ºè®®ã€‚"
            },
            {
                "name": "ç®€å•å†²çªè¯†åˆ«", 
                "prompt": "äº§å“åé¦ˆï¼šæ–°åŠŸèƒ½å¾ˆå—æ¬¢è¿ï¼Œä½†è€ç”¨æˆ·ä¸é€‚åº”ã€‚è¯·åˆ†æè¿™ä¸ªå†²çªã€‚"
            },
            {
                "name": "åŸºç¡€è§£å†³æ–¹æ¡ˆ",
                "prompt": "é—®é¢˜ï¼šå®¢æˆ·è¦æ±‚é™ä»·ï¼Œä½†æˆæœ¬åœ¨ä¸Šå‡ã€‚è¯·æå‡ºè§£å†³æ€è·¯ã€‚"
            }
        ]
        
        for i, test in enumerate(simple_emergence_tests, 1):
            print(f"\nğŸ” æ¶Œç°æµ‹è¯• {i}: {test['name']}")
            print(f"æç¤ºè¯é•¿åº¦: {len(test['prompt'])}å­—ç¬¦")
            
            success, response, metadata = self.smart_chat(test['prompt'])
            
            if success:
                # è¯„ä¼°æ¶Œç°åˆ†æè´¨é‡
                has_conflict = any(word in response for word in ['å†²çª', 'çŸ›ç›¾', 'å¯¹ç«‹', 'é—®é¢˜'])
                has_solution = any(word in response for word in ['å»ºè®®', 'è§£å†³', 'æ–¹æ¡ˆ', 'ç­–ç•¥'])
                
                quality_score = 0
                if has_conflict: quality_score += 1
                if has_solution: quality_score += 1
                if len(response) > 50: quality_score += 1
                
                print(f"  âœ… æˆåŠŸ - è´¨é‡åˆ†æ•°: {quality_score}/3")
                result = {'success': True, 'quality_score': quality_score, 'response_length': len(response)}
            else:
                print(f"  âŒ å¤±è´¥")
                result = {'success': False, 'quality_score': 0, 'response_length': 0}
            
            result.update({
                'test_name': test['name'],
                'prompt_length': len(test['prompt']),
                'metadata': metadata
            })
            self.capability_results['emergence'].append(result)
        
        return self.capability_results['emergence']
    
    def test_simplified_math(self):
        """ç®€åŒ–çš„æ•°å­¦æ¨ç†æµ‹è¯•"""
        print("\nğŸ”¢ ç®€åŒ–æ•°å­¦æ¨ç†æµ‹è¯•")
        print("="*50)
        
        # ç®€åŒ–çš„æ•°å­¦é—®é¢˜
        simple_math_tests = [
            {
                "name": "åŸºç¡€æ¯”ä¾‹é—®é¢˜",
                "prompt": "ç”²ç®¡3å°æ—¶æ³¨æ»¡æ°´æ± ï¼Œä¹™ç®¡5å°æ—¶æ³¨æ»¡ã€‚ä¸¤ç®¡ä¸€èµ·å¼€ï¼Œå¤šä¹…æ³¨æ»¡ï¼Ÿ",
                "expected_keywords": ["å°æ—¶", "æ³¨æ»¡", "è®¡ç®—"]
            },
            {
                "name": "ç®€å•åº”ç”¨é¢˜",
                "prompt": "å°æ˜ä¹°3ä¸ªè‹¹æœèŠ±äº†6å…ƒï¼Œå°çº¢ä¹°5ä¸ªè‹¹æœèŠ±äº†å¤šå°‘å…ƒï¼Ÿ",
                "expected_keywords": ["å…ƒ", "è‹¹æœ", "è®¡ç®—"]
            },
            {
                "name": "åŸºç¡€å·¥ç¨‹é—®é¢˜",
                "prompt": "ä¸€é¡¹å·¥ä½œï¼Œç”²å•ç‹¬åšéœ€è¦4å¤©ï¼Œä¹™å•ç‹¬åšéœ€è¦6å¤©ã€‚ä¸¤äººåˆä½œéœ€è¦å‡ å¤©ï¼Ÿ",
                "expected_keywords": ["å¤©", "åˆä½œ", "å·¥ä½œ"]
            }
        ]
        
        for i, test in enumerate(simple_math_tests, 1):
            print(f"\nğŸ§® æ•°å­¦æµ‹è¯• {i}: {test['name']}")
            print(f"æç¤ºè¯é•¿åº¦: {len(test['prompt'])}å­—ç¬¦")
            
            success, response, metadata = self.smart_chat(test['prompt'])
            
            if success:
                # è¯„ä¼°æ•°å­¦æ¨ç†è´¨é‡
                has_keywords = sum(1 for keyword in test['expected_keywords'] if keyword in response)
                has_numbers = any(char.isdigit() for char in response)
                has_process = len(response) > 30
                
                quality_score = 0
                if has_keywords >= 2: quality_score += 1
                if has_numbers: quality_score += 1
                if has_process: quality_score += 1
                
                print(f"  âœ… æˆåŠŸ - è´¨é‡åˆ†æ•°: {quality_score}/3")
                result = {'success': True, 'quality_score': quality_score, 'response_length': len(response)}
            else:
                print(f"  âŒ å¤±è´¥")
                result = {'success': False, 'quality_score': 0, 'response_length': 0}
            
            result.update({
                'test_name': test['name'],
                'prompt_length': len(test['prompt']),
                'expected_keywords': test['expected_keywords'],
                'metadata': metadata
            })
            self.capability_results['math'].append(result)
        
        return self.capability_results['math']
    
    def test_simplified_persona(self):
        """ç®€åŒ–çš„è§’è‰²æ‰®æ¼”æµ‹è¯•"""
        print("\nğŸ­ ç®€åŒ–è§’è‰²æ‰®æ¼”æµ‹è¯•")
        print("="*50)
        
        # ç®€åŒ–çš„è§’è‰²æ‰®æ¼”åœºæ™¯
        simple_persona_tests = [
            {
                "name": "å‹å¥½åŠ©æ‰‹",
                "setup": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹ã€‚",
                "questions": [
                    "ä½ å¥½ï¼Œä½ èƒ½å¸®æˆ‘ä»€ä¹ˆï¼Ÿ",
                    "è°¢è°¢ä½ çš„å¸®åŠ©ã€‚"
                ],
                "expected_traits": ["å‹å¥½", "å¸®åŠ©", "åŠ©æ‰‹"]
            },
            {
                "name": "ä¸“ä¸šé¡¾é—®",
                "setup": "ä½ æ˜¯ä¸€åä¸“ä¸šé¡¾é—®ã€‚",
                "questions": [
                    "è¯·ç»™æˆ‘ä¸€äº›å»ºè®®ã€‚",
                    "ä½ çš„ä¸“ä¸šé¢†åŸŸæ˜¯ä»€ä¹ˆï¼Ÿ"
                ],
                "expected_traits": ["å»ºè®®", "ä¸“ä¸š", "é¡¾é—®"]
            }
        ]
        
        for scenario in simple_persona_tests:
            print(f"\nğŸª è§’è‰²æµ‹è¯•: {scenario['name']}")
            
            # è®¾å®šè§’è‰²
            setup_success, setup_response, setup_metadata = self.smart_chat(scenario['setup'])
            
            if not setup_success:
                print(f"  âŒ è§’è‰²è®¾å®šå¤±è´¥")
                continue
            
            print(f"  âœ… è§’è‰²è®¾å®šæˆåŠŸ")
            
            # æ„å»ºä¸Šä¸‹æ–‡
            context = [
                {'role': 'user', 'content': scenario['setup']},
                {'role': 'assistant', 'content': setup_response}
            ]
            
            consistency_scores = []
            
            for i, question in enumerate(scenario['questions'], 1):
                print(f"    é—®é¢˜ {i}: ", end="")
                
                success, response, metadata = self.smart_chat(question, context)
                
                if success:
                    # æ£€æŸ¥è§’è‰²ä¸€è‡´æ€§
                    trait_matches = sum(1 for trait in scenario['expected_traits'] if trait in response)
                    consistency_score = trait_matches / len(scenario['expected_traits'])
                    consistency_scores.append(consistency_score)
                    
                    if consistency_score >= 0.3:  # é™ä½ä¸€è‡´æ€§è¦æ±‚
                        print(f"âœ… ä¸€è‡´æ€§è‰¯å¥½ ({consistency_score:.2f})")
                    else:
                        print(f"âš ï¸ ä¸€è‡´æ€§ä¸€èˆ¬ ({consistency_score:.2f})")
                    
                    # æ›´æ–°ä¸Šä¸‹æ–‡
                    context.extend([
                        {'role': 'user', 'content': question},
                        {'role': 'assistant', 'content': response}
                    ])
                else:
                    print(f"âŒ å¤±è´¥")
                    consistency_scores.append(0.0)
                    break
            
            avg_consistency = sum(consistency_scores) / len(consistency_scores) if consistency_scores else 0
            
            result = {
                'scenario_name': scenario['name'],
                'setup_success': setup_success,
                'questions_completed': len(consistency_scores),
                'average_consistency': avg_consistency,
                'consistency_scores': consistency_scores,
                'expected_traits': scenario['expected_traits']
            }
            
            self.capability_results['persona'].append(result)
        
        return self.capability_results['persona']
    
    def run_simplified_test(self):
        """è¿è¡Œç®€åŒ–æµ‹è¯•å¥—ä»¶"""
        print("ğŸŒŸ ç®€åŒ–ç‰ˆæœ€ä½³èƒ½åŠ›æµ‹è¯•å¥—ä»¶")
        print("="*70)
        print(f"æ¨¡å‹: {self.model}")
        print(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ç®€åŒ–å‚æ•°: é•¿åº¦é™åˆ¶{self.max_prompt_length}å­—ç¬¦, å¤æ‚åº¦é™åˆ¶Level{self.max_complexity_level}")
        print()
        
        # è¿è¡Œç®€åŒ–æµ‹è¯•
        emergence_results = self.test_simplified_emergence()
        math_results = self.test_simplified_math()
        persona_results = self.test_simplified_persona()
        
        # ç”ŸæˆæŠ¥å‘Š
        self._generate_simplified_report()
    
    def _generate_simplified_report(self):
        """ç”Ÿæˆç®€åŒ–æµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "="*70)
        print("ğŸ“Š ç®€åŒ–æµ‹è¯•ç»“æœæŠ¥å‘Š")
        print("="*70)
        
        # è®¡ç®—å„èƒ½åŠ›è¡¨ç°
        capabilities = [
            ('æ¶Œç°åˆ†æ', 'emergence'),
            ('æ•°å­¦æ¨ç†', 'math'),
            ('è§’è‰²æ‰®æ¼”', 'persona')
        ]
        
        overall_stats = {'total': 0, 'success': 0, 'avg_quality': 0}
        
        for cap_name, cap_key in capabilities:
            results = self.capability_results[cap_key]
            
            if cap_key == 'persona':
                # è§’è‰²æ‰®æ¼”ç‰¹æ®Šå¤„ç†
                total_tests = sum(r['questions_completed'] for r in results)
                successful_tests = sum(1 for r in results for score in r['consistency_scores'] if score >= 0.3)
                avg_consistency = sum(r['average_consistency'] for r in results) / len(results) if results else 0
                
                print(f"\n  {cap_name}:")
                print(f"    åœºæ™¯æ•°é‡: {len(results)}")
                print(f"    æ€»é—®é¢˜æ•°: {total_tests}")
                print(f"    ä¸€è‡´æ€§è‰¯å¥½: {successful_tests}")
                print(f"    å¹³å‡ä¸€è‡´æ€§: {avg_consistency:.2f}")
                
                overall_stats['total'] += total_tests
                overall_stats['success'] += successful_tests
            else:
                # æ¶Œç°åˆ†æå’Œæ•°å­¦æ¨ç†
                total_tests = len(results)
                successful_tests = sum(1 for r in results if r['success'])
                avg_quality = sum(r['quality_score'] for r in results) / len(results) if results else 0
                success_rate = (successful_tests / total_tests * 100) if total_tests > 0 else 0
                
                print(f"\n  {cap_name}:")
                print(f"    æµ‹è¯•æ•°é‡: {total_tests}")
                print(f"    æˆåŠŸæ•°é‡: {successful_tests}")
                print(f"    æˆåŠŸç‡: {success_rate:.1f}%")
                print(f"    å¹³å‡è´¨é‡: {avg_quality:.1f}/3")
                
                overall_stats['total'] += total_tests
                overall_stats['success'] += successful_tests
                overall_stats['avg_quality'] += avg_quality
        
        # æ€»ä½“è¡¨ç°
        overall_success_rate = (overall_stats['success'] / overall_stats['total'] * 100) if overall_stats['total'] > 0 else 0
        
        print(f"\nğŸ“ˆ æ€»ä½“è¡¨ç°:")
        print(f"  æ€»æµ‹è¯•æ•°: {overall_stats['total']}")
        print(f"  æˆåŠŸæ•°é‡: {overall_stats['success']}")
        print(f"  æ€»æˆåŠŸç‡: {overall_success_rate:.1f}%")
        
        # ä¸å¤æ‚æµ‹è¯•å¯¹æ¯”
        print(f"\nğŸ“Š ä¸å¤æ‚æµ‹è¯•å¯¹æ¯”:")
        print(f"  å¤æ‚æµ‹è¯•æˆåŠŸç‡: 8.3%")
        print(f"  ç®€åŒ–æµ‹è¯•æˆåŠŸç‡: {overall_success_rate:.1f}%")
        print(f"  æ”¹è¿›å¹…åº¦: +{overall_success_rate - 8.3:.1f}%")
        
        # ä¿å­˜æŠ¥å‘Š
        report_data = {
            'timestamp': datetime.now().isoformat(),
            'model': self.model,
            'test_type': 'simplified',
            'optimization_params': {
                'max_prompt_length': self.max_prompt_length,
                'max_complexity_level': self.max_complexity_level,
                'timeout_seconds': self.timeout_seconds
            },
            'overall_stats': overall_stats,
            'overall_success_rate': overall_success_rate,
            'capability_results': self.capability_results,
            'comparison': {
                'complex_test_success_rate': 8.3,
                'simplified_test_success_rate': overall_success_rate,
                'improvement': overall_success_rate - 8.3
            }
        }
        
        with open('simplified_best_capabilities_report.json', 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: simplified_best_capabilities_report.json")
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        if overall_success_rate > 50:
            print("  âœ… ç®€åŒ–æµ‹è¯•è¡¨ç°è‰¯å¥½ï¼Œå¯ä»¥é€æ­¥å¢åŠ å¤æ‚åº¦")
        elif overall_success_rate > 30:
            print("  âš ï¸ è¡¨ç°ä¸­ç­‰ï¼Œå»ºè®®ç»§ç»­ä¼˜åŒ–æç¤ºè¯ç»“æ„")
        else:
            print("  âŒ è¡¨ç°è¾ƒå·®ï¼Œå»ºè®®è¿›ä¸€æ­¥ç®€åŒ–æµ‹è¯•æˆ–æ£€æŸ¥æ¨¡å‹é…ç½®")
        
        print(f"âœ… ç®€åŒ–æµ‹è¯•å®Œæˆï¼")

def main():
    test_suite = SimplifiedBestCapabilitiesTest()
    test_suite.run_simplified_test()

if __name__ == "__main__":
    main()
