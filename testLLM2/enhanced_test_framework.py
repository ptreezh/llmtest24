#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºæµ‹è¯•æ¡†æ¶ - åŸºäºé›¶å“åº”åˆ†æçš„ä¼˜åŒ–ç­–ç•¥
å®ç°æ™ºèƒ½é‡è¯•ã€åˆ†æ®µæç¤ºã€æ¸è¿›å¼å¤æ‚åº¦ç­‰ä¼˜åŒ–æŠ€æœ¯
"""

import os
import sys
import time
import json
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import ollama

class EnhancedTestFramework:
    def __init__(self):
        # åŠ è½½é…ç½®
        try:
            sys.path.append(os.path.abspath('.'))
            from config import MODEL_TO_TEST
            self.model = MODEL_TO_TEST
        except ImportError:
            print("é”™è¯¯: æ— æ³•å¯¼å…¥é…ç½®æ–‡ä»¶")
            sys.exit(1)
        
        # åŸºäºè¯Šæ–­ç»“æœçš„ä¼˜åŒ–å‚æ•°
        self.max_prompt_length = 400  # åŸºäº488å­—ç¬¦é˜ˆå€¼ï¼Œä¿å®ˆè®¾ç½®
        self.max_complexity_level = 3  # åŸºäºLevel 4å¤±è´¥ï¼Œé™åˆ¶ä¸ºLevel 3
        self.retry_attempts = 3
        self.timeout_seconds = 25
        
        # æµ‹è¯•ç»Ÿè®¡
        self.stats = {
            'total_tests': 0,
            'successful_tests': 0,
            'zero_responses': 0,
            'retries_used': 0,
            'segmentation_used': 0,
            'progressive_used': 0
        }
    
    def smart_chat(self, prompt: str, context: List[Dict] = None, use_retry: bool = True) -> Tuple[bool, str, Dict]:
        """
        æ™ºèƒ½èŠå¤©æ–¹æ³•ï¼Œé›†æˆæ‰€æœ‰ä¼˜åŒ–ç­–ç•¥
        è¿”å›: (æˆåŠŸæ ‡å¿—, å“åº”å†…å®¹, å…ƒæ•°æ®)
        """
        self.stats['total_tests'] += 1
        metadata = {'strategy_used': 'direct', 'attempts': 0, 'original_length': len(prompt)}
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†æ®µ
        if len(prompt) > self.max_prompt_length:
            return self._segmented_chat(prompt, context, metadata)
        
        # ç›´æ¥è¯·æ±‚
        messages = context or []
        messages.append({'role': 'user', 'content': prompt})
        
        for attempt in range(self.retry_attempts if use_retry else 1):
            metadata['attempts'] = attempt + 1
            
            try:
                response = ollama.chat(
                    model=self.model,
                    messages=messages,
                    options={
                        'timeout': self.timeout_seconds,
                        'temperature': 0.7,
                        'top_p': 0.9
                    }
                )
                
                content = response.get('message', {}).get('content', '')
                
                if content:
                    self.stats['successful_tests'] += 1
                    if attempt > 0:
                        self.stats['retries_used'] += 1
                    return True, content, metadata
                else:
                    self.stats['zero_responses'] += 1
                    if attempt < self.retry_attempts - 1:
                        print(f"    ğŸ”„ é›¶å“åº”ï¼Œé‡è¯• {attempt + 1}/{self.retry_attempts}")
                        time.sleep(1)
                        continue
                    else:
                        return False, '', metadata
                        
            except Exception as e:
                if attempt < self.retry_attempts - 1:
                    print(f"    ğŸ”„ é”™è¯¯é‡è¯• {attempt + 1}/{self.retry_attempts}: {str(e)[:50]}...")
                    time.sleep(2)
                    continue
                else:
                    metadata['error'] = str(e)
                    return False, '', metadata
        
        return False, '', metadata
    
    def _segmented_chat(self, long_prompt: str, context: List[Dict], metadata: Dict) -> Tuple[bool, str, Dict]:
        """åˆ†æ®µå¤„ç†é•¿æç¤ºè¯"""
        self.stats['segmentation_used'] += 1
        metadata['strategy_used'] = 'segmented'
        
        # æ™ºèƒ½åˆ†æ®µç­–ç•¥
        segments = self._intelligent_segmentation(long_prompt)
        metadata['segments_count'] = len(segments)
        
        print(f"    ğŸ“ é•¿æç¤ºè¯åˆ†æ®µ: {len(long_prompt)}å­—ç¬¦ â†’ {len(segments)}æ®µ")
        
        combined_response = ""
        current_context = context or []
        
        for i, segment in enumerate(segments):
            print(f"      æ®µè½{i+1}/{len(segments)}: ", end="")
            
            success, response, seg_meta = self.smart_chat(segment, current_context, use_retry=False)
            
            if success:
                print(f"âœ… ({len(response)}å­—ç¬¦)")
                combined_response += f"\n\n=== æ®µè½{i+1}å›ç­” ===\n{response}"
                
                # æ›´æ–°ä¸Šä¸‹æ–‡
                current_context.append({'role': 'user', 'content': segment})
                current_context.append({'role': 'assistant', 'content': response})
            else:
                print(f"âŒ æ®µè½{i+1}å¤±è´¥")
                metadata['failed_segment'] = i + 1
                return False, combined_response, metadata
        
        return True, combined_response, metadata
    
    def _intelligent_segmentation(self, prompt: str) -> List[str]:
        """æ™ºèƒ½åˆ†æ®µç®—æ³•"""
        # ç®€å•çš„åŸºäºå¥å·å’Œé—®å·çš„åˆ†æ®µ
        sentences = []
        current = ""
        
        for char in prompt:
            current += char
            if char in 'ã€‚ï¼Ÿï¼.?!' and len(current.strip()) > 20:
                sentences.append(current.strip())
                current = ""
        
        if current.strip():
            sentences.append(current.strip())
        
        # åˆå¹¶çŸ­å¥ï¼Œç¡®ä¿æ¯æ®µä¸è¶…è¿‡æœ€å¤§é•¿åº¦
        segments = []
        current_segment = ""
        
        for sentence in sentences:
            if len(current_segment + sentence) <= self.max_prompt_length:
                current_segment += sentence
            else:
                if current_segment:
                    segments.append(current_segment)
                current_segment = sentence
        
        if current_segment:
            segments.append(current_segment)
        
        return segments
    
    def progressive_complexity_test(self, base_topic: str, levels: List[str]) -> Dict:
        """æ¸è¿›å¼å¤æ‚åº¦æµ‹è¯•"""
        self.stats['progressive_used'] += 1
        
        print(f"ğŸ”„ æ¸è¿›å¼å¤æ‚åº¦æµ‹è¯•: {base_topic}")
        
        results = {
            'topic': base_topic,
            'levels_tested': 0,
            'successful_levels': 0,
            'responses': [],
            'failure_level': None
        }
        
        context = []
        
        for i, level_prompt in enumerate(levels[:self.max_complexity_level], 1):
            print(f"  Level {i}: ", end="")
            
            success, response, metadata = self.smart_chat(level_prompt, context)
            
            results['levels_tested'] = i
            results['responses'].append({
                'level': i,
                'prompt': level_prompt,
                'success': success,
                'response': response,
                'metadata': metadata
            })
            
            if success:
                print(f"âœ… æˆåŠŸ ({len(response)}å­—ç¬¦)")
                results['successful_levels'] = i
                
                # æ›´æ–°ä¸Šä¸‹æ–‡
                context.append({'role': 'user', 'content': level_prompt})
                context.append({'role': 'assistant', 'content': response})
            else:
                print(f"âŒ å¤±è´¥åœ¨Level {i}")
                results['failure_level'] = i
                break
        
        return results
    
    def run_enhanced_tests(self):
        """è¿è¡Œå¢å¼ºæµ‹è¯•å¥—ä»¶"""
        print("ğŸš€ å¢å¼ºæµ‹è¯•æ¡†æ¶")
        print("="*60)
        print(f"æ¨¡å‹: {self.model}")
        print(f"ä¼˜åŒ–å‚æ•°: æœ€å¤§é•¿åº¦{self.max_prompt_length}å­—ç¬¦, æœ€å¤§å¤æ‚åº¦Level{self.max_complexity_level}")
        print()
        
        test_results = []
        
        # æµ‹è¯•1: åŸºç¡€åŠŸèƒ½éªŒè¯
        print("ğŸ” æµ‹è¯•1: åŸºç¡€åŠŸèƒ½éªŒè¯")
        basic_tests = [
            "è¯·ç®€è¦ä»‹ç»äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹ã€‚",
            "è¯·åˆ†æäº‘è®¡ç®—çš„ä¸»è¦ä¼˜åŠ¿ã€‚",
            "è¯·è§£é‡ŠåŒºå—é“¾æŠ€æœ¯çš„æ ¸å¿ƒåŸç†ã€‚"
        ]
        
        for i, test in enumerate(basic_tests, 1):
            print(f"  åŸºç¡€æµ‹è¯•{i}: ", end="")
            success, response, metadata = self.smart_chat(test)
            
            if success:
                print(f"âœ… æˆåŠŸ ({len(response)}å­—ç¬¦)")
            else:
                print(f"âŒ å¤±è´¥")
            
            test_results.append({
                'category': 'basic',
                'test_id': i,
                'success': success,
                'metadata': metadata
            })
        
        # æµ‹è¯•2: é•¿æ–‡æœ¬å¤„ç†
        print(f"\nğŸ” æµ‹è¯•2: é•¿æ–‡æœ¬å¤„ç† (åˆ†æ®µç­–ç•¥)")
        long_prompt = """è¯·è¯¦ç»†åˆ†æä»¥ä¸‹å¤æ‚çš„å•†ä¸šåœºæ™¯ï¼š
        ä¸€å®¶è·¨å›½ç§‘æŠ€å…¬å¸é¢ä¸´å¤šé‡æŒ‘æˆ˜ï¼šå¸‚åœºç«äº‰åŠ å‰§å¯¼è‡´åˆ©æ¶¦ä¸‹é™ï¼Œæ–°å…´æŠ€æœ¯è¦æ±‚å¤§é‡ç ”å‘æŠ•å…¥ï¼Œ
        ç›‘ç®¡ç¯å¢ƒå˜åŒ–å¢åŠ åˆè§„æˆæœ¬ï¼Œå…¨çƒä¾›åº”é“¾ä¸ç¨³å®šå½±å“ç”Ÿäº§ï¼Œå‘˜å·¥å¯¹è¿œç¨‹å·¥ä½œçš„éœ€æ±‚æ”¹å˜äº†ç»„ç»‡ç»“æ„ï¼Œ
        å®¢æˆ·å¯¹æ•°æ®éšç§å’Œç¯å¢ƒè´£ä»»çš„è¦æ±‚ä¸æ–­æé«˜ã€‚è¯·ä»æˆ˜ç•¥è§„åˆ’ã€è¿è¥ä¼˜åŒ–ã€é£é™©ç®¡ç†ã€
        äººåŠ›èµ„æºã€æŠ€æœ¯åˆ›æ–°äº”ä¸ªç»´åº¦æå‡ºç»¼åˆè§£å†³æ–¹æ¡ˆã€‚"""
        
        print(f"  é•¿æ–‡æœ¬æµ‹è¯• ({len(long_prompt)}å­—ç¬¦): ", end="")
        success, response, metadata = self.smart_chat(long_prompt)
        
        if success:
            print(f"âœ… æˆåŠŸ (ç­–ç•¥: {metadata.get('strategy_used', 'unknown')})")
        else:
            print(f"âŒ å¤±è´¥")
        
        test_results.append({
            'category': 'long_text',
            'success': success,
            'metadata': metadata
        })
        
        # æµ‹è¯•3: æ¸è¿›å¼å¤æ‚åº¦
        print(f"\nğŸ” æµ‹è¯•3: æ¸è¿›å¼å¤æ‚åº¦æµ‹è¯•")
        complexity_levels = [
            "è¯·åˆ†æä¼ä¸šæ•°å­—åŒ–è½¬å‹çš„åŸºæœ¬æ¦‚å¿µã€‚",
            "åŸºäºä¸Šè¿°åˆ†æï¼Œè¯·æ¢è®¨æ•°å­—åŒ–è½¬å‹è¿‡ç¨‹ä¸­çš„ä¸»è¦æŒ‘æˆ˜ã€‚",
            "ç»“åˆå‰é¢çš„è®¨è®ºï¼Œè¯·æå‡ºä¸€ä¸ªå®Œæ•´çš„æ•°å­—åŒ–è½¬å‹å®æ–½æ–¹æ¡ˆã€‚"
        ]
        
        progressive_result = self.progressive_complexity_test("ä¼ä¸šæ•°å­—åŒ–è½¬å‹", complexity_levels)
        test_results.append({
            'category': 'progressive',
            'result': progressive_result
        })
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        self._generate_test_report(test_results)
    
    def _generate_test_report(self, test_results: List[Dict]):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print(f"\n" + "="*60)
        print("ğŸ“Š æµ‹è¯•ç»“æœæŠ¥å‘Š")
        print("="*60)
        
        # ç»Ÿè®¡ä¿¡æ¯
        print(f"ğŸ“ˆ æ‰§è¡Œç»Ÿè®¡:")
        print(f"  æ€»æµ‹è¯•æ•°: {self.stats['total_tests']}")
        print(f"  æˆåŠŸæµ‹è¯•: {self.stats['successful_tests']}")
        print(f"  é›¶å“åº”æ•°: {self.stats['zero_responses']}")
        print(f"  ä½¿ç”¨é‡è¯•: {self.stats['retries_used']}")
        print(f"  ä½¿ç”¨åˆ†æ®µ: {self.stats['segmentation_used']}")
        print(f"  ä½¿ç”¨æ¸è¿›: {self.stats['progressive_used']}")
        
        success_rate = (self.stats['successful_tests'] / max(self.stats['total_tests'], 1)) * 100
        print(f"  æ€»æˆåŠŸç‡: {success_rate:.1f}%")
        
        # ç­–ç•¥æ•ˆæœåˆ†æ
        print(f"\nğŸ¯ ç­–ç•¥æ•ˆæœåˆ†æ:")
        
        segmented_tests = [r for r in test_results if r.get('metadata', {}).get('strategy_used') == 'segmented']
        if segmented_tests:
            seg_success = sum(1 for r in segmented_tests if r.get('success', False))
            seg_rate = (seg_success / len(segmented_tests)) * 100
            print(f"  åˆ†æ®µç­–ç•¥æˆåŠŸç‡: {seg_rate:.1f}% ({seg_success}/{len(segmented_tests)})")
        
        progressive_tests = [r for r in test_results if r.get('category') == 'progressive']
        if progressive_tests:
            prog_result = progressive_tests[0]['result']
            prog_rate = (prog_result['successful_levels'] / prog_result['levels_tested']) * 100
            print(f"  æ¸è¿›ç­–ç•¥æˆåŠŸç‡: {prog_rate:.1f}% ({prog_result['successful_levels']}/{prog_result['levels_tested']})")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        report_data = {
            'timestamp': datetime.now().isoformat(),
            'model': self.model,
            'optimization_params': {
                'max_prompt_length': self.max_prompt_length,
                'max_complexity_level': self.max_complexity_level,
                'retry_attempts': self.retry_attempts
            },
            'statistics': self.stats,
            'test_results': test_results,
            'success_rate': success_rate
        }
        
        with open('enhanced_test_report.json', 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ è¯¦ç»†æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: enhanced_test_report.json")
        print(f"âœ… å¢å¼ºæµ‹è¯•æ¡†æ¶æ‰§è¡Œå®Œæˆï¼")

def main():
    framework = EnhancedTestFramework()
    framework.run_enhanced_tests()

if __name__ == "__main__":
    main()
