#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡æ¨¡å‹æµ‹è¯„è„šæœ¬
å¯¹models.txtä¸­çš„æ‰€æœ‰æ¨¡å‹è¿è¡Œä»Šå¤©å¼€å‘çš„æµ‹è¯„ç³»ç»Ÿ
åŒ…æ‹¬é›¶å“åº”è¯Šæ–­ã€å¢å¼ºæµ‹è¯•æ¡†æ¶å’Œç®€åŒ–æœ€ä½³èƒ½åŠ›æµ‹è¯•
"""

import os
import sys
import time
import json
import traceback
from datetime import datetime
from typing import Dict, List, Tuple
import ollama

# å¯¼å…¥æµ‹è¯•æ¨¡å—
sys.path.append(os.path.abspath('.'))

class BatchModelEvaluator:
    def __init__(self):
        self.models = self.load_models()
        self.results = {}
        self.failed_models = []
        self.max_retries = 3
        
    def load_models(self) -> List[str]:
        """åŠ è½½æ¨¡å‹åˆ—è¡¨"""
        models = []
        try:
            with open('models.txt', 'r', encoding='utf-8') as f:
                for line in f:
                    model = line.strip()
                    if model and not model.startswith('#'):
                        models.append(model)
        except FileNotFoundError:
            print("é”™è¯¯: æœªæ‰¾åˆ°models.txtæ–‡ä»¶")
            sys.exit(1)
        
        print(f"ğŸ“‹ åŠ è½½äº† {len(models)} ä¸ªæ¨¡å‹:")
        for i, model in enumerate(models, 1):
            print(f"  {i}. {model}")
        
        return models
    
    def test_model_connectivity(self, model: str) -> bool:
        """æµ‹è¯•æ¨¡å‹è¿æ¥æ€§"""
        try:
            response = ollama.chat(
                model=model,
                messages=[{'role': 'user', 'content': 'ä½ å¥½'}],
                options={'timeout': 10}
            )
            return len(response.get('message', {}).get('content', '')) > 0
        except Exception as e:
            print(f"    è¿æ¥æµ‹è¯•å¤±è´¥: {str(e)[:100]}...")
            return False
    
    def run_pillars_9_19_tests(self, model: str) -> Dict:
        """è¿è¡ŒPillar 9-19æµ‹è¯•"""
        print(f"  ğŸ“‹ è¿è¡ŒPillar 9-19æµ‹è¯•...")

        try:
            # ä¸´æ—¶ä¿®æ”¹config.pyä¸­çš„æ¨¡å‹
            self.update_config_model(model)

            # è¿è¡Œå„ä¸ªPillaræµ‹è¯•
            pillar_results = {}

            # Pillaræµ‹è¯•åˆ—è¡¨
            pillar_tests = [
                ('pillar_9_reasoning', 'é€»è¾‘æ¨ç†'),
                ('pillar_10_math', 'æ•°å­¦æ¨ç†'),
                ('pillar_11_creativity', 'åˆ›æ„ç”Ÿæˆ'),
                ('pillar_12_persona', 'è§’è‰²æ‰®æ¼”'),
                ('pillar_13_init', 'ç¯å¢ƒåˆå§‹åŒ–'),
                ('pillar_14_persona_depth', 'æ·±åº¦è§’è‰²æ‰®æ¼”'),
                ('pillar_15_collaboration', 'å¤šè§’è‰²åä½œ'),
                ('pillar_16_emergence', 'æ¶Œç°åˆ†æ'),
                ('pillar_17_task_graph', 'ä»»åŠ¡å›¾è°±'),
                ('pillar_18_adaptive_exec', 'è‡ªé€‚åº”æ‰§è¡Œ'),
                ('pillar_19_network_analysis', 'ç½‘ç»œåˆ†æ')
            ]

            for pillar_name, pillar_desc in pillar_tests:
                print(f"    ğŸ” æµ‹è¯• {pillar_name} ({pillar_desc})...")

                try:
                    # åŠ¨æ€å¯¼å…¥å¹¶è¿è¡Œæµ‹è¯•
                    test_module = __import__(f'tests.test_{pillar_name}', fromlist=['run_test'])

                    if hasattr(test_module, 'run_test'):
                        # æ•è·æµ‹è¯•è¾“å‡º
                        import io
                        import contextlib

                        output_buffer = io.StringIO()
                        with contextlib.redirect_stdout(output_buffer):
                            test_module.run_test()

                        output = output_buffer.getvalue()
                        pillar_results[pillar_name] = {
                            'status': 'completed',
                            'output': output,
                            'description': pillar_desc
                        }
                        print(f"      âœ… {pillar_desc} å®Œæˆ")

                    else:
                        pillar_results[pillar_name] = {
                            'status': 'no_run_test_function',
                            'description': pillar_desc
                        }
                        print(f"      âš ï¸ {pillar_desc} æ— run_testå‡½æ•°")

                except ImportError as e:
                    pillar_results[pillar_name] = {
                        'status': 'import_error',
                        'error': str(e),
                        'description': pillar_desc
                    }
                    print(f"      âŒ {pillar_desc} å¯¼å…¥å¤±è´¥")

                except Exception as e:
                    pillar_results[pillar_name] = {
                        'status': 'execution_error',
                        'error': str(e),
                        'description': pillar_desc
                    }
                    print(f"      âŒ {pillar_desc} æ‰§è¡Œå¤±è´¥")

            return {
                'timestamp': datetime.now().isoformat(),
                'model': model,
                'pillar_results': pillar_results,
                'completed_tests': sum(1 for r in pillar_results.values() if r['status'] == 'completed'),
                'total_tests': len(pillar_tests)
            }

        except Exception as e:
            print(f"    Pillaræµ‹è¯•å¤±è´¥: {str(e)[:100]}...")
            return {'error': str(e), 'model': model}

    def run_zero_response_diagnosis(self, model: str) -> Dict:
        """è¿è¡Œé›¶å“åº”è¯Šæ–­"""
        print(f"  ğŸ”¬ è¿è¡Œé›¶å“åº”è¯Šæ–­...")

        try:
            # ä¸´æ—¶ä¿®æ”¹config.pyä¸­çš„æ¨¡å‹
            self.update_config_model(model)

            # å¯¼å…¥å¹¶è¿è¡Œè¯Šæ–­
            from zero_response_diagnosis import ZeroResponseDiagnostic

            diagnostic = ZeroResponseDiagnostic()

            # è¿è¡Œç®€åŒ–ç‰ˆè¯Šæ–­ä»¥èŠ‚çœæ—¶é—´
            results = {
                'timestamp': datetime.now().isoformat(),
                'model': model,
                'tests': {}
            }

            # åŸºæœ¬è¿æ¥æ€§æµ‹è¯•
            basic_success = diagnostic.test_basic_connectivity()
            results['basic_connectivity'] = basic_success

            if not basic_success:
                return results

            # é•¿åº¦é˜ˆå€¼æµ‹è¯•ï¼ˆç®€åŒ–ç‰ˆï¼‰
            length_threshold = self.simplified_length_test(model)
            results['length_threshold'] = length_threshold

            # å¤æ‚åº¦é˜ˆå€¼æµ‹è¯•ï¼ˆç®€åŒ–ç‰ˆï¼‰
            complexity_threshold = self.simplified_complexity_test(model)
            results['complexity_threshold'] = complexity_threshold

            return results

        except Exception as e:
            print(f"    è¯Šæ–­å¤±è´¥: {str(e)[:100]}...")
            return {'error': str(e), 'model': model}
    
    def simplified_length_test(self, model: str) -> int:
        """ç®€åŒ–çš„é•¿åº¦é˜ˆå€¼æµ‹è¯•"""
        test_lengths = [50, 100, 200, 400]
        base_prompt = "è¯·åˆ†æä»¥ä¸‹é—®é¢˜ï¼š"
        
        for length in test_lengths:
            filler = "è¿™æ˜¯ä¸€ä¸ªå•†ä¸šåˆ†æé—®é¢˜ã€‚" * (length // 12)
            prompt = base_prompt + filler[:length-len(base_prompt)]
            
            try:
                response = ollama.chat(
                    model=model,
                    messages=[{'role': 'user', 'content': prompt}],
                    options={'timeout': 15}
                )
                content = response.get('message', {}).get('content', '')
                
                if len(content) == 0:
                    return len(prompt)
                    
            except Exception:
                return len(prompt)
        
        return None  # æœªå‘ç°é˜ˆå€¼
    
    def simplified_complexity_test(self, model: str) -> int:
        """ç®€åŒ–çš„å¤æ‚åº¦é˜ˆå€¼æµ‹è¯•"""
        complexity_tests = [
            "è¯·åˆ†æï¼šå…¬å¸åº”è¯¥æé«˜ä»·æ ¼è¿˜æ˜¯é™ä½æˆæœ¬ï¼Ÿ",
            "è¯·åˆ†æï¼šå…¬å¸é¢ä¸´ä»·æ ¼ç«äº‰å’Œæˆæœ¬ä¸Šå‡ï¼Œåº”è¯¥å¦‚ä½•åº”å¯¹ï¼Ÿ",
            "è¯·åˆ†æå¤šæ–¹å†²çªï¼šè‚¡ä¸œè¦æ±‚æé«˜åˆ©æ¶¦ï¼Œå®¢æˆ·è¦æ±‚é™ä»·ï¼Œå‘˜å·¥è¦æ±‚åŠ è–ªã€‚"
        ]
        
        for i, test_prompt in enumerate(complexity_tests, 1):
            try:
                response = ollama.chat(
                    model=model,
                    messages=[{'role': 'user', 'content': test_prompt}],
                    options={'timeout': 20}
                )
                content = response.get('message', {}).get('content', '')
                
                if len(content) == 0:
                    return i
                    
            except Exception:
                return i
        
        return None  # æœªå‘ç°é˜ˆå€¼
    
    def run_simplified_capabilities_test(self, model: str) -> Dict:
        """è¿è¡Œç®€åŒ–èƒ½åŠ›æµ‹è¯•"""
        print(f"  ğŸŒŸ è¿è¡Œç®€åŒ–èƒ½åŠ›æµ‹è¯•...")
        
        try:
            # ä¸´æ—¶ä¿®æ”¹config.pyä¸­çš„æ¨¡å‹
            self.update_config_model(model)
            
            # å¯¼å…¥å¹¶è¿è¡Œç®€åŒ–æµ‹è¯•
            from simplified_best_capabilities_test import SimplifiedBestCapabilitiesTest
            
            test_suite = SimplifiedBestCapabilitiesTest()
            
            # è¿è¡Œå„é¡¹èƒ½åŠ›æµ‹è¯•
            emergence_results = test_suite.test_simplified_emergence()
            math_results = test_suite.test_simplified_math()
            persona_results = test_suite.test_simplified_persona()
            
            # è®¡ç®—æ€»ä½“ç»Ÿè®¡
            total_tests = len(emergence_results) + len(math_results)
            successful_tests = sum(1 for r in emergence_results if r['success']) + \
                             sum(1 for r in math_results if r['success'])
            
            # è§’è‰²æ‰®æ¼”ç»Ÿè®¡
            persona_total = sum(r['questions_completed'] for r in persona_results)
            persona_success = sum(1 for r in persona_results 
                                for score in r['consistency_scores'] if score >= 0.3)
            
            total_tests += persona_total
            successful_tests += persona_success
            
            overall_success_rate = (successful_tests / max(total_tests, 1)) * 100
            
            return {
                'timestamp': datetime.now().isoformat(),
                'model': model,
                'overall_success_rate': overall_success_rate,
                'emergence_success_rate': (sum(1 for r in emergence_results if r['success']) / 
                                         max(len(emergence_results), 1)) * 100,
                'math_success_rate': (sum(1 for r in math_results if r['success']) / 
                                    max(len(math_results), 1)) * 100,
                'persona_success_rate': (persona_success / max(persona_total, 1)) * 100,
                'detailed_results': {
                    'emergence': emergence_results,
                    'math': math_results,
                    'persona': persona_results
                }
            }
            
        except Exception as e:
            print(f"    èƒ½åŠ›æµ‹è¯•å¤±è´¥: {str(e)[:100]}...")
            return {'error': str(e), 'model': model}
    
    def update_config_model(self, model: str):
        """ä¸´æ—¶æ›´æ–°config.pyä¸­çš„æ¨¡å‹"""
        config_content = f"""# ä¸´æ—¶é…ç½®æ–‡ä»¶ - æ‰¹é‡æµ‹è¯•
MODEL_TO_TEST = '{model}'
"""
        with open('config.py', 'w', encoding='utf-8') as f:
            f.write(config_content)
    
    def evaluate_single_model(self, model: str) -> Dict:
        """è¯„ä¼°å•ä¸ªæ¨¡å‹"""
        print(f"\nğŸ¤– å¼€å§‹è¯„ä¼°æ¨¡å‹: {model}")
        print("="*60)

        model_results = {
            'model': model,
            'start_time': datetime.now().isoformat(),
            'status': 'unknown',
            'connectivity': False,
            'pillars_results': {},
            'diagnosis_results': {},
            'capabilities_results': {},
            'error_count': 0,
            'retry_count': 0
        }

        # è¿æ¥æ€§æµ‹è¯•
        print(f"  ğŸ”Œ æµ‹è¯•è¿æ¥æ€§...")
        if not self.test_model_connectivity(model):
            model_results['status'] = 'connection_failed'
            print(f"  âŒ æ¨¡å‹è¿æ¥å¤±è´¥ï¼Œè·³è¿‡")
            return model_results

        model_results['connectivity'] = True
        print(f"  âœ… è¿æ¥æˆåŠŸ")

        # ç¬¬ä¸€æ­¥ï¼šè¿è¡ŒPillar 9-19æµ‹è¯•ï¼ˆå¸¦é‡è¯•ï¼‰
        for attempt in range(self.max_retries):
            try:
                pillars_results = self.run_pillars_9_19_tests(model)
                if 'error' not in pillars_results:
                    model_results['pillars_results'] = pillars_results
                    print(f"  âœ… Pillar 9-19æµ‹è¯•å®Œæˆ")
                    break
                else:
                    raise Exception(pillars_results['error'])
            except Exception as e:
                model_results['retry_count'] += 1
                model_results['error_count'] += 1
                print(f"  âš ï¸ Pillaræµ‹è¯•å¤±è´¥ (å°è¯• {attempt + 1}/{self.max_retries}): {str(e)[:50]}...")
                if attempt < self.max_retries - 1:
                    time.sleep(2)
                else:
                    print(f"  âŒ Pillaræµ‹è¯•æœ€ç»ˆå¤±è´¥ï¼Œç»§ç»­åç»­æµ‹è¯•")
                    model_results['pillars_results'] = {'error': str(e)}

        # ç¬¬äºŒæ­¥ï¼šé›¶å“åº”è¯Šæ–­ï¼ˆå¸¦é‡è¯•ï¼‰
        for attempt in range(self.max_retries):
            try:
                diagnosis_results = self.run_zero_response_diagnosis(model)
                if 'error' not in diagnosis_results:
                    model_results['diagnosis_results'] = diagnosis_results
                    print(f"  âœ… é›¶å“åº”è¯Šæ–­å®Œæˆ")
                    break
                else:
                    raise Exception(diagnosis_results['error'])
            except Exception as e:
                model_results['retry_count'] += 1
                model_results['error_count'] += 1
                print(f"  âš ï¸ è¯Šæ–­å¤±è´¥ (å°è¯• {attempt + 1}/{self.max_retries}): {str(e)[:50]}...")
                if attempt < self.max_retries - 1:
                    time.sleep(2)
                else:
                    print(f"  âŒ è¯Šæ–­æœ€ç»ˆå¤±è´¥ï¼Œç»§ç»­èƒ½åŠ›æµ‹è¯•")
                    model_results['diagnosis_results'] = {'error': str(e)}

        # ç¬¬ä¸‰æ­¥ï¼šå¢å¼ºèƒ½åŠ›æµ‹è¯•ï¼ˆå¸¦é‡è¯•ï¼‰
        for attempt in range(self.max_retries):
            try:
                capabilities_results = self.run_simplified_capabilities_test(model)
                if 'error' not in capabilities_results:
                    model_results['capabilities_results'] = capabilities_results
                    model_results['status'] = 'completed'
                    print(f"  âœ… å¢å¼ºèƒ½åŠ›æµ‹è¯•å®Œæˆ")
                    break
                else:
                    raise Exception(capabilities_results['error'])
            except Exception as e:
                model_results['retry_count'] += 1
                model_results['error_count'] += 1
                print(f"  âš ï¸ èƒ½åŠ›æµ‹è¯•å¤±è´¥ (å°è¯• {attempt + 1}/{self.max_retries}): {str(e)[:50]}...")
                if attempt < self.max_retries - 1:
                    time.sleep(2)
                else:
                    print(f"  âŒ èƒ½åŠ›æµ‹è¯•æœ€ç»ˆå¤±è´¥")
                    model_results['capabilities_results'] = {'error': str(e)}
                    model_results['status'] = 'partial_completed'

        model_results['end_time'] = datetime.now().isoformat()
        return model_results
    
    def run_batch_evaluation(self):
        """è¿è¡Œæ‰¹é‡è¯„ä¼°"""
        print("ğŸš€ æ‰¹é‡æ¨¡å‹è¯„ä¼°å¼€å§‹")
        print("="*80)
        print(f"è¯„ä¼°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"æ¨¡å‹æ•°é‡: {len(self.models)}")
        print(f"æœ€å¤§é‡è¯•æ¬¡æ•°: {self.max_retries}")
        print()
        
        start_time = datetime.now()
        
        for i, model in enumerate(self.models, 1):
            print(f"\nğŸ“Š è¿›åº¦: {i}/{len(self.models)}")
            
            try:
                model_results = self.evaluate_single_model(model)
                self.results[model] = model_results
                
                if model_results['status'] != 'completed':
                    self.failed_models.append(model)
                
            except Exception as e:
                print(f"âŒ æ¨¡å‹ {model} è¯„ä¼°å‡ºç°ä¸¥é‡é”™è¯¯: {str(e)}")
                self.failed_models.append(model)
                self.results[model] = {
                    'model': model,
                    'status': 'critical_error',
                    'error': str(e),
                    'traceback': traceback.format_exc()
                }
        
        end_time = datetime.now()
        duration = end_time - start_time
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        self.generate_batch_report(start_time, end_time, duration)
    
    def generate_batch_report(self, start_time, end_time, duration):
        """ç”Ÿæˆæ‰¹é‡è¯„ä¼°æŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ğŸ“Š æ‰¹é‡è¯„ä¼°æ€»ç»“æŠ¥å‘Š")
        print("="*80)
        
        successful_models = [m for m in self.models if m not in self.failed_models]
        
        print(f"â±ï¸ è¯„ä¼°æ—¶é—´: {duration}")
        print(f"ğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
        print(f"  æ€»æ¨¡å‹æ•°: {len(self.models)}")
        print(f"  æˆåŠŸè¯„ä¼°: {len(successful_models)}")
        print(f"  å¤±è´¥æ¨¡å‹: {len(self.failed_models)}")
        print(f"  æˆåŠŸç‡: {len(successful_models)/len(self.models)*100:.1f}%")
        
        if successful_models:
            print(f"\nâœ… æˆåŠŸè¯„ä¼°çš„æ¨¡å‹:")
            for model in successful_models:
                result = self.results[model]

                # Pillaræµ‹è¯•ç»“æœ
                pillar_info = ""
                if 'pillars_results' in result and 'error' not in result['pillars_results']:
                    completed = result['pillars_results'].get('completed_tests', 0)
                    total = result['pillars_results'].get('total_tests', 11)
                    pillar_info = f"Pillar:{completed}/{total}"

                # èƒ½åŠ›æµ‹è¯•ç»“æœ
                capability_info = ""
                if 'capabilities_results' in result and 'error' not in result['capabilities_results']:
                    success_rate = result['capabilities_results'].get('overall_success_rate', 0)
                    capability_info = f"èƒ½åŠ›:{success_rate:.1f}%"

                # è¯Šæ–­ç»“æœ
                diagnosis_info = ""
                if 'diagnosis_results' in result and 'error' not in result['diagnosis_results']:
                    diagnosis_info = "è¯Šæ–­:âœ“"

                status_parts = [info for info in [pillar_info, diagnosis_info, capability_info] if info]
                status = " | ".join(status_parts) if status_parts else "éƒ¨åˆ†å®Œæˆ"

                print(f"  â€¢ {model}: {status}")
        
        if self.failed_models:
            print(f"\nâŒ å¤±è´¥çš„æ¨¡å‹:")
            for model in self.failed_models:
                result = self.results[model]
                print(f"  â€¢ {model}: {result.get('status', 'unknown')}")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_data = {
            'batch_evaluation_summary': {
                'start_time': start_time.isoformat(),
                'end_time': end_time.isoformat(),
                'duration_seconds': duration.total_seconds(),
                'total_models': len(self.models),
                'successful_models': len(successful_models),
                'failed_models': len(self.failed_models),
                'success_rate': len(successful_models)/len(self.models)*100
            },
            'model_results': self.results,
            'successful_models': successful_models,
            'failed_models': self.failed_models
        }
        
        with open('batch_evaluation_report.json', 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: batch_evaluation_report.json")
        print(f"âœ… æ‰¹é‡è¯„ä¼°å®Œæˆï¼")

def main():
    evaluator = BatchModelEvaluator()
    evaluator.run_batch_evaluation()

if __name__ == "__main__":
    main()
