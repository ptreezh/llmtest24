#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆæµ‹è¯•è¿è¡Œå™¨
åŒ…å«è‡ªåŠ¨é‡æµ‹ã€è¶…æ—¶å¤„ç†ã€å¤±è´¥æ¢å¤ç­‰åŠŸèƒ½
"""

import os
import sys
import time
import json
import subprocess
from datetime import datetime
from typing import Dict, List, Optional
import ollama

class EnhancedTestRunner:
    def __init__(self, max_retries=3, timeout_seconds=60, retry_delay=5):
        self.max_retries = max_retries
        self.timeout_seconds = timeout_seconds
        self.retry_delay = retry_delay
        self.test_results = {}
        self.failed_tests = []
        
        # åŠ è½½é…ç½®
        try:
            sys.path.append(os.path.abspath('.'))
            from config import MODEL_TO_TEST
            self.model = MODEL_TO_TEST
        except ImportError:
            print("é”™è¯¯: æ— æ³•å¯¼å…¥é…ç½®æ–‡ä»¶")
            sys.exit(1)
    
    def test_ollama_connection(self) -> bool:
        """æµ‹è¯•Ollamaè¿æ¥"""
        try:
            ollama.list()
            return True
        except Exception as e:
            print(f"Ollamaè¿æ¥å¤±è´¥: {e}")
            return False
    
    def call_ollama_with_retry(self, messages: List[Dict], test_id: str) -> Optional[str]:
        """å¸¦é‡è¯•çš„Ollamaè°ƒç”¨"""
        for attempt in range(self.max_retries):
            try:
                print(f"  å°è¯• {attempt + 1}/{self.max_retries}...")
                
                # è®¾ç½®è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´
                response = ollama.chat(
                    model=self.model, 
                    messages=messages,
                    options={
                        'timeout': self.timeout_seconds,
                        'temperature': 0.7,
                        'top_p': 0.9
                    }
                )
                
                content = response['message']['content']
                
                # æ£€æŸ¥å“åº”æ˜¯å¦æœ‰æ•ˆ
                if content and len(content.strip()) > 10:
                    print(f"  âœ… æˆåŠŸè·å¾—å“åº” ({len(content)}å­—ç¬¦)")
                    return content
                else:
                    print(f"  âš ï¸ å“åº”è¿‡çŸ­æˆ–ä¸ºç©º: {len(content) if content else 0}å­—ç¬¦")
                    if attempt < self.max_retries - 1:
                        time.sleep(self.retry_delay)
                        continue
                    
            except Exception as e:
                print(f"  âŒ å°è¯• {attempt + 1} å¤±è´¥: {e}")
                if attempt < self.max_retries - 1:
                    print(f"  ç­‰å¾… {self.retry_delay} ç§’åé‡è¯•...")
                    time.sleep(self.retry_delay)
                else:
                    print(f"  æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†")
                    self.failed_tests.append({
                        'test_id': test_id,
                        'error': str(e),
                        'attempts': self.max_retries
                    })
        
        return None
    
    def run_single_case_test(self, test_script: str, case_info: Dict) -> bool:
        """è¿è¡Œå•ä¸ªæµ‹è¯•æ¡ˆä¾‹"""
        test_id = f"{test_script}_{case_info.get('case', 'unknown')}"
        print(f"\nğŸ§ª è¿è¡Œæµ‹è¯•: {test_id}")
        print(f"   æè¿°: {case_info.get('desc', 'N/A')}")
        
        messages = [{'role': 'user', 'content': case_info['prompt']}]
        
        # è°ƒç”¨æ¨¡å‹
        response = self.call_ollama_with_retry(messages, test_id)
        
        if response:
            # ä¿å­˜ç»“æœ
            output_dir = "testout"
            os.makedirs(output_dir, exist_ok=True)
            
            output_file = os.path.join(output_dir, f"{test_script}_{case_info['case']}.txt")
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(f"ç”¨ä¾‹ç¼–å·: {case_info['case']}\n")
                f.write(f"ç±»å‹: {case_info['desc']}\n")
                f.write(f"PROMPT:\n{case_info['prompt']}\n\n")
                f.write(f"MODEL RESPONSE:\n{response}")
            
            print(f"  ğŸ’¾ ç»“æœå·²ä¿å­˜: {output_file}")
            
            # è®°å½•æˆåŠŸ
            self.test_results[test_id] = {
                'status': 'success',
                'response_length': len(response),
                'output_file': output_file
            }
            return True
        else:
            # è®°å½•å¤±è´¥
            self.test_results[test_id] = {
                'status': 'failed',
                'attempts': self.max_retries
            }
            return False
    
    def run_multi_round_test(self, test_script: str, cases: List[Dict]) -> bool:
        """è¿è¡Œå¤šè½®å¯¹è¯æµ‹è¯•ï¼ˆç‰¹æ®Šå¤„ç†ï¼‰"""
        print(f"\nğŸ”„ è¿è¡Œå¤šè½®æµ‹è¯•: {test_script}")
        
        messages = []
        all_success = True
        
        for idx, case_info in enumerate(cases):
            round_num = idx + 1
            test_id = f"{test_script}_case{case_info['case']}_round{round_num}"
            
            print(f"\n  è½®æ¬¡ {round_num}: {case_info['desc']}")
            
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            messages.append({'role': 'user', 'content': case_info['prompt']})
            
            # è·å–å“åº”
            response = self.call_ollama_with_retry(messages, test_id)
            
            if response:
                # æ·»åŠ åŠ©æ‰‹å“åº”åˆ°å¯¹è¯å†å²
                messages.append({'role': 'assistant', 'content': response})
                
                # ä¿å­˜ç»“æœ
                output_dir = "testout"
                os.makedirs(output_dir, exist_ok=True)
                
                output_file = os.path.join(output_dir, f"persona_case{case_info['case']}_round{round_num}.txt")
                with open(output_file, 'w', encoding='utf-8') as f:
                    f.write(f"ç”¨ä¾‹ç¼–å·: case{case_info['case']} è½®æ¬¡: {round_num}\n")
                    f.write(f"ç±»å‹: {case_info['desc']}\n")
                    f.write(f"PROMPT:\n{case_info['prompt']}\n\n")
                    f.write(f"MODEL RESPONSE:\n{response}")
                
                print(f"    ğŸ’¾ ç»“æœå·²ä¿å­˜: {output_file}")
                
                self.test_results[test_id] = {
                    'status': 'success',
                    'response_length': len(response),
                    'output_file': output_file,
                    'round': round_num
                }
            else:
                print(f"    âŒ è½®æ¬¡ {round_num} å¤±è´¥")
                all_success = False
                
                # å°è¯•é‡ç½®å¯¹è¯ä¸Šä¸‹æ–‡ç»§ç»­
                if len(messages) > 2:  # å¦‚æœæœ‰å†å²å¯¹è¯
                    print(f"    ğŸ”„ é‡ç½®å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œç»§ç»­ä¸‹ä¸€è½®...")
                    messages = []  # æ¸…ç©ºå†å²
                
                self.test_results[test_id] = {
                    'status': 'failed',
                    'attempts': self.max_retries,
                    'round': round_num
                }
        
        return all_success
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ å¯åŠ¨å¢å¼ºç‰ˆæµ‹è¯•è¿è¡Œå™¨")
        print(f"é…ç½®: æ¨¡å‹={self.model}, é‡è¯•={self.max_retries}æ¬¡, è¶…æ—¶={self.timeout_seconds}ç§’")
        
        # æ£€æŸ¥è¿æ¥
        if not self.test_ollama_connection():
            print("âŒ Ollamaè¿æ¥å¤±è´¥ï¼Œé€€å‡º")
            return
        
        start_time = datetime.now()
        
        # å®šä¹‰æµ‹è¯•é…ç½®
        test_configs = [
            {
                'script': 'creativity',
                'cases': [
                    {'case': '1', 'desc': 'é²è¿…æ–‡é£å¹¿å‘Š', 'prompt': 'è¯·ä»¥é²è¿…çš„æ–‡é£ï¼Œä¸ºä¸€æ¬¾åä¸º"èµ›åšåŠ é€Ÿ"çš„èƒ½é‡é¥®æ–™å†™ä¸€æ®µå¹¿å‘Šè¯ï¼Œä¸è¶…è¿‡100å­—ã€‚'},
                    {'case': '2', 'desc': 'æµ·æ˜å¨æ–‡é£å¹¿å‘Š', 'prompt': 'è¯·ä»¥æµ·æ˜å¨çš„æ–‡é£ï¼Œä¸ºä¸€æ¬¾åä¸º"æœªæ¥èƒ½é‡æ£’"çš„èƒ½é‡é£Ÿå“å†™ä¸€æ®µå¹¿å‘Šè¯ï¼Œä¸è¶…è¿‡80å­—ã€‚'},
                    {'case': '3', 'desc': 'ç½‘ç»œæµè¡Œè¯­å¹¿å‘Š', 'prompt': 'è¯·ç”¨ç½‘ç»œæµè¡Œè¯­é£æ ¼ï¼Œä¸ºä¸€æ¬¾åä¸º"AIæ™ºèƒ½é¥®æ–™"çš„äº§å“å†™ä¸€æ®µæœ‰è¶£çš„å¹¿å‘Šè¯ï¼Œä¸è¶…è¿‡60å­—ã€‚'}
                ],
                'multi_round': False
            },
            {
                'script': 'persona',
                'cases': [
                    {'case': '1', 'desc': 'èµ›åšæœ‹å…‹çŒ«ä¸–ç•Œè§‚', 'prompt': 'ä»ç°åœ¨å¼€å§‹ï¼Œä½ æ˜¯ä¸€åªç”Ÿæ´»åœ¨èµ›åšæœ‹å…‹åŸå¸‚é‡Œçš„çŒ«ï¼Œæ‹¥æœ‰ä¸€äº›åŸºç¡€çš„ç”µå­è„‘æ¥å£ï¼Œèƒ½ç†è§£äººç±»è¯­è¨€ã€‚è¯·æè¿°ä¸€ä¸‹ä½ çœ¼ä¸­çš„ä¸–ç•Œã€‚'},
                    {'case': '2', 'desc': 'çŒ«çš„æœ€çˆ±', 'prompt': 'ä½ æœ€å–œæ¬¢åƒä»€ä¹ˆï¼Ÿ'},
                    {'case': '3', 'desc': 'çŒ«çš„æ—¥å¸¸', 'prompt': 'ä½ ä¸€å¤©çš„ç”Ÿæ´»æ˜¯æ€æ ·çš„ï¼Ÿ'},
                    {'case': '4', 'desc': 'çŒ«ä¸äººç±»çš„å…³ç³»', 'prompt': 'ä½ å¦‚ä½•çœ‹å¾…äººç±»ï¼Ÿ'}
                ],
                'multi_round': True
            }
        ]
        
        # è¿è¡Œæµ‹è¯•
        total_tests = 0
        successful_tests = 0
        
        for config in test_configs:
            if config['multi_round']:
                success = self.run_multi_round_test(config['script'], config['cases'])
                total_tests += len(config['cases'])
                if success:
                    successful_tests += len(config['cases'])
                else:
                    # è®¡ç®—å®é™…æˆåŠŸçš„è½®æ¬¡
                    for case in config['cases']:
                        test_id = f"{config['script']}_case{case['case']}_round{config['cases'].index(case)+1}"
                        if self.test_results.get(test_id, {}).get('status') == 'success':
                            successful_tests += 1
            else:
                for case in config['cases']:
                    success = self.run_single_case_test(config['script'], case)
                    total_tests += 1
                    if success:
                        successful_tests += 1
        
        # ç”ŸæˆæŠ¥å‘Š
        end_time = datetime.now()
        duration = end_time - start_time
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š æµ‹è¯•å®ŒæˆæŠ¥å‘Š")
        print(f"{'='*60}")
        print(f"æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"æˆåŠŸæµ‹è¯•: {successful_tests}")
        print(f"å¤±è´¥æµ‹è¯•: {total_tests - successful_tests}")
        print(f"æˆåŠŸç‡: {successful_tests/total_tests*100:.1f}%")
        print(f"è¿è¡Œæ—¶é—´: {duration}")
        
        if self.failed_tests:
            print(f"\nâŒ å¤±è´¥çš„æµ‹è¯•:")
            for failed in self.failed_tests:
                print(f"  - {failed['test_id']}: {failed['error']}")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        with open('test_results_detailed.json', 'w', encoding='utf-8') as f:
            json.dump({
                'summary': {
                    'total_tests': total_tests,
                    'successful_tests': successful_tests,
                    'failed_tests': total_tests - successful_tests,
                    'success_rate': successful_tests/total_tests*100,
                    'duration': str(duration),
                    'timestamp': datetime.now().isoformat()
                },
                'detailed_results': self.test_results,
                'failed_tests': self.failed_tests
            }, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜: test_results_detailed.json")

def main():
    runner = EnhancedTestRunner(
        max_retries=3,      # æœ€å¤šé‡è¯•3æ¬¡
        timeout_seconds=60, # 60ç§’è¶…æ—¶
        retry_delay=5       # é‡è¯•é—´éš”5ç§’
    )
    runner.run_all_tests()

if __name__ == "__main__":
    main()
