# å…±äº«å·¥å…·å‡½æ•°

import ollama
import os
import shutil
import tempfile
import time
import json
import subprocess
import sys
import requests
from pathlib import Path
from dotenv import load_dotenv
from config.config import OLLAMA_HOST, TEST_WORKSPACE_DIR, LOG_DIR, REPORT_DIR, MODELS_LIST_FILE
from cloud_connection_cache import connection_cache

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# === å¤–éƒ¨APIé…ç½® - ä»ç¯å¢ƒå˜é‡è¯»å– ===
OPENROUTER_API_URL = os.getenv("OPENROUTER_API_URL", "https://openrouter.ai/api/v1/chat/completions")
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")

TOGETHER_API_URL = os.getenv("TOGETHER_API_URL", "https://api.together.xyz/v1/chat/completions")
TOGETHER_API_KEY = os.getenv("TOGETHER_API_KEY")

QINIU_API_URL = os.getenv("QINIU_API_URL", "https://api.qnaigc.com/v1/chat/completions")
QINIU_API_KEY = os.getenv("QINIU_API_KEY")
QINIU_GROUP = os.getenv("QINIU_GROUP", "DeepSeek")

PPINFRA_API_URL = os.getenv("PPINFRA_API_URL", "https://api.ppinfra.com/v3/openai/chat/completions")
PPINFRA_API_KEY = os.getenv("PPINFRA_API_KEY")

GEMINI_API_URL = os.getenv("GEMINI_API_URL", "https://generativelanguage.googleapis.com/v1beta/models")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

DASHSCOPE_API_URL = os.getenv("DASHSCOPE_API_URL", "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions")
DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY")

GLM_API_URL = os.getenv("GLM_API_URL", "https://open.bigmodel.cn/api/paas/v4/chat/completions")
GLM_API_KEY = os.getenv("GLM_API_KEY")

BAIDU_API_KEY = os.getenv("BAIDU_API_KEY")
BAIDU_SECRET_KEY = os.getenv("BAIDU_SECRET_KEY")
BAIDU_TOKEN_URL = os.getenv("BAIDU_TOKEN_URL", "https://aip.baidubce.com/oauth/2.0/token")
BAIDU_API_BASE = os.getenv("BAIDU_API_BASE", "https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat")
BAIDU_ACCESS_TOKEN = None  # åŠ¨æ€è·å–
BAIDU_TOKEN_EXPIRE_TIME = 0

def get_baidu_access_token():
    """è·å–ç™¾åº¦äº‘access_token"""
    global BAIDU_ACCESS_TOKEN, BAIDU_TOKEN_EXPIRE_TIME
    
    # æ£€æŸ¥tokenæ˜¯å¦è¿˜æœ‰æ•ˆï¼ˆæå‰5åˆ†é’Ÿåˆ·æ–°ï¼‰
    if BAIDU_ACCESS_TOKEN and time.time() < BAIDU_TOKEN_EXPIRE_TIME - 300:
        return BAIDU_ACCESS_TOKEN
    
    url = "https://aip.baidubce.com/oauth/2.0/token"
    params = {
        "grant_type": "client_credentials",
        "client_id": BAIDU_API_KEY,
        "client_secret": BAIDU_SECRET_KEY
    }
    
    try:
        response = requests.post(url, params=params, timeout=30)
        response.raise_for_status()
        data = response.json()
        
        BAIDU_ACCESS_TOKEN = data["access_token"]
        BAIDU_TOKEN_EXPIRE_TIME = time.time() + data.get("expires_in", 2592000)  # é»˜è®¤30å¤©
        
        print(f"    âœ… ç™¾åº¦äº‘access_tokenè·å–æˆåŠŸ")
        return BAIDU_ACCESS_TOKEN
        
    except Exception as e:
        print(f"    âŒ è·å–ç™¾åº¦äº‘access_tokenå¤±è´¥: {e}")
        return None

def call_baidu_llm(model_name: str, messages: list, options: dict = None, max_retries: int = 3) -> tuple:
    """è°ƒç”¨ç™¾åº¦äº‘LLM API"""
    if options is None:
        options = {}
    
    # è·å–access_token
    access_token = get_baidu_access_token()
    if not access_token:
        return "[API Error: Failed to get Baidu access token]", None
    
    # æ¨¡å‹æ˜ å°„åˆ°å¯¹åº”çš„APIç«¯ç‚¹
    model_endpoints = {
        "ernie-4.0-8k": "completions_pro",
        "ernie-4.0-8k-preview": "ernie-4.0-8k-preview",
        "ernie-3.5-8k": "completions",
        "ernie-3.5-8k-0205": "ernie-3.5-8k-0205",
        "ernie-bot-turbo": "eb-instant",
        "ernie-bot": "completions",
        "ernie-speed-8k": "ernie_speed",
        "ernie-speed-128k": "ernie-speed-128k",
        "ernie-lite-8k": "ernie-lite-8k",
        "ernie-tiny-8k": "ernie-tiny-8k"
    }
    
    endpoint = model_endpoints.get(model_name, "completions")
    url = f"https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/{endpoint}?access_token={access_token}"
    
    headers = {
        "Content-Type": "application/json"
    }
    
    # è½¬æ¢æ¶ˆæ¯æ ¼å¼ - ç™¾åº¦äº‘éœ€è¦ç‰¹å®šæ ¼å¼
    baidu_messages = []
    system_content = ""
    
    for msg in messages:
        if msg["role"] == "system":
            system_content = msg["content"]
        elif msg["role"] in ["user", "assistant"]:
            baidu_messages.append({
                "role": msg["role"],
                "content": msg["content"]
            })
    
    payload = {
        "messages": baidu_messages,
        "temperature": options.get("temperature", 0.7),
        "top_p": options.get("top_p", 0.8),
        "penalty_score": options.get("penalty_score", 1.0),
        "max_output_tokens": options.get("max_tokens", 4096),
        "stream": False
    }
    
    # å¦‚æœæœ‰ç³»ç»Ÿæ¶ˆæ¯ï¼Œæ·»åŠ åˆ°payloadä¸­
    if system_content:
        payload["system"] = system_content
    
    for attempt in range(max_retries):
        try:
            print(f"    ğŸ”— Calling ç™¾åº¦äº‘ API: {model_name}")
            
            response = requests.post(url, headers=headers, json=payload, timeout=120)
            response.raise_for_status()
            data = response.json()
            
            # æ£€æŸ¥APIé”™è¯¯
            if "error_code" in data:
                print(f"    âŒ ç™¾åº¦äº‘APIé”™è¯¯: {data.get('error_msg', 'Unknown error')}")
                if attempt < max_retries - 1:
                    time.sleep(2)
                    continue
                else:
                    return f"[API Error: Baidu API error - {data.get('error_msg', 'Unknown')}]", None
            
            # æå–å“åº”å†…å®¹
            if "result" in data:
                content = data["result"]
                if content and content.strip():
                    print(f"    âœ… ç™¾åº¦äº‘ '{model_name}' success: {len(content)} chars")
                    return content, {"role": "assistant", "content": content}
                else:
                    print(f"    âš ï¸ Empty response from ç™¾åº¦äº‘ on attempt {attempt + 1}")
            else:
                print(f"    âš ï¸ No result in response: {data}")
                
        except requests.exceptions.HTTPError as e:
            print(f"    âŒ ç™¾åº¦äº‘ HTTP error on attempt {attempt + 1}: {e}")
            if hasattr(e, 'response'):
                print(f"    Response: {e.response.text}")
            if attempt < max_retries - 1:
                time.sleep(2)
                continue
        except Exception as e:
            print(f"    âŒ ç™¾åº¦äº‘ API error on attempt {attempt + 1}: {e}")
            if attempt < max_retries - 1:
                time.sleep(2)
                continue
    
    return f"[API Error: ç™¾åº¦äº‘ failed after {max_retries} attempts]", None

# --- Ollama Client Initialization ---
try:
    # === å¤–éƒ¨APIé…ç½® ===
    QINIU_API_URL = "https://api.qnaigc.com/v1/chat/completions"
    QINIU_API_KEY = "sk-85a07f1fd99e9ebb760104e7257a8678c0f0e018fd1a22019e4506323b6db0af"
    QINIU_GROUP = "DeepSeek"

    # --- æ–°å¢ Together.ai é…ç½® ---
    TOGETHER_API_URL = "https://api.together.xyz/v1/chat/completions"
    TOGETHER_API_KEY = "08f6372819b36b569686377e08dc82c61d32a984c4ce5a6e3c00f5a92d33a0f6"  # TODO: æ›¿æ¢ä¸ºä½ çš„Together.aiå¯†é’¥


    client = ollama.Client(host=OLLAMA_HOST)
    # å°è¯•è¿æ¥ï¼Œä»¥ä¾¿å°½æ—©å‘ç°é—®é¢˜
    client.list()
    print(f"[INFO] Connected to Ollama at {OLLAMA_HOST}")
except Exception as e:
    print(f"--- FATAL ERROR ---")
    print(f"æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡äº {OLLAMA_HOST}")
    print(f"è¯·ç¡®ä¿Ollamaæ­£åœ¨è¿è¡Œï¼Œå¹¶ä¸”config.pyä¸­çš„OLLAMA_HOSTé…ç½®æ­£ç¡®ã€‚")
    print(f"é”™è¯¯è¯¦æƒ…: {e}")
    # Allow proceeding for cases where ollama is not strictly required by all scripts, but exit if critical.
    # For now, we'll allow continuation but warn. For robustness, consider more granular error handling per script.
    pass

# --- å¤–éƒ¨APIè°ƒç”¨å‡½æ•° ---
def call_qiniu_deepseek(messages: list, options: dict = None, max_retries: int = 3) -> tuple:
    """
    è°ƒç”¨ä¸ƒç‰›äº‘ DeepSeek (OpenAIå…¼å®¹) API
    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ ¼å¼ä¸º [{"role": "user", "content": "..."}]
        options: é€‰é¡¹å‚æ•°ï¼ˆæ¸©åº¦ç­‰ï¼‰
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    Returns:
        tuple: (content, response_message)
    """
    headers = {
        "Authorization": f"Bearer {QINIU_API_KEY}",
        "Content-Type": "application/json"
    }

    # æå–ç”¨æˆ·æ¶ˆæ¯å†…å®¹
    user_content = ""
    for msg in messages:
        if msg["role"] == "user":
            user_content += msg["content"] + "\n"

    payload = {
        "model": "deepseek-v3",
        "messages": messages,
        "max_tokens": 1024,
        "temperature": options.get("temperature", 0.7) if options else 0.7,
    }

    for attempt in range(max_retries):
        try:
            response = requests.post(QINIU_API_URL, headers=headers, json=payload, timeout=240)
            response.raise_for_status()
            data = response.json()
            content = data['choices'][0]['message']['content']

            if content and content.strip():
                print(f"    âœ… Qiniu DeepSeek success: {len(content)} chars")
                # æ„é€ ä¸Ollamaå…¼å®¹çš„å“åº”æ ¼å¼
                response_message = {
                    "role": "assistant",
                    "content": content
                }
                return content, response_message
            else:
                print(f"    âš ï¸ Empty response from Qiniu DeepSeek on attempt {attempt + 1}")

        except Exception as e:
            print(f"    âŒ Qiniu DeepSeek API error on attempt {attempt + 1}: {e}")
            if attempt < max_retries - 1:
                time.sleep(2)
                continue

    error_msg = f"[API Error: Qiniu DeepSeek API failed after {max_retries} attempts]"
    return error_msg, None

def call_togetherai(model_name: str, messages: list, options: dict = None, max_retries: int = 3) -> tuple:
    """
    è°ƒç”¨ Together.ai (OpenAIå…¼å®¹) API
    Args:
        model_name: è¦åœ¨Together.aiä¸Šè°ƒç”¨çš„æ¨¡å‹åç§° (å¦‚ 'mistralai/Mixtral-8x7B-Instruct-v0.1')
        messages: æ¶ˆæ¯åˆ—è¡¨
        options: é€‰é¡¹å‚æ•° (æ¸©åº¦ç­‰)
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    Returns:
        tuple: (content, response_message)
    """
    headers = {
        "Authorization": f"Bearer {TOGETHER_API_KEY}",
        "Content-Type": "application/json",
    }
    if options is None:
        options = {}
    payload = {
        "model": model_name,
        "messages": messages,
        "max_tokens": options.get("max_tokens", 4096),
        "temperature": options.get("temperature", 0.7),
        "top_p": options.get("top_p", 0.7),
        "top_k": options.get("top_k", 50),
        "repetition_penalty": options.get("repetition_penalty", 1)
    }
    for attempt in range(max_retries):
        try:
            response = requests.post(TOGETHER_API_URL, headers=headers, json=payload, timeout=120)
            response.raise_for_status()
            data = response.json()
            content = data['choices'][0]['message']['content']
            if content and content.strip():
                print(f"    âœ… Together.ai '{model_name}' success: {len(content)} chars")
                response_message = {
                    "role": "assistant",
                    "content": content
                }
                return content, response_message
            else:
                print(f"    âš ï¸ Empty response from Together.ai on attempt {attempt + 1}")
        except requests.exceptions.HTTPError as e:
            print(f"    âŒ Together.ai API HTTP error on attempt {attempt + 1}: {e.response.status_code} {e.response.reason}")
            print(f"    Response Body: {e.response.text}")
            if attempt < max_retries - 1:
                time.sleep(2)
                continue
        except Exception as e:
            print(f"    âŒ Together.ai API error on attempt {attempt + 1}: {e}")
            if attempt < max_retries - 1:
                time.sleep(2)
                continue
    error_msg = f"[API Error: Together.ai API failed for model '{model_name}' after {max_retries} attempts]"
    return error_msg, None

def call_openrouter(model_name: str, messages: list, options: dict = None, max_retries: int = 3) -> tuple:
    """
    è°ƒç”¨ OpenRouter (OpenAIå…¼å®¹) API
    Args:
        model_name: OpenRouterä¸Šçš„æ¨¡å‹å (å¦‚ 'mistralai/mixtral-8x7b-instruct')
        messages: æ¶ˆæ¯åˆ—è¡¨
        options: é€‰é¡¹å‚æ•° (æ¸©åº¦ç­‰)
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    Returns:
        tuple: (content, response_message)
    """
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json",
    }
    if options is None:
        options = {}
    payload = {
        "model": model_name,
        "messages": messages,
        "max_tokens": options.get("max_tokens", 4096),
        "temperature": options.get("temperature", 0.7),
        "top_p": options.get("top_p", 0.7),
        "top_k": options.get("top_k", 50),
        "repetition_penalty": options.get("repetition_penalty", 1)
    }
    for attempt in range(max_retries):
        try:
            response = requests.post(OPENROUTER_API_URL, headers=headers, json=payload, timeout=120)
            response.raise_for_status()
            data = response.json()
            content = data['choices'][0]['message']['content']
            if content and content.strip():
                print(f"    âœ… OpenRouter '{model_name}' success: {len(content)} chars")
                response_message = {
                    "role": "assistant",
                    "content": content
                }
                return content, response_message
            else:
                print(f"    âš ï¸ Empty response from OpenRouter on attempt {attempt + 1}")
        except requests.exceptions.HTTPError as e:
            print(f"    âŒ OpenRouter API HTTP error on attempt {attempt + 1}: {e.response.status_code} {e.response.reason}")
            print(f"    Response Body: {e.response.text}")
            if attempt < max_retries - 1:
                time.sleep(2)
                continue
        except Exception as e:
            print(f"    âŒ OpenRouter API error on attempt {attempt + 1}: {e}")
            if attempt < max_retries - 1:
                time.sleep(2)
                continue
    error_msg = f"[API Error: OpenRouter API failed for model '{model_name}' after {max_retries} attempts]"
    return error_msg, None

def call_multi_cloud_smart(model_name, messages, options=None, max_retries=3):
    """
    æ™ºèƒ½äº‘ç«¯è°ƒç”¨ - åŸºäºè¿æ¥ç¼“å­˜ä¼˜åŒ–æœåŠ¡é€‰æ‹©é¡ºåº
    """
    # è·å–å¯ç”¨çš„APIå‡½æ•°æ˜ å°„
    api_mapping = {
        'openrouter': lambda: call_openrouter(model_name, messages, options, max_retries),
        'together': lambda: call_togetherai(model_name, messages, options, max_retries),
    }
    
    # è·å–ä¼˜åŒ–åçš„æœåŠ¡é¡ºåº
    available_services = list(api_mapping.keys())
    preferred_services = connection_cache.get_preferred_services(available_services)
    
    print(f"ğŸ”„ æ™ºèƒ½äº‘ç«¯è°ƒç”¨é¡ºåº: {preferred_services}")
    
    for service_name in preferred_services:
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡
        if connection_cache.should_skip_service(service_name):
            continue
            
        try:
            print(f"ğŸŒ å°è¯•æœåŠ¡: {service_name}")
            api_func = api_mapping[service_name]
            content, response_message = api_func()
            
            if content and not str(content).startswith("[API Error"):
                print(f"âœ… {service_name} è¿æ¥æˆåŠŸ")
                connection_cache.mark_service_success(service_name)
                return content, response_message
            else:
                print(f"âŒ {service_name} è¿”å›é”™è¯¯: {content}")
                connection_cache.mark_service_failed(service_name, "APIè¿”å›é”™è¯¯")
                
        except Exception as e:
            print(f"âŒ {service_name} è¿æ¥å¼‚å¸¸: {e}")
            connection_cache.mark_service_failed(service_name, str(e))
    
    # å…¨éƒ¨å¤±è´¥
    print("âŒ æ‰€æœ‰äº‘ç«¯æœåŠ¡éƒ½å¤±è´¥")
    return "[API Error: All cloud APIs failed]", None

# ä¿æŒåŸå‡½æ•°ä½œä¸ºå¤‡ç”¨
def call_multi_cloud(model_name, messages, options=None, max_retries=3):
    """
    åŸç‰ˆå¤šäº‘è°ƒç”¨å‡½æ•° - ä¿æŒå‘åå…¼å®¹
    """
    return call_multi_cloud_smart(model_name, messages, options, max_retries)

def call_ppinfra(model_name: str, messages: list, options: dict = None, max_retries: int = 3) -> tuple:
    """è°ƒç”¨ PPInfra (OpenAIå…¼å®¹) API"""
    headers = {
        "Authorization": f"Bearer {PPINFRA_API_KEY}",
        "Content-Type": "application/json",
    }
    if options is None:
        options = {}
    payload = {
        "model": model_name,
        "messages": messages,
        "max_tokens": options.get("max_tokens", 4096),
        "temperature": options.get("temperature", 0.7),
    }
    for attempt in range(max_retries):
        try:
            response = requests.post(PPINFRA_API_URL, headers=headers, json=payload, timeout=120)
            response.raise_for_status()
            data = response.json()
            content = data['choices'][0]['message']['content']
            if content and content.strip():
                print(f"    âœ… PPInfra '{model_name}' success: {len(content)} chars")
                return content, {"role": "assistant", "content": content}
        except Exception as e:
            print(f"    âŒ PPInfra '{model_name}' error on attempt {attempt + 1}: {e}")
            if attempt < max_retries - 1:
                time.sleep(2)
    return f"[API Error: PPInfra failed after {max_retries} attempts]", None

def call_gemini(model_name: str, messages: list, options: dict = None, max_retries: int = 3) -> tuple:
    """è°ƒç”¨ Google Gemini API"""
    headers = {
        "x-goog-api-key": GEMINI_API_KEY,
        "Content-Type": "application/json",
    }
    if options is None:
        options = {}
    
    # ä¿®æ­£æ¨¡å‹åç§°æ˜ å°„
    model_mapping = {
        "gemini-1.5-flash-latest": "gemini-1.5-flash",
        "gemini-1.5-pro-latest": "gemini-1.5-pro", 
        "gemini-1.5-flash": "gemini-1.5-flash",
        "gemini-1.5-pro": "gemini-1.5-pro",
        "gemini-2.0-flash-exp": "gemini-2.0-flash-exp"
    }
    
    actual_model = model_mapping.get(model_name, model_name)
    
    # è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸ºGeminiæ ¼å¼
    contents = []
    system_instruction = None
    
    for i, msg in enumerate(messages):
        if msg["role"] == "system":
            system_instruction = msg["content"]
        elif msg["role"] == "user":
            contents.append({
                "role": "user",
                "parts": [{"text": msg["content"]}]
            })
        elif msg["role"] == "assistant":
            contents.append({
                "role": "model", 
                "parts": [{"text": msg["content"]}]
            })
    
    # æ„å»ºè¯·æ±‚payload
    payload = {
        "contents": contents,
        "generationConfig": {
            "temperature": options.get("temperature", 0.7),
            "maxOutputTokens": options.get("max_tokens", 4096),
            "topP": options.get("top_p", 0.95),
            "topK": options.get("top_k", 40),
        }
    }
    
    # å¦‚æœæœ‰ç³»ç»ŸæŒ‡ä»¤ï¼Œæ·»åŠ åˆ°payloadä¸­
    if system_instruction:
        payload["systemInstruction"] = {
            "parts": [{"text": system_instruction}]
        }
    
    for attempt in range(max_retries):
        try:
            url = f"{GEMINI_API_URL}/{actual_model}:generateContent"
            print(f"    ğŸ”— Calling Gemini API: {url}")
            
            response = requests.post(url, headers=headers, json=payload, timeout=120)
            
            # æ£€æŸ¥é…é¢é™åˆ¶
            if response.status_code == 429:
                print(f"    âš ï¸ Gemini APIé…é¢å·²ç”¨å®Œï¼Œè·³è¿‡æ­¤æ¨¡å‹")
                return "[API Error: Gemini quota exceeded - skipping]", None
            
            if response.status_code != 200:
                print(f"    âŒ HTTP {response.status_code}: {response.text}")
                response.raise_for_status()
            
            data = response.json()
            
            if 'candidates' not in data or not data['candidates']:
                print(f"    âš ï¸ No candidates in response")
                continue
                
            candidate = data['candidates'][0]
            if 'content' not in candidate or 'parts' not in candidate['content']:
                print(f"    âš ï¸ Invalid response structure")
                continue
                
            content = candidate['content']['parts'][0]['text']
            
            if content and content.strip():
                print(f"    âœ… Gemini '{model_name}' success: {len(content)} chars")
                return content, {"role": "assistant", "content": content}
            else:
                print(f"    âš ï¸ Empty response from Gemini on attempt {attempt + 1}")
                
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 429:
                print(f"    âš ï¸ Gemini APIé…é¢é™åˆ¶ï¼Œè·³è¿‡æµ‹è¯•")
                return "[API Error: Gemini quota exceeded]", None
            print(f"    âŒ Gemini HTTP error on attempt {attempt + 1}: {e}")
            if attempt < max_retries - 1:
                time.sleep(2)
                continue
        except Exception as e:
            print(f"    âŒ Gemini API error on attempt {attempt + 1}: {e}")
            if attempt < max_retries - 1:
                time.sleep(2)
                continue
    
    return f"[API Error: Gemini failed after {max_retries} attempts]", None

def call_dashscope(model_name: str, messages: list, options: dict = None, max_retries: int = 3) -> tuple:
    """è°ƒç”¨ é˜¿é‡Œäº‘DashScope (OpenAIå…¼å®¹) API"""
    headers = {
        "Authorization": f"Bearer {DASHSCOPE_API_KEY}",
        "Content-Type": "application/json",
    }
    if options is None:
        options = {}
    
    payload = {
        "model": model_name,
        "messages": messages,
        "max_tokens": options.get("max_tokens", 4096),
        "temperature": options.get("temperature", 0.7),
        "top_p": options.get("top_p", 0.9),
    }
    
    for attempt in range(max_retries):
        try:
            response = requests.post(DASHSCOPE_API_URL, headers=headers, json=payload, timeout=120)
            response.raise_for_status()
            data = response.json()
            content = data['choices'][0]['message']['content']
            if content and content.strip():
                print(f"    âœ… DashScope '{model_name}' success: {len(content)} chars")
                return content, {"role": "assistant", "content": content}
            else:
                print(f"    âš ï¸ Empty response from DashScope on attempt {attempt + 1}")
        except requests.exceptions.HTTPError as e:
            print(f"    âŒ DashScope HTTP error on attempt {attempt + 1}: {e.response.status_code} {e.response.reason}")
            print(f"    Response Body: {e.response.text}")
            if attempt < max_retries - 1:
                time.sleep(2)
                continue
        except Exception as e:
            print(f"    âŒ DashScope API error on attempt {attempt + 1}: {e}")
            if attempt < max_retries - 1:
                time.sleep(2)
                continue
    
    return f"[API Error: DashScope failed after {max_retries} attempts]", None

def call_glm(model_name: str, messages: list, options: dict = None, max_retries: int = 3) -> tuple:
    """è°ƒç”¨ æ™ºè°±AI GLM (OpenAIå…¼å®¹) API"""
    headers = {
        "Authorization": f"Bearer {GLM_API_KEY}",
        "Content-Type": "application/json",
    }
    if options is None:
        options = {}
    
    payload = {
        "model": model_name,
        "messages": messages,
        "max_tokens": options.get("max_tokens", 4096),
        "temperature": options.get("temperature", 0.7),
        "top_p": options.get("top_p", 0.9),
        "stream": False
    }
    
    for attempt in range(max_retries):
        try:
            response = requests.post(GLM_API_URL, headers=headers, json=payload, timeout=120)
            response.raise_for_status()
            data = response.json()
            content = data['choices'][0]['message']['content']
            if content and content.strip():
                print(f"    âœ… GLM '{model_name}' success: {len(content)} chars")
                return content, {"role": "assistant", "content": content}
            else:
                print(f"    âš ï¸ Empty response from GLM on attempt {attempt + 1}")
        except requests.exceptions.HTTPError as e:
            print(f"    âŒ GLM HTTP error on attempt {attempt + 1}: {e.response.status_code} {e.response.reason}")
            print(f"    Response Body: {e.response.text}")
            if attempt < max_retries - 1:
                time.sleep(2)
                continue
        except Exception as e:
            print(f"    âŒ GLM API error on attempt {attempt + 1}: {e}")
            if attempt < max_retries - 1:
                time.sleep(2)
                continue
    
    return f"[API Error: GLM failed after {max_retries} attempts]", None

# --- Test Execution Core ---
def run_single_test(pillar_name: str, prompt: str, model: str, options: dict, messages: list = None, test_script_name: str = None):
    """
    æ‰§è¡Œå•æ¬¡æµ‹è¯•å¹¶æ‰“å°ç»“æœçš„æ ¸å¿ƒå‡½æ•°ã€‚
    Args:
        pillar_name: å½“å‰æµ‹è¯•çš„Pillaråç§°ã€‚
        prompt: å½“å‰è½®æ¬¡çš„è¾“å…¥Promptã€‚
        model: è¦æµ‹è¯•çš„æ¨¡å‹åç§°ã€‚
        options: Ollama APIçš„é€‰é¡¹ã€‚
        messages: ç”¨äºå¤šè½®å¯¹è¯çš„å†å²æ¶ˆæ¯åˆ—è¡¨ã€‚
        test_script_name: å½“å‰æ­£åœ¨æ‰§è¡Œçš„æµ‹è¯•è„šæœ¬åï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰ã€‚
    Returns:
        Tuple[str, dict]: æ¨¡å‹çš„å“åº”å†…å®¹å’Œå®Œæ•´çš„å“åº”æ¶ˆæ¯å¯¹è±¡ã€‚
    """
    print("\n" + "="*80)
    print(f"  Pillar: {pillar_name}")
    if test_script_name:
        print(f"  Script: {test_script_name}")
    print("="*80)
    
    current_messages = messages.copy() if messages else []
    
    if prompt:
        print(f"\n[Prompt]")
        print(prompt)
        current_messages.append({'role': 'user', 'content': prompt})
    else: # ç”¨äºå¤šè½®å¯¹è¯çš„åœºæ™¯ï¼Œç¬¬ä¸€è½®promptåœ¨å¤–éƒ¨å¤„ç†
        print(f"\n[Multi-turn Context Provided]")

    print(f"\n[Model: {model}] [Options: {options}]")
    print("--- MODEL RESPONSE (RAW) ---")
    
    try:
        start_time = time.time()

        # --- æ–°å¢è‡ªåŠ¨å¤šäº‘APIè½®è¯¢é€»è¾‘ ---
        if model.startswith("auto/"):
            actual_model_name = model.split('/', 1)[1]
            print(f"    Auto-trying all cloud APIs for model: {actual_model_name}")
            content, response_message = call_multi_cloud(actual_model_name, current_messages, options)
            if content == "[API Error: All cloud APIs failed]":
                print("æ‰€æœ‰äº‘APIå‡å¤±è´¥ã€‚\n")
                return content, response_message
        elif model.startswith("openrouter/"):
            actual_model_name = model.split('/', 1)[1]
            print(f"    Routing to OpenRouter with model: {actual_model_name}")
            content, response_message = call_openrouter(actual_model_name, current_messages, options)
        elif model.startswith("together/"):
            actual_model_name = model.split('/', 1)[1]
            print(f"    Routing to Together.ai with model: {actual_model_name}")
            content, response_message = call_togetherai(actual_model_name, current_messages, options)
        elif model == "deepseek-v3-qiniu":
            print("    Routing to Qiniu DeepSeek API")
            content, response_message = call_qiniu_deepseek(current_messages, options)
        elif model.startswith("ppinfra/"):
            actual_model_name = model.split('/', 1)[1]
            print(f"    Routing to PPInfra with model: {actual_model_name}")
            content, response_message = call_ppinfra(actual_model_name, current_messages, options)
        elif model.startswith("gemini/"):
            actual_model_name = model.split('/', 1)[1]
            print(f"    Routing to Gemini with model: {actual_model_name}")
            content, response_message = call_gemini(actual_model_name, current_messages, options)
        elif model.startswith("dashscope/"):
            actual_model_name = model.split('/', 1)[1]
            print(f"    Routing to DashScope with model: {actual_model_name}")
            content, response_message = call_dashscope(actual_model_name, current_messages, options)
        elif model.startswith("glm/"):
            actual_model_name = model.split('/', 1)[1]
            print(f"    Routing to GLM with model: {actual_model_name}")
            content, response_message = call_glm(actual_model_name, current_messages, options)
        elif model.startswith("baidu/"):
            actual_model_name = model.split('/', 1)[1]
            print(f"    Routing to ç™¾åº¦äº‘ with model: {actual_model_name}")
            content, response_message = call_baidu_llm(actual_model_name, current_messages, options)
        else:
            print("    Routing to local Ollama")
            response = client.chat(
                model=model,
                messages=current_messages,
                options=options
            )
            content = response['message']['content']
            response_message = response['message']
        # --- END ---

        end_time = time.time()

        print(content)

        print("--- END OF RESPONSE ---")
        duration = end_time - start_time
        print(f"Response generated in: {duration:.2f} seconds.")

        return content, response_message # è¿”å›å†…å®¹å’Œå®Œæ•´çš„æ¶ˆæ¯å¯¹è±¡ä»¥ä¾›å¤šè½®æµ‹è¯•ä½¿ç”¨

    except Exception as e:
        print(f"!!! ERROR during API call: {e}")
        return f"ERROR: {e}", None

def print_assessment_criteria(criteria: str):
    """
    æ ¼å¼åŒ–å¹¶æ‰“å°è¯„ä¼°æ ‡å‡†ã€‚
    """
    print("\n[Assessment Criteria]")
    print(criteria.strip())
    print("="*80)

# --- Environment Management ---
def setup_test_environment(subdir_name: str = None) -> str:
    """ä¸ºå±‚çº§ä¸‰/å·¥ä½œæµæµ‹è¯•åˆ›å»ºä¸´æ—¶å·¥ä½œç›®å½•"""
    base_dir = TEST_WORKSPACE_DIR
    if subdir_name:
        base_dir = os.path.join(base_dir, subdir_name)
        
    if os.path.exists(base_dir):
        shutil.rmtree(base_dir)
    os.makedirs(base_dir)
    print(f"\n*** [SETUP] Created temporary workspace for tests: {base_dir} ***")
    return base_dir

def cleanup_test_environment(work_dir: str):
    """æ¸…ç†å±‚çº§ä¸‰/å·¥ä½œæµæµ‹è¯•çš„ä¸´æ—¶å·¥ä½œç›®å½•"""
    if os.path.exists(work_dir):
        shutil.rmtree(work_dir)
        print(f"\n*** [CLEANUP] Removed temporary workspace: {work_dir} ***")

def ensure_log_dir():
    """ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨"""
    if not os.path.exists(LOG_DIR):
        os.makedirs(LOG_DIR)
        print(f"[INFO] Created log directory: {LOG_DIR}")

def ensure_report_dir():
    """ç¡®ä¿æŠ¥å‘Šç›®å½•å­˜åœ¨"""
    if not os.path.exists(REPORT_DIR):
        os.makedirs(REPORT_DIR)
        print(f"[INFO] Created report directory: {REPORT_DIR}")

# --- File Operations ---
def save_file(filepath, content):
    """ä¿å­˜æ–‡ä»¶åˆ°æŒ‡å®šè·¯å¾„"""
    try:
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        with open(filepath, 'w', encoding='utf-8', newline='\n') as f:
            f.write(content)
        print(f"[INFO] Saved file: {filepath}")
    except Exception as e:
        print(f"[ERROR] Failed to save file {filepath}: {e}")
        
def read_file(filepath):
    """è¯»å–æ–‡ä»¶å†…å®¹"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return f.read()
    except FileNotFoundError:
        print(f"[ERROR] File not found: {filepath}")
        return None
    except Exception as e:
        print(f"[ERROR] Failed to read file {filepath}: {e}")
        return None

# --- Subprocess Execution ---
def execute_bash_script(script_path: str, cwd: str = None) -> str:
    """
    æ‰§è¡Œbashè„šæœ¬å¹¶è¿”å›å…¶æ ‡å‡†è¾“å‡ºã€‚
    Args:
        script_path: bashè„šæœ¬çš„å®Œæ•´è·¯å¾„ã€‚
        cwd: å‘½ä»¤æ‰§è¡Œçš„å·¥ä½œç›®å½•ã€‚
    Returns:
        str: è„šæœ¬çš„æ ‡å‡†è¾“å‡ºã€‚
    """
    print(f"\n--- EXECUTING BASH SCRIPT: {script_path} ---")
    try:
        # Make the script executable
        subprocess.run(['chmod', '+x', script_path], check=True, cwd=cwd)
        result = subprocess.run(
            ['/bin/bash', script_path],
            capture_output=True,
            text=True,
            check=True,
            encoding='utf-8',
            cwd=cwd
        )
        output = result.stdout
        if result.stderr:
            print(f"[STDERR] from {script_path}:\n{result.stderr}")
        print(f"--- END OF BASH SCRIPT OUTPUT ---")
        return output

    except FileNotFoundError:
        return f"ERROR: Bash script not found at {script_path}"
    except subprocess.CalledProcessError as e:
        error_msg = f"ERROR: Bash script execution failed for {script_path}. Return code: {e.returncode}\n"
        error_msg += f"STDOUT:\n{e.stdout}\n"
        error_msg += f"STDERR:\n{e.stderr}\n"
        print(error_msg)
        return f"ERROR: Script execution failed."
    except Exception as e:
        return f"ERROR: An unexpected error occurred during script execution: {e}"

def check_ollama_dependency():
    """æ£€æŸ¥å¹¶å®‰è£…ollama PythonåŒ…"""
    try:
        __import__('ollama')
        print("[SETUP] 'ollama' python package is already installed.")
    except ImportError:
        print("[SETUP] 'ollama' python package not found. Attempting to install...")
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", "ollama"])
            print("[SETUP] 'ollama' installed successfully.")
        except subprocess.CalledProcessError:
            print("[ERROR] Failed to install 'ollama'. Please install it manually using 'pip install ollama'.")
            print("[INFO] On Windows, you may need to run this script from a terminal with Administrator privileges.")
            sys.exit(1)
    # Check if ollama service is running after dependency check
    try:
        client.list()
        print(f"[INFO] Ollama service is reachable at {OLLAMA_HOST}")
    except Exception as e:
        print(f"[WARN] Could not reach Ollama service at {OLLAMA_HOST}. Ensure Ollama is running.")
        # Allow continuation but warn; specific tests might fail.
        pass

# --- Helper for modifying test scripts ---
def modify_test_script(script_content: str, model_name: str) -> str:
    """
    ä¿®æ”¹æµ‹è¯•è„šæœ¬å†…å®¹ï¼Œå°† MODEL_TO_TEST æ›¿æ¢ä¸ºæŒ‡å®šçš„æ¨¡å‹åç§°ã€‚
    """
    lines = script_content.splitlines()
    modified_lines = []
    for line in lines:
        if line.strip().startswith('MODEL_TO_TEST'):
            modified_lines.append(f"MODEL_TO_TEST = '{model_name}'")
        else:
            modified_lines.append(line)
    return "\n".join(modified_lines)

# --- Enhanced Test Execution with Retry Logic ---
def run_single_test_with_retry(pillar_name: str, prompt: str, model: str, options: dict,
                              messages: list = None, test_script_name: str = None,
                              max_retries: int = 3, retry_delay: int = 2):
    """
    å¸¦é‡è¯•æœºåˆ¶çš„æµ‹è¯•æ‰§è¡Œå‡½æ•°ï¼Œå¤„ç†é›¶å“åº”å’ŒAPIé”™è¯¯
    """
    for attempt in range(max_retries):
        try:
            content, response_message = run_single_test(
                pillar_name, prompt, model, options, messages, test_script_name
            )

            # æ£€æŸ¥æ˜¯å¦ä¸ºé›¶å“åº”æˆ–æ— æ•ˆå“åº”
            if not content or content.strip() == "" or "ERROR:" in content:
                if attempt < max_retries - 1:
                    print(f"[RETRY] Attempt {attempt + 1} failed, retrying in {retry_delay} seconds...")
                    time.sleep(retry_delay)
                    continue
                else:
                    print(f"[FAIL] All {max_retries} attempts failed for {pillar_name}")
                    return content, response_message
            else:
                if attempt > 0:
                    print(f"[SUCCESS] Test succeeded on attempt {attempt + 1}")
                return content, response_message

        except Exception as e:
            if attempt < max_retries - 1:
                print(f"[RETRY] Attempt {attempt + 1} failed with error: {e}, retrying...")
                time.sleep(retry_delay)
                continue
            else:
                print(f"[FAIL] All {max_retries} attempts failed with error: {e}")
                return f"ERROR: {e}", None

    return "ERROR: Max retries exceeded", None

# --- Adaptive Prompts Integration ---
def get_adaptive_prompt(model_name: str, base_prompt: str, prompt_type: str = "default"):
    """
    æ ¹æ®æ¨¡å‹ç±»å‹å’Œæç¤ºç±»å‹è·å–è‡ªé€‚åº”æç¤ºè¯
    """
    try:
        from adaptive_prompts import ADAPTIVE_SYSTEM_PROMPTS

        # æ£€æŸ¥æ˜¯å¦ä¸ºatlasæ¨¡å‹ï¼ˆéœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰
        if "atlas" in model_name.lower():
            if prompt_type in ADAPTIVE_SYSTEM_PROMPTS.get("atlas", {}):
                system_prompt = ADAPTIVE_SYSTEM_PROMPTS["atlas"][prompt_type]
                # å¯¹atlasæ¨¡å‹è¿›è¡Œé•¿åº¦é™åˆ¶
                if len(base_prompt) > 360:  # åŸºäºç”¨æˆ·åå¥½çš„æ”¾å®½é™åˆ¶
                    base_prompt = base_prompt[:360] + "..."
                return system_prompt, base_prompt

        # å…¶ä»–æ¨¡å‹ä½¿ç”¨é€šç”¨è‡ªé€‚åº”æç¤º
        if prompt_type in ADAPTIVE_SYSTEM_PROMPTS.get("general", {}):
            system_prompt = ADAPTIVE_SYSTEM_PROMPTS["general"][prompt_type]
            return system_prompt, base_prompt

    except ImportError:
        print("[WARN] adaptive_prompts module not found, using default prompts")
    except Exception as e:
        print(f"[WARN] Error loading adaptive prompts: {e}")

    # å›é€€åˆ°é»˜è®¤è¡Œä¸º
    return None, base_prompt
