#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ‰¹é‡å¯è§£é‡Šè®¤çŸ¥ç”Ÿæ€ç³»ç»Ÿæµ‹è¯•

å¯¹å¤šä¸ªäº‘æ¨¡å‹è¿›è¡Œè¯¦ç»†çš„è®¤çŸ¥ç”Ÿæ€ç³»ç»Ÿæµ‹è¯•ï¼Œæä¾›å®Œæ•´çš„è¯„åˆ†è§£é‡Šã€‚
"""

import sys
import os
import json
import time
from datetime import datetime
from typing import Dict, List, Any, Tuple
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append('.')

# å¯¼å…¥å¯è§£é‡Šæµ‹è¯•æ¨¡å—
from run_explainable_cognitive_test import run_explainable_test, ExplainableScorer

def get_available_test_models() -> List[Tuple[str, str]]:
    """è·å–å¯ç”¨çš„æµ‹è¯•æ¨¡å‹åˆ—è¡¨"""
    return [
        ('siliconflow', 'THUDM/glm-4-9b-chat'),
        ('siliconflow', 'Qwen/Qwen2.5-7B-Instruct'),
        ('together', 'mistralai/Mixtral-8x7B-Instruct-v0.1'),
        ('together', 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo'),
        ('ppinfra', 'qwen/qwen3-235b-a22b-fp8'),
        ('ppinfra', 'meta-llama/llama-3.1-405b-instruct'),
        ('glm', 'glm-4-plus'),
        ('glm', 'glm-4-0520'),
        ('gemini', 'gemini-1.5-flash'),
        ('gemini', 'gemini-1.5-pro')
    ]

def run_batch_explainable_tests():
    """è¿è¡Œæ‰¹é‡å¯è§£é‡Šæµ‹è¯•"""
    print("ğŸ§  æ‰¹é‡å¯è§£é‡Šè®¤çŸ¥ç”Ÿæ€ç³»ç»Ÿæµ‹è¯•")
    print("=" * 60)
    
    test_models = get_available_test_models()
    print(f"ğŸ“‹ è®¡åˆ’æµ‹è¯• {len(test_models)} ä¸ªæ¨¡å‹:")
    
    for i, (service, model) in enumerate(test_models, 1):
        print(f"  {i:2d}. {service:12s} / {model}")
    
    print(f"\nğŸ¯ æµ‹è¯•ç‰¹ç‚¹:")
    print(f"  - è¯¦ç»†çš„è¯„åˆ†è§£é‡Šå’Œè®¡ç®—è¿‡ç¨‹")
    print(f"  - å®æ—¶æ˜¾ç¤ºæµ‹è¯•è¿›åº¦å’Œç»“æœ")
    print(f"  - ä¿å­˜å®Œæ•´çš„æµ‹è¯•æ•°æ®å’Œè§£é‡Š")
    
    # è¯¢é—®æ˜¯å¦ç»§ç»­
    confirm = input(f"\næ˜¯å¦å¼€å§‹æ‰¹é‡æµ‹è¯•ï¼Ÿ(y/N): ").strip().lower()
    if confirm not in ['y', 'yes', 'æ˜¯']:
        print("æµ‹è¯•å·²å–æ¶ˆ")
        return
    
    # å¼€å§‹æ‰¹é‡æµ‹è¯•
    start_time = datetime.now()
    all_results = {}
    successful_tests = []
    failed_tests = []
    
    print(f"\nğŸš€ å¼€å§‹æ‰¹é‡æµ‹è¯• - {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    
    for i, (service_name, model_name) in enumerate(test_models, 1):
        print(f"\nğŸ“ è¿›åº¦: {i}/{len(test_models)} - {service_name}/{model_name}")
        print("â”€" * 60)
        
        try:
            # è¿è¡Œå•ä¸ªæ¨¡å‹çš„è¯¦ç»†æµ‹è¯•
            result = run_explainable_test(service_name, model_name)
            
            # ä¿å­˜ç»“æœ
            model_key = f"{service_name}/{model_name}"
            all_results[model_key] = result
            
            if result.get('status') == 'success':
                successful_tests.append(result)
                scores = result['scores']
                print(f"âœ… æµ‹è¯•æˆåŠŸ - ç»¼åˆå¾—åˆ†: {scores['overall_score']:.3f}")
                print(f"   å¹»è§‰æŠµæŠ—: {scores['hallucination_resistance']:.3f} | "
                      f"è§’è‰²ä¸€è‡´æ€§: {scores['role_consistency']:.3f} | "
                      f"è®¤çŸ¥å¤šæ ·æ€§: {scores['cognitive_diversity']:.3f}")
            else:
                failed_tests.append(result)
                print(f"âŒ æµ‹è¯•å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        
        except Exception as e:
            print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}")
            failed_result = {
                'model_name': f"{service_name}/{model_name}",
                'status': 'failed',
                'error': str(e),
                'test_timestamp': datetime.now().isoformat()
            }
            all_results[f"{service_name}/{model_name}"] = failed_result
            failed_tests.append(failed_result)
        
        # æ·»åŠ å»¶è¿Ÿä»¥é¿å…APIé™åˆ¶
        if i < len(test_models):
            print("â³ ç­‰å¾…3ç§’åç»§ç»­ä¸‹ä¸€ä¸ªæµ‹è¯•...")
            time.sleep(3)
    
    end_time = datetime.now()
    total_duration = (end_time - start_time).total_seconds()
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    print(f"\n" + "=" * 80)
    print(f"ğŸ“Š æ‰¹é‡æµ‹è¯•å®Œæˆæ±‡æ€»")
    print(f"=" * 80)
    
    print(f"ğŸ• æµ‹è¯•æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')} - {end_time.strftime('%H:%M:%S')}")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_duration:.1f}ç§’ ({total_duration/60:.1f}åˆ†é’Ÿ)")
    print(f"ğŸ“ˆ æµ‹è¯•ç»Ÿè®¡:")
    print(f"   æ€»æµ‹è¯•æ•°: {len(test_models)}")
    print(f"   æˆåŠŸæµ‹è¯•: {len(successful_tests)}")
    print(f"   å¤±è´¥æµ‹è¯•: {len(failed_tests)}")
    print(f"   æˆåŠŸç‡: {len(successful_tests)/len(test_models):.1%}")
    
    if successful_tests:
        # è®¡ç®—å¹³å‡åˆ†æ•°
        avg_hallucination = sum(t['scores']['hallucination_resistance'] for t in successful_tests) / len(successful_tests)
        avg_consistency = sum(t['scores']['role_consistency'] for t in successful_tests) / len(successful_tests)
        avg_diversity = sum(t['scores']['cognitive_diversity'] for t in successful_tests) / len(successful_tests)
        avg_overall = sum(t['scores']['overall_score'] for t in successful_tests) / len(successful_tests)
        
        print(f"\nğŸ“ˆ å¹³å‡åˆ†æ•°:")
        print(f"   å¹»è§‰æŠµæŠ—: {avg_hallucination:.3f}")
        print(f"   è§’è‰²ä¸€è‡´æ€§: {avg_consistency:.3f}")
        print(f"   è®¤çŸ¥å¤šæ ·æ€§: {avg_diversity:.3f}")
        print(f"   ç»¼åˆå¾—åˆ†: {avg_overall:.3f}")
        
        # æ’åºå¹¶æ˜¾ç¤ºå‰5å
        successful_tests.sort(key=lambda x: x['scores']['overall_score'], reverse=True)
        
        print(f"\nğŸ† æ¨¡å‹æ’å (å‰5å):")
        for i, test in enumerate(successful_tests[:5], 1):
            scores = test['scores']
            print(f"   {i}. {test['model_name']}")
            print(f"      ç»¼åˆ: {scores['overall_score']:.3f} | "
                  f"å¹»è§‰: {scores['hallucination_resistance']:.3f} | "
                  f"ä¸€è‡´æ€§: {scores['role_consistency']:.3f} | "
                  f"å¤šæ ·æ€§: {scores['cognitive_diversity']:.3f}")
        
        # åˆ†ææœ€ä½³å’Œæœ€å·®è¡¨ç°
        best_model = successful_tests[0]
        worst_model = successful_tests[-1]
        
        print(f"\nğŸ¥‡ æœ€ä½³è¡¨ç°: {best_model['model_name']}")
        print(f"   ç»¼åˆå¾—åˆ†: {best_model['scores']['overall_score']:.3f}")
        print(f"   ä¼˜åŠ¿: ", end="")
        best_scores = best_model['scores']
        strengths = []
        if best_scores['hallucination_resistance'] >= 0.7:
            strengths.append("å¹»è§‰æŠµæŠ—å¼º")
        if best_scores['role_consistency'] >= 0.7:
            strengths.append("è§’è‰²ä¸€è‡´æ€§å¥½")
        if best_scores['cognitive_diversity'] >= 0.8:
            strengths.append("è®¤çŸ¥å¤šæ ·æ€§é«˜")
        print(", ".join(strengths) if strengths else "ç»¼åˆè¡¨ç°å‡è¡¡")
        
        if len(successful_tests) > 1:
            print(f"\nğŸ”» æœ€å¼±è¡¨ç°: {worst_model['model_name']}")
            print(f"   ç»¼åˆå¾—åˆ†: {worst_model['scores']['overall_score']:.3f}")
            print(f"   å¾…æ”¹è¿›: ", end="")
            worst_scores = worst_model['scores']
            weaknesses = []
            if worst_scores['hallucination_resistance'] < 0.3:
                weaknesses.append("å¹»è§‰æŠµæŠ—å¼±")
            if worst_scores['role_consistency'] < 0.3:
                weaknesses.append("è§’è‰²ä¸€è‡´æ€§å·®")
            if worst_scores['cognitive_diversity'] < 0.6:
                weaknesses.append("è®¤çŸ¥å¤šæ ·æ€§ä½")
            print(", ".join(weaknesses) if weaknesses else "å„é¡¹èƒ½åŠ›å‡éœ€æå‡")
    
    # æ˜¾ç¤ºå¤±è´¥çš„æµ‹è¯•
    if failed_tests:
        print(f"\nâŒ å¤±è´¥çš„æµ‹è¯•:")
        for test in failed_tests:
            print(f"   {test['model_name']}: {test.get('error', 'æœªçŸ¥é”™è¯¯')}")
    
    # ä¿å­˜å®Œæ•´ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"batch_explainable_test_results_{timestamp}.json"
    
    final_results = {
        'test_metadata': {
            'test_type': 'batch_explainable_cognitive_ecosystem',
            'start_time': start_time.isoformat(),
            'end_time': end_time.isoformat(),
            'total_duration_seconds': total_duration,
            'total_models_tested': len(test_models),
            'successful_tests': len(successful_tests),
            'failed_tests': len(failed_tests),
            'success_rate': len(successful_tests) / len(test_models)
        },
        'summary_statistics': {
            'average_scores': {
                'hallucination_resistance': avg_hallucination if successful_tests else 0,
                'role_consistency': avg_consistency if successful_tests else 0,
                'cognitive_diversity': avg_diversity if successful_tests else 0,
                'overall_score': avg_overall if successful_tests else 0
            } if successful_tests else None,
            'best_model': best_model['model_name'] if successful_tests else None,
            'worst_model': worst_model['model_name'] if successful_tests else None
        },
        'individual_results': all_results
    }
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(final_results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“ å®Œæ•´æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {filename}")
    print(f"ğŸ“– æ–‡ä»¶åŒ…å«æ‰€æœ‰æ¨¡å‹çš„è¯¦ç»†è¯„åˆ†è§£é‡Šå’Œè®¡ç®—è¿‡ç¨‹")
    
    # ç”Ÿæˆç®€åŒ–çš„CSVæŠ¥å‘Š
    csv_filename = f"batch_test_summary_{timestamp}.csv"
    if successful_tests:
        import pandas as pd
        
        csv_data = []
        for test in successful_tests:
            scores = test['scores']
            csv_data.append({
                'æ¨¡å‹åç§°': test['model_name'],
                'æœåŠ¡å•†': test['service_name'],
                'ç»¼åˆå¾—åˆ†': f"{scores['overall_score']:.3f}",
                'å¹»è§‰æŠµæŠ—': f"{scores['hallucination_resistance']:.3f}",
                'è§’è‰²ä¸€è‡´æ€§': f"{scores['role_consistency']:.3f}",
                'è®¤çŸ¥å¤šæ ·æ€§': f"{scores['cognitive_diversity']:.3f}",
                'æµ‹è¯•æ—¶é•¿(ç§’)': f"{test['test_duration']:.1f}"
            })
        
        df = pd.DataFrame(csv_data)
        df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
        print(f"ğŸ“Š ç®€åŒ–æŠ¥å‘Šå·²ä¿å­˜åˆ°: {csv_filename}")
    
    print(f"\nğŸ‰ æ‰¹é‡æµ‹è¯•å®Œæˆï¼")
    return final_results

def main():
    """ä¸»å‡½æ•°"""
    try:
        results = run_batch_explainable_tests()
        return results
    except KeyboardInterrupt:
        print(f"\n\nâš ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        return None
    except Exception as e:
        print(f"\n\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°ä¸¥é‡é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    main()
