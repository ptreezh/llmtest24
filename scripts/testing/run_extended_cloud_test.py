#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ‰©å±•äº‘æ¨¡å‹è®¤çŸ¥ç”Ÿæ€ç³»ç»Ÿæµ‹è¯•

æµ‹è¯•æ›´å¤šçš„äº‘å¤§æ¨¡å‹ï¼ŒåŒ…æ‹¬å›½å†…å¤–ä¸»æµçš„LLMæœåŠ¡ã€‚
"""

import sys
import os
import json
import time
from datetime import datetime
from typing import Dict, List, Any, Tuple
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append('.')

# å¯¼å…¥å¯è§£é‡Šæµ‹è¯•æ¨¡å—
from run_explainable_cognitive_test import run_explainable_test, ExplainableScorer

def get_extended_test_models() -> List[Tuple[str, str]]:
    """è·å–æ‰©å±•çš„æµ‹è¯•æ¨¡å‹åˆ—è¡¨"""
    return [
        # SiliconFlow æ¨¡å‹
        ('siliconflow', 'THUDM/glm-4-9b-chat'),
        ('siliconflow', 'Qwen/Qwen2.5-7B-Instruct'),
        ('siliconflow', 'deepseek-ai/DeepSeek-V2.5'),
        ('siliconflow', 'deepseek-ai/DeepSeek-V3'),
        
        # Together.ai æ¨¡å‹
        ('together', 'mistralai/Mixtral-8x7B-Instruct-v0.1'),
        ('together', 'meta-llama/Llama-3-8b-chat'),
        
        # OpenRouter æ¨¡å‹
        ('openrouter', 'openai/gpt-3.5-turbo'),
        ('openrouter', 'anthropic/claude-3-opus'),
        ('openrouter', 'google/gemma-2-9b-it'),
        
        # PPInfra æ¨¡å‹
        ('ppinfra', 'qwen/qwen3-235b-a22b-fp8'),
        ('ppinfra', 'minimaxai/minimax-m1-80k'),
        
        # Google Gemini æ¨¡å‹
        ('gemini', 'gemini-1.5-flash'),
        ('gemini', 'gemini-1.5-pro'),
        ('gemini', 'gemini-2.0-flash-exp'),
        
        # é˜¿é‡Œäº‘DashScope æ¨¡å‹
        ('dashscope', 'qwen-plus'),
        ('dashscope', 'qwen-max'),
        ('dashscope', 'qwen-turbo'),
        
        # æ™ºè°±AI GLM æ¨¡å‹
        ('glm', 'glm-4-plus'),
        ('glm', 'glm-4-air'),
        ('glm', 'glm-4-airx'),
        ('glm', 'glm-4-flash'),
        
        # ç™¾åº¦æ–‡å¿ƒ æ¨¡å‹
        ('baidu', 'ernie-4.0-8k'),
        ('baidu', 'ernie-3.5-8k'),
        ('baidu', 'ernie-speed-8k'),
    ]

def run_extended_batch_test():
    """è¿è¡Œæ‰©å±•çš„æ‰¹é‡æµ‹è¯•"""
    print("ğŸŒ æ‰©å±•äº‘æ¨¡å‹è®¤çŸ¥ç”Ÿæ€ç³»ç»Ÿæµ‹è¯•")
    print("=" * 70)
    
    test_models = get_extended_test_models()
    print(f"ğŸ“‹ è®¡åˆ’æµ‹è¯• {len(test_models)} ä¸ªæ¨¡å‹:")
    
    # æŒ‰æœåŠ¡å•†åˆ†ç»„æ˜¾ç¤º
    services = {}
    for service, model in test_models:
        if service not in services:
            services[service] = []
        services[service].append(model)
    
    for service, models in services.items():
        print(f"\nğŸ”¹ {service.upper()}:")
        for i, model in enumerate(models, 1):
            print(f"   {i:2d}. {model}")
    
    print(f"\nğŸ¯ æµ‹è¯•ç‰¹ç‚¹:")
    print(f"  - è¦†ç›–å›½å†…å¤–ä¸»æµLLMæœåŠ¡")
    print(f"  - è¯¦ç»†çš„è¯„åˆ†è§£é‡Šå’Œè®¡ç®—è¿‡ç¨‹")
    print(f"  - å®æ—¶æ˜¾ç¤ºæµ‹è¯•è¿›åº¦å’Œç»“æœ")
    print(f"  - ä¿å­˜å®Œæ•´çš„æµ‹è¯•æ•°æ®å’Œè§£é‡Š")
    
    # è¯¢é—®æµ‹è¯•æ¨¡å¼
    print(f"\nğŸ›ï¸ é€‰æ‹©æµ‹è¯•æ¨¡å¼:")
    print(f"  1. å¿«é€Ÿæµ‹è¯• (è·³è¿‡å¤±è´¥çš„æœåŠ¡)")
    print(f"  2. å®Œæ•´æµ‹è¯• (æµ‹è¯•æ‰€æœ‰æ¨¡å‹)")
    print(f"  3. é€‰æ‹©æ€§æµ‹è¯• (é€‰æ‹©ç‰¹å®šæœåŠ¡)")
    
    mode = input(f"è¯·é€‰æ‹©æµ‹è¯•æ¨¡å¼ (1-3ï¼Œé»˜è®¤1): ").strip() or "1"
    
    if mode == "3":
        print(f"\nğŸ“‹ å¯ç”¨æœåŠ¡:")
        service_list = list(services.keys())
        for i, service in enumerate(service_list, 1):
            print(f"  {i}. {service}")
        
        selected = input(f"è¯·é€‰æ‹©è¦æµ‹è¯•çš„æœåŠ¡ (ç”¨é€—å·åˆ†éš”ï¼Œå¦‚1,3,5): ").strip()
        if selected:
            try:
                indices = [int(x.strip()) - 1 for x in selected.split(',')]
                selected_services = [service_list[i] for i in indices if 0 <= i < len(service_list)]
                test_models = [(s, m) for s, m in test_models if s in selected_services]
                print(f"å·²é€‰æ‹© {len(selected_services)} ä¸ªæœåŠ¡ï¼Œå…± {len(test_models)} ä¸ªæ¨¡å‹")
            except:
                print("è¾“å…¥æ ¼å¼é”™è¯¯ï¼Œä½¿ç”¨é»˜è®¤æµ‹è¯•")
    
    # è¯¢é—®æ˜¯å¦ç»§ç»­
    confirm = input(f"\næ˜¯å¦å¼€å§‹æµ‹è¯•ï¼Ÿ(y/N): ").strip().lower()
    if confirm not in ['y', 'yes', 'æ˜¯']:
        print("æµ‹è¯•å·²å–æ¶ˆ")
        return
    
    # å¼€å§‹æ‰¹é‡æµ‹è¯•
    start_time = datetime.now()
    all_results = {}
    successful_tests = []
    failed_tests = []
    skip_fast_mode = mode == "1"
    
    print(f"\nğŸš€ å¼€å§‹æ‰©å±•æµ‹è¯• - {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    
    for i, (service_name, model_name) in enumerate(test_models, 1):
        print(f"\nğŸ“ è¿›åº¦: {i}/{len(test_models)} - {service_name}/{model_name}")
        print("â”€" * 70)
        
        try:
            # è¿è¡Œå•ä¸ªæ¨¡å‹çš„è¯¦ç»†æµ‹è¯•
            result = run_explainable_test(service_name, model_name)
            
            # ä¿å­˜ç»“æœ
            model_key = f"{service_name}/{model_name}"
            all_results[model_key] = result
            
            if result.get('status') == 'success':
                successful_tests.append(result)
                scores = result['scores']
                print(f"âœ… æµ‹è¯•æˆåŠŸ - ç»¼åˆå¾—åˆ†: {scores['overall_score']:.3f}")
                print(f"   å¹»è§‰æŠµæŠ—: {scores['hallucination_resistance']:.3f} | "
                      f"è§’è‰²ä¸€è‡´æ€§: {scores['role_consistency']:.3f} | "
                      f"è®¤çŸ¥å¤šæ ·æ€§: {scores['cognitive_diversity']:.3f}")
            else:
                failed_tests.append(result)
                error_msg = result.get('error', 'æœªçŸ¥é”™è¯¯')
                print(f"âŒ æµ‹è¯•å¤±è´¥: {error_msg}")
                
                # å¿«é€Ÿæ¨¡å¼ä¸‹è·³è¿‡åŒä¸€æœåŠ¡çš„å…¶ä»–æ¨¡å‹
                if skip_fast_mode and 'connectivity_failed' in error_msg:
                    remaining_same_service = [
                        (s, m) for s, m in test_models[i:] if s == service_name
                    ]
                    if remaining_same_service:
                        print(f"âš¡ å¿«é€Ÿæ¨¡å¼ï¼šè·³è¿‡ {service_name} çš„å…¶ä½™ {len(remaining_same_service)} ä¸ªæ¨¡å‹")
                        for s, m in remaining_same_service:
                            model_key = f"{s}/{m}"
                            all_results[model_key] = {
                                'model_name': model_key,
                                'status': 'skipped',
                                'error': f'è·³è¿‡ï¼š{service_name} æœåŠ¡ä¸å¯ç”¨',
                                'test_timestamp': datetime.now().isoformat()
                            }
                            failed_tests.append(all_results[model_key])
                        # è·³è¿‡è¿™äº›æ¨¡å‹
                        test_models = [
                            (s, m) for s, m in test_models 
                            if not (s == service_name and (s, m) in remaining_same_service)
                        ]
        
        except Exception as e:
            print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}")
            failed_result = {
                'model_name': f"{service_name}/{model_name}",
                'status': 'failed',
                'error': str(e),
                'test_timestamp': datetime.now().isoformat()
            }
            all_results[f"{service_name}/{model_name}"] = failed_result
            failed_tests.append(failed_result)
        
        # æ·»åŠ å»¶è¿Ÿä»¥é¿å…APIé™åˆ¶
        if i < len(test_models):
            print("â³ ç­‰å¾…3ç§’åç»§ç»­ä¸‹ä¸€ä¸ªæµ‹è¯•...")
            time.sleep(3)
    
    end_time = datetime.now()
    total_duration = (end_time - start_time).total_seconds()
    
    # ç”Ÿæˆè¯¦ç»†æ±‡æ€»æŠ¥å‘Š
    generate_extended_report(
        all_results, successful_tests, failed_tests, 
        start_time, end_time, total_duration, len(test_models)
    )

def generate_extended_report(all_results, successful_tests, failed_tests, 
                           start_time, end_time, total_duration, total_planned):
    """ç”Ÿæˆæ‰©å±•æµ‹è¯•æŠ¥å‘Š"""
    
    print(f"\n" + "=" * 80)
    print(f"ğŸ“Š æ‰©å±•äº‘æ¨¡å‹æµ‹è¯•å®Œæˆæ±‡æ€»")
    print(f"=" * 80)
    
    print(f"ğŸ• æµ‹è¯•æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')} - {end_time.strftime('%H:%M:%S')}")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_duration:.1f}ç§’ ({total_duration/60:.1f}åˆ†é’Ÿ)")
    print(f"ğŸ“ˆ æµ‹è¯•ç»Ÿè®¡:")
    print(f"   è®¡åˆ’æµ‹è¯•: {total_planned}")
    print(f"   å®é™…æµ‹è¯•: {len(all_results)}")
    print(f"   æˆåŠŸæµ‹è¯•: {len(successful_tests)}")
    print(f"   å¤±è´¥æµ‹è¯•: {len(failed_tests)}")
    print(f"   æˆåŠŸç‡: {len(successful_tests)/len(all_results)*100:.1f}%")
    
    if successful_tests:
        # è®¡ç®—å¹³å‡åˆ†æ•°
        avg_hallucination = sum(t['scores']['hallucination_resistance'] for t in successful_tests) / len(successful_tests)
        avg_consistency = sum(t['scores']['role_consistency'] for t in successful_tests) / len(successful_tests)
        avg_diversity = sum(t['scores']['cognitive_diversity'] for t in successful_tests) / len(successful_tests)
        avg_overall = sum(t['scores']['overall_score'] for t in successful_tests) / len(successful_tests)
        
        print(f"\nğŸ“ˆ å¹³å‡åˆ†æ•°:")
        print(f"   å¹»è§‰æŠµæŠ—: {avg_hallucination:.3f}")
        print(f"   è§’è‰²ä¸€è‡´æ€§: {avg_consistency:.3f}")
        print(f"   è®¤çŸ¥å¤šæ ·æ€§: {avg_diversity:.3f}")
        print(f"   ç»¼åˆå¾—åˆ†: {avg_overall:.3f}")
        
        # æŒ‰æœåŠ¡å•†åˆ†ç»„åˆ†æ
        service_stats = {}
        for test in successful_tests:
            service = test['service_name']
            if service not in service_stats:
                service_stats[service] = []
            service_stats[service].append(test)
        
        print(f"\nğŸ“Š æŒ‰æœåŠ¡å•†åˆ†æ:")
        for service, tests in service_stats.items():
            if tests:
                service_avg = sum(t['scores']['overall_score'] for t in tests) / len(tests)
                best_model = max(tests, key=lambda x: x['scores']['overall_score'])
                print(f"   {service.upper():12s}: å¹³å‡ {service_avg:.3f} | "
                      f"æœ€ä½³ {best_model['model_display_name']} ({best_model['scores']['overall_score']:.3f})")
        
        # æ’åºå¹¶æ˜¾ç¤ºå‰10å
        successful_tests.sort(key=lambda x: x['scores']['overall_score'], reverse=True)
        
        print(f"\nğŸ† æ¨¡å‹æ’å (å‰10å):")
        for i, test in enumerate(successful_tests[:10], 1):
            scores = test['scores']
            print(f"   {i:2d}. {test['model_name']}")
            print(f"       ç»¼åˆ: {scores['overall_score']:.3f} | "
                  f"å¹»è§‰: {scores['hallucination_resistance']:.3f} | "
                  f"ä¸€è‡´æ€§: {scores['role_consistency']:.3f} | "
                  f"å¤šæ ·æ€§: {scores['cognitive_diversity']:.3f}")
        
        # åˆ†ææœ€ä½³å’Œæœ€å·®è¡¨ç°
        if len(successful_tests) >= 2:
            best_model = successful_tests[0]
            worst_model = successful_tests[-1]
            
            print(f"\nğŸ¥‡ æœ€ä½³è¡¨ç°: {best_model['model_name']}")
            print(f"   ç»¼åˆå¾—åˆ†: {best_model['scores']['overall_score']:.3f}")
            
            print(f"\nğŸ”» æœ€å¼±è¡¨ç°: {worst_model['model_name']}")
            print(f"   ç»¼åˆå¾—åˆ†: {worst_model['scores']['overall_score']:.3f}")
    
    # æ˜¾ç¤ºå¤±è´¥çš„æµ‹è¯•
    if failed_tests:
        print(f"\nâŒ å¤±è´¥çš„æµ‹è¯•:")
        failure_reasons = {}
        for test in failed_tests:
            reason = test.get('error', 'æœªçŸ¥é”™è¯¯')
            if reason not in failure_reasons:
                failure_reasons[reason] = []
            failure_reasons[reason].append(test['model_name'])
        
        for reason, models in failure_reasons.items():
            print(f"   {reason}: {len(models)} ä¸ªæ¨¡å‹")
            for model in models[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"     - {model}")
            if len(models) > 3:
                print(f"     - ... è¿˜æœ‰ {len(models)-3} ä¸ª")
    
    # ä¿å­˜å®Œæ•´ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"extended_cloud_test_results_{timestamp}.json"
    
    final_results = {
        'test_metadata': {
            'test_type': 'extended_cloud_cognitive_ecosystem',
            'start_time': start_time.isoformat(),
            'end_time': end_time.isoformat(),
            'total_duration_seconds': total_duration,
            'total_models_planned': total_planned,
            'total_models_tested': len(all_results),
            'successful_tests': len(successful_tests),
            'failed_tests': len(failed_tests),
            'success_rate': len(successful_tests) / len(all_results) if all_results else 0
        },
        'summary_statistics': {
            'average_scores': {
                'hallucination_resistance': avg_hallucination if successful_tests else 0,
                'role_consistency': avg_consistency if successful_tests else 0,
                'cognitive_diversity': avg_diversity if successful_tests else 0,
                'overall_score': avg_overall if successful_tests else 0
            } if successful_tests else None,
            'service_statistics': service_stats if successful_tests else None,
            'best_model': successful_tests[0]['model_name'] if successful_tests else None,
            'worst_model': successful_tests[-1]['model_name'] if len(successful_tests) > 1 else None
        },
        'individual_results': all_results
    }
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(final_results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“ å®Œæ•´æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {filename}")
    print(f"ğŸ“– æ–‡ä»¶åŒ…å«æ‰€æœ‰æ¨¡å‹çš„è¯¦ç»†è¯„åˆ†è§£é‡Šå’Œè®¡ç®—è¿‡ç¨‹")
    
    # ç”Ÿæˆç®€åŒ–çš„CSVæŠ¥å‘Š
    if successful_tests:
        csv_filename = f"extended_test_summary_{timestamp}.csv"
        try:
            import pandas as pd
            
            csv_data = []
            for test in successful_tests:
                scores = test['scores']
                csv_data.append({
                    'æ¨¡å‹åç§°': test['model_name'],
                    'æœåŠ¡å•†': test['service_name'],
                    'ç»¼åˆå¾—åˆ†': f"{scores['overall_score']:.3f}",
                    'å¹»è§‰æŠµæŠ—': f"{scores['hallucination_resistance']:.3f}",
                    'è§’è‰²ä¸€è‡´æ€§': f"{scores['role_consistency']:.3f}",
                    'è®¤çŸ¥å¤šæ ·æ€§': f"{scores['cognitive_diversity']:.3f}",
                    'æµ‹è¯•æ—¶é•¿(ç§’)': f"{test['test_duration']:.1f}"
                })
            
            df = pd.DataFrame(csv_data)
            df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
            print(f"ğŸ“Š ç®€åŒ–æŠ¥å‘Šå·²ä¿å­˜åˆ°: {csv_filename}")
        except ImportError:
            print("ğŸ“Š éœ€è¦å®‰è£…pandasæ‰èƒ½ç”ŸæˆCSVæŠ¥å‘Š")
    
    print(f"\nğŸ‰ æ‰©å±•æµ‹è¯•å®Œæˆï¼")
    return final_results

def main():
    """ä¸»å‡½æ•°"""
    try:
        results = run_extended_batch_test()
        return results
    except KeyboardInterrupt:
        print(f"\n\nâš ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        return None
    except Exception as e:
        print(f"\n\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°ä¸¥é‡é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    main()
