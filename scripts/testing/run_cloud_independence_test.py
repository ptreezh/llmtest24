#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
äº‘æ¨¡å‹è§’è‰²ç‹¬ç«‹æ€§æ‰¹é‡æµ‹è¯•è„šæœ¬

ä½¿ç”¨çœŸå®çš„äº‘LLMæ¨¡å‹è¿›è¡Œè§’è‰²ç‹¬ç«‹æ€§æµ‹è¯•ï¼Œè¯„ä¼°ä¸åŒæ¨¡å‹çš„è¡¨ç°ã€‚
"""

import sys
import os
import json
import time
from datetime import datetime
from typing import Dict, List, Any
from pathlib import Path
import argparse # Import argparse

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
print(f"Current sys.path: {sys.path}")
print(f"Project root: {project_root}")

# å¯¼å…¥äº‘æœåŠ¡å’Œç‹¬ç«‹æ€§æµ‹è¯•æ¨¡å—
import importlib.util
import os

# æ„å»ºcloud_services.pyçš„ç»å¯¹è·¯å¾„
cloud_services_path = os.path.join(project_root, "scripts", "utils", "cloud_services.py")
spec = importlib.util.spec_from_file_location("cloud_services", cloud_services_path)
cloud_services = importlib.util.module_from_spec(spec)
spec.loader.exec_module(cloud_services)

# ä»æ¨¡å—ä¸­å¯¼å…¥éœ€è¦çš„å˜é‡
CLOUD_SERVICES = cloud_services.CLOUD_SERVICES
get_available_services = cloud_services.get_available_services
from independence.character_breaking import BreakingStressTest
from independence.implicit_cognition import ImplicitCognitionTest
from independence.longitudinal_consistency import LongitudinalConsistencyTest
from independence.metrics.independence_calculator import IndependenceCalculator
from config.config import INDEPENDENCE_CONFIG

def load_role_prompt(role_name: str) -> str:
    """ä»æ–‡ä»¶åŠ è½½è§’è‰²æç¤ºè¯"""
    prompt_path = project_root / "role_prompts" / f"{role_name}_prompt.txt"
    if not prompt_path.exists():
        print(f"âš ï¸ è­¦å‘Š: è§’è‰²æç¤ºè¯æ–‡ä»¶ä¸å­˜åœ¨: {prompt_path}ï¼Œå°†ä½¿ç”¨é»˜è®¤æç¤ºè¯ã€‚")
        return f"ä½ æ˜¯ä¸€ä½èµ„æ·±çš„{role_name}ã€‚"
    
    with open(prompt_path, 'r', encoding='utf-8') as f:
        return f.read()

class CloudIndependenceTester:
    """äº‘æ¨¡å‹è§’è‰²ç‹¬ç«‹æ€§æµ‹è¯•å™¨"""

    def __init__(self):
        self.available_services = get_available_services()

    def get_available_models(self) -> List[Dict[str, str]]:
        """è·å–å¯ç”¨çš„äº‘æ¨¡å‹åˆ—è¡¨"""
        models = []
        for service_name in self.available_services:
            service_config = CLOUD_SERVICES.get(service_name, {})
            for model_name in service_config.get('models', []):
                models.append({
                    'service': service_name,
                    'model': model_name,
                    'full_name': f"{service_name}/{model_name}",
                    'service_display_name': service_config.get('name', service_name)
                })
        return models

    def run_single_model_test(self, model_full_name: str) -> Dict[str, Any]:
        """å¯¹å•ä¸ªæ¨¡å‹è¿è¡Œå®Œæ•´çš„ç‹¬ç«‹æ€§æµ‹è¯•"""
        print(f"\nğŸ§  æµ‹è¯•æ¨¡å‹: {model_full_name}")
        print("=" * 60)
        
        start_time = time.time()
        
        # åŠ è½½é˜²å¾¡åŠ å¼ºçš„è§’è‰²æç¤ºè¯
        test_role_prompt = load_role_prompt("software_engineer")
        
        test_config = INDEPENDENCE_CONFIG.copy()
        test_config['model_name'] = model_full_name
        
        stress_test = BreakingStressTest(test_config)
        cognition_test = ImplicitCognitionTest(test_config)
        consistency_test = LongitudinalConsistencyTest(test_config)
        calculator = IndependenceCalculator()
        
        individual_results = {}
        
        try:
            # 1. è§’è‰²ç ´åŠŸå‹åŠ›æµ‹è¯•
            print("  ğŸ§ª è¿è¡Œ E1: è§’è‰²ç ´åŠŸå‹åŠ›æµ‹è¯•...")
            stress_config = {'test_roles': {'software_engineer': test_role_prompt}, 'stress_levels': ['low', 'medium', 'high']}
            stress_result = stress_test.run_experiment(model_full_name, stress_config)
            individual_results['breaking_stress'] = stress_result
            print(f"    âœ… E1 å®Œæˆ - æŠµæŠ—åŠ›: {stress_result.get('summary', {}).get('overall_resistance', 0):.3f}")

            # 2. éšå¼è®¤çŸ¥æµ‹è¯•
            print("  ğŸ§ª è¿è¡Œ E2: éšå¼è®¤çŸ¥æµ‹è¯•...")
            cognition_config = {'role_prompt': test_role_prompt}
            cognition_result = cognition_test.run_experiment(model_full_name, cognition_config)
            individual_results['implicit_cognition'] = cognition_result
            print(f"    âœ… E2 å®Œæˆ - å¾—åˆ†: {cognition_result.get('summary', {}).get('overall_implicit_score', 0):.3f}")

            # 3. çºµå‘ä¸€è‡´æ€§æµ‹è¯•
            print("  ğŸ§ª è¿è¡Œ E3: çºµå‘ä¸€è‡´æ€§æµ‹è¯•...")
            consistency_config = {'role_prompt': test_role_prompt}
            consistency_result = consistency_test.run_experiment(model_full_name, consistency_config)
            individual_results['longitudinal_consistency'] = consistency_result
            print(f"    âœ… E3 å®Œæˆ - ä¸€è‡´æ€§: {consistency_result.get('summary', {}).get('overall_consistency', 0):.3f}")

            # 4. è®¡ç®—ç»¼åˆå¾—åˆ†
            print("  ğŸ“Š è®¡ç®—ç»¼åˆç‹¬ç«‹æ€§å¾—åˆ†...")
            final_score = calculator.calculate_comprehensive_independence(
                breaking_stress_result=stress_result,
                implicit_cognition_result=cognition_result,
                longitudinal_consistency_result=consistency_result
            )
            individual_results['final_independence'] = final_score
            print(f"    âœ… ç»¼åˆå¾—åˆ†: {final_score.get('final_score', 0):.3f}, ç­‰çº§: {final_score.get('grade', 'N/A')}")

            end_time = time.time()
            return {
                'model_name': model_full_name, 'status': 'success',
                'test_duration': end_time - start_time, 'scores': final_score,
                'details': individual_results
            }

        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                'model_name': model_full_name, 'status': 'failed',
                'error': str(e), 'test_duration': time.time() - start_time
            }

    def run_batch_test(self, models_to_run: List[Dict[str, str]]):
        """æ‰¹é‡æµ‹è¯•å¤šä¸ªæ¨¡å‹"""
        print("ğŸš€ å¼€å§‹æ‰¹é‡è§’è‰²ç‹¬ç«‹æ€§æµ‹è¯• (äº‘æ¨¡å‹)")
        print(f"ğŸ“Š æµ‹è¯•æ¨¡å‹æ•°é‡: {len(models_to_run)}")
        print("=" * 80)

        results = {}
        for i, model_info in enumerate(models_to_run, 1):
            print(f"\nğŸ“ è¿›åº¦: {i}/{len(models_to_run)}")
            result = self.run_single_model_test(model_info['full_name'])
            results[model_info['full_name']] = result
            time.sleep(2)

        return results

    def save_results(self, results: Dict[str, Any], filename: str = None):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"cloud_independence_test_results_{timestamp}.json"
        
        results_dir = Path("testout")
        results_dir.mkdir(exist_ok=True)
        filepath = results_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {filepath}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§  äº‘æ¨¡å‹è§’è‰²ç‹¬ç«‹æ€§æµ‹è¯•å™¨")
    print("=" * 50)
    
    parser = argparse.ArgumentParser(description="Cloud Model Independence Test Runner.")
    parser.add_argument("--model", type=str, help="Specify a particular cloud model to test (e.g., 'service_name/model_name'). If not provided, all available models will be tested.")
    args = parser.parse_args()

    tester = CloudIndependenceTester()
    available_models = tester.get_available_models()

    if not available_models:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„äº‘æ¨¡å‹ã€‚è¯·æ£€æŸ¥ cloud_services.py å’Œ .env æ–‡ä»¶ã€‚")
        return

    print(f"ğŸ“‹ å‘ç° {len(available_models)} ä¸ªå¯ç”¨æ¨¡å‹:")
    for model in available_models:
        print(f"  - {model['full_name']} ({model['service_display_name']})")

    models_to_test = []
    if args.model:
        # Check if the specified model is available
        specified_model_found = False
        for model_info in available_models:
            if model_info['full_name'] == args.model:
                models_to_test.append(model_info)
                specified_model_found = True
                break
        if not specified_model_found:
            print(f"âŒ æŒ‡å®šçš„æ¨¡å‹ '{args.model}' æœªæ‰¾åˆ°ã€‚è¯·æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®ã€‚")
            return
        print(f"\nå°†ä»…æµ‹è¯•æŒ‡å®šçš„æ¨¡å‹: {args.model}")
    else:
        confirm = input("\næ˜¯å¦å¼€å§‹å¯¹æ‰€æœ‰å¯ç”¨æ¨¡å‹è¿›è¡Œæµ‹è¯•ï¼Ÿ(y/N): ").strip().lower()
        if confirm not in ['y', 'yes', 'æ˜¯']:
            print("æµ‹è¯•å·²å–æ¶ˆã€‚")
            return
        models_to_test = available_models

    if models_to_test:
        results = tester.run_batch_test(models_to_test)
        tester.save_results(results)
    else:
        print("æ²¡æœ‰æ¨¡å‹å¯ä¾›æµ‹è¯•ã€‚")

if __name__ == "__main__":
    main()
