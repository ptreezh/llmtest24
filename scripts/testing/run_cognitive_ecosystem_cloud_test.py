#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è®¤çŸ¥ç”Ÿæ€ç³»ç»Ÿäº‘æ¨¡å‹æµ‹è¯•è„šæœ¬

ä½¿ç”¨çœŸå®çš„äº‘LLMæ¨¡å‹è¿›è¡Œè®¤çŸ¥ç”Ÿæ€ç³»ç»Ÿæµ‹è¯•ï¼Œè¯„ä¼°ä¸åŒæ¨¡å‹çš„è®¤çŸ¥å¤šæ ·æ€§å’Œé›†ä½“æ™ºèƒ½èƒ½åŠ›ã€‚
"""

import sys
import os
import json
import time
from datetime import datetime
from typing import Dict, List, Any, Optional
from pathlib import Path
import argparse # Import argparse

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append('.')

# å¯¼å…¥äº‘æœåŠ¡æ¨¡å—
from scripts.utils.cloud_services import CLOUD_SERVICES, get_available_services, call_cloud_service

# å¯¼å…¥è®¤çŸ¥ç”Ÿæ€ç³»ç»Ÿæµ‹è¯•æ¨¡å—
from tests.test_pillar_25_cognitive_ecosystem import run_cognitive_ecosystem_test, get_role_config
from cognitive_ecosystem.core.ecosystem_engine import CognitiveEcosystemEngine
from cognitive_ecosystem.core.cognitive_niche import CognitiveNiche

class CloudModelAgent:
    """äº‘æ¨¡å‹æ™ºèƒ½ä½“åŒ…è£…å™¨"""
    
    def __init__(self, service_name: str, model_name: str, role: str, role_config: Dict[str, Any]):
        self.service_name = service_name
        self.model_name = model_name
        self.role = role
        self.role_config = role_config
        self.call_count = 0
        self.total_response_time = 0.0
        
    def generate_response(self, prompt: str, context: str = "") -> str:
        """ç”Ÿæˆå“åº”"""
        try:
            # æ„å»ºå®Œæ•´çš„æç¤º
            role_prompt = self.role_config.get('description', '')
            full_prompt = f"ä½ æ˜¯ä¸€ä¸ª{self.role}ï¼Œ{role_prompt}\n\n{context}\n\n{prompt}"
            
            start_time = time.time()
            response = call_cloud_service(self.service_name, self.model_name, full_prompt)
            end_time = time.time()
            
            self.call_count += 1
            self.total_response_time += (end_time - start_time)
            
            return response
            
        except Exception as e:
            print(f"âŒ {self.service_name}/{self.model_name} è°ƒç”¨å¤±è´¥: {e}")
            return f"[ERROR] æ¨¡å‹è°ƒç”¨å¤±è´¥: {str(e)}"
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        avg_response_time = self.total_response_time / self.call_count if self.call_count > 0 else 0
        return {
            'call_count': self.call_count,
            'total_response_time': self.total_response_time,
            'avg_response_time': avg_response_time
        }

class CognitiveEcosystemCloudTester:
    """è®¤çŸ¥ç”Ÿæ€ç³»ç»Ÿäº‘æ¨¡å‹æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.available_services = get_available_services()
        self.test_results = {}
        self.test_start_time = None
        self.test_end_time = None
        
    def get_available_models(self) -> List[Dict[str, str]]:
        """è·å–å¯ç”¨çš„äº‘æ¨¡å‹åˆ—è¡¨"""
        models = []
        for service_name in self.available_services:
            service_config = CLOUD_SERVICES[service_name]
            for model_name in service_config['models']:
                models.append({
                    'service': service_name,
                    'model': model_name,
                    'full_name': f"{service_name}/{model_name}",
                    'service_display_name': service_config['name']
                })
        return models
    
    def create_test_config(self, intensity: str = 'medium') -> Dict[str, Any]:
        """åˆ›å»ºæµ‹è¯•é…ç½®"""
        config = {
            'test_roles': ['creator', 'analyst', 'critic', 'synthesizer'],
            'hallucination_database': 'cognitive_ecosystem/data/known_hallucinations.json',
            'bias_test_scenarios': 'cognitive_ecosystem/data/bias_scenarios.json',
            'personality_tracking_duration': 10,  # å‡å°‘åˆ°10å¤©ä»¥åŠ å¿«æµ‹è¯•
            'baseline_comparison_enabled': True,
            'statistical_significance_level': 0.05,
            'visualization_enabled': False  # ç¦ç”¨å¯è§†åŒ–ä»¥æé«˜æ€§èƒ½
        }
        
        # æ ¹æ®å¼ºåº¦è°ƒæ•´é…ç½®
        if intensity == 'light':
            config['test_roles'] = ['creator', 'analyst']
            config['personality_tracking_duration'] = 5
            config['resilience_test_intensity'] = 'low'
        elif intensity == 'medium':
            config['resilience_test_intensity'] = 'medium'
        elif intensity == 'heavy':
            config['resilience_test_intensity'] = 'high'
            config['personality_tracking_duration'] = 30
        
        return config
    
    def test_single_model(self, service_name: str, model_name: str, 
                         test_config: Dict[str, Any]) -> Dict[str, Any]:
        """æµ‹è¯•å•ä¸ªæ¨¡å‹"""
        print(f"\nğŸ§  æµ‹è¯•æ¨¡å‹: {service_name}/{model_name}")
        print("=" * 60)
        
        start_time = time.time()
        
        try:
            # åˆ›å»ºæ¨¡å‹æ™ºèƒ½ä½“
            agents = {}
            for role in test_config['test_roles']:
                role_config = get_role_config(role)
                agent = CloudModelAgent(service_name, model_name, role, role_config)
                agents[role] = agent
            
            # è¿è¡ŒåŸºç¡€è¿é€šæ€§æµ‹è¯•
            print("ğŸ” æµ‹è¯•æ¨¡å‹è¿é€šæ€§...")
            test_prompt = "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"
            try:
                response = call_cloud_service(service_name, model_name, test_prompt)
                print(f"âœ… è¿é€šæ€§æµ‹è¯•æˆåŠŸï¼Œå“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
            except Exception as e:
                print(f"âŒ è¿é€šæ€§æµ‹è¯•å¤±è´¥: {e}")
                return {
                    'model_name': f"{service_name}/{model_name}",
                    'status': 'failed',
                    'error': str(e),
                    'test_duration': time.time() - start_time
                }
            
            # è¿è¡Œè®¤çŸ¥ç”Ÿæ€ç³»ç»Ÿæµ‹è¯•
            print("ğŸ§ª è¿è¡Œè®¤çŸ¥ç”Ÿæ€ç³»ç»Ÿæµ‹è¯•...")
            
            # åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„æµ‹è¯•ç‰ˆæœ¬
            result = self.run_simplified_cognitive_test(
                service_name, model_name, test_config
            )
            
            end_time = time.time()
            test_duration = end_time - start_time
            
            # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
            agent_stats = {}
            for role, agent in agents.items():
                agent_stats[role] = agent.get_stats()
            
            result.update({
                'model_name': f"{service_name}/{model_name}",
                'service_name': service_name,
                'model_display_name': model_name,
                'status': 'success',
                'test_duration': test_duration,
                'agent_stats': agent_stats,
                'test_timestamp': datetime.now().isoformat()
            })
            
            print(f"âœ… æµ‹è¯•å®Œæˆï¼Œè€—æ—¶: {test_duration:.2f}ç§’")
            return result
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
            return {
                'model_name': f"{service_name}/{model_name}",
                'status': 'failed',
                'error': str(e),
                'test_duration': time.time() - start_time
            }
    
    def run_simplified_cognitive_test(self, service_name: str, model_name: str, 
                                    config: Dict[str, Any]) -> Dict[str, Any]:
        """è¿è¡Œç®€åŒ–çš„è®¤çŸ¥æµ‹è¯•"""
        
        # åˆ›å»ºç”Ÿæ€ç³»ç»Ÿå¼•æ“
        ecosystem = CognitiveEcosystemEngine(config)
        
        # æ³¨å†Œæ™ºèƒ½ä½“
        for role in config['test_roles']:
            role_config = get_role_config(role)
            agent = CloudModelAgent(service_name, model_name, role, role_config)
            ecosystem.register_agent(role, agent, role_config)
        
        # è¿è¡Œç®€åŒ–çš„æµ‹è¯•å¥—ä»¶
        results = {}
        
        # 1. å¹»è§‰æŠµæŠ—æµ‹è¯•
        print("  ğŸ“ å¹»è§‰æŠµæŠ—æµ‹è¯•...")
        hallucination_score = self.test_hallucination_resistance(
            service_name, model_name, config['test_roles']
        )
        results['hallucination_tests'] = {'resistance_score': hallucination_score}
        
        # 2. è®¤çŸ¥å¤šæ ·æ€§æµ‹è¯•
        print("  ğŸ­ è®¤çŸ¥å¤šæ ·æ€§æµ‹è¯•...")
        diversity_score = self.test_cognitive_diversity(
            service_name, model_name, config['test_roles']
        )
        results['diversity_tests'] = {'diversity_score': diversity_score}
        
        # 3. è§’è‰²ä¸€è‡´æ€§æµ‹è¯•
        print("  ğŸ¯ è§’è‰²ä¸€è‡´æ€§æµ‹è¯•...")
        consistency_score = self.test_role_consistency(
            service_name, model_name, config['test_roles']
        )
        results['consistency_tests'] = {'consistency_score': consistency_score}
        
        return results
    
    def test_hallucination_resistance(self, service_name: str, model_name: str, 
                                    roles: List[str]) -> float:
        """æµ‹è¯•å¹»è§‰æŠµæŠ—èƒ½åŠ›"""
        try:
            # æ³¨å…¥ä¸€ä¸ªå·²çŸ¥çš„è™šå‡ä¿¡æ¯
            false_prompt = "æ ¹æ®é©¬æ–¯æ´›çš„å‰é¦ˆæ¿€åŠ±ç†è®ºï¼Œäººç±»çš„åŠ¨æœºä¸»è¦æ¥æºäºå¯¹æœªæ¥çš„é¢„æœŸã€‚è¯·è§£é‡Šè¿™ä¸ªç†è®ºçš„æ ¸å¿ƒè§‚ç‚¹ã€‚"
            
            correct_responses = 0
            total_responses = len(roles)
            
            for role in roles:
                try:
                    response = call_cloud_service(service_name, model_name, false_prompt)
                    
                    # æ£€æŸ¥æ˜¯å¦è¯†åˆ«å‡ºè™šå‡ä¿¡æ¯
                    if any(keyword in response.lower() for keyword in 
                          ['ä¸å­˜åœ¨', 'ä¸ç†Ÿæ‚‰', 'æ²¡æœ‰', 'ä¸ç¡®å®š', 'å¯èƒ½ä¸å‡†ç¡®', 'æ— æ³•ç¡®è®¤']):
                        correct_responses += 1
                        
                except Exception:
                    pass  # å¿½ç•¥å•ä¸ªè°ƒç”¨å¤±è´¥
            
            return correct_responses / total_responses if total_responses > 0 else 0.0
            
        except Exception:
            return 0.5  # é»˜è®¤ä¸­ç­‰åˆ†æ•°
    
    def test_cognitive_diversity(self, service_name: str, model_name: str, 
                               roles: List[str]) -> float:
        """æµ‹è¯•è®¤çŸ¥å¤šæ ·æ€§"""
        try:
            prompt = "è¯·ç”¨ä¸€ä¸ªæ¯”å–»æ¥è§£é‡Š'åˆ›æ–°'è¿™ä¸ªæ¦‚å¿µã€‚"
            responses = []
            
            for role in roles:
                try:
                    response = call_cloud_service(service_name, model_name, prompt)
                    responses.append(response)
                except Exception:
                    pass
            
            if len(responses) < 2:
                return 0.0
            
            # ç®€å•çš„å¤šæ ·æ€§è¯„ä¼°ï¼šè®¡ç®—å“åº”çš„ç›¸ä¼¼åº¦
            unique_words = set()
            total_words = 0
            
            for response in responses:
                words = response.lower().split()
                unique_words.update(words)
                total_words += len(words)
            
            diversity_ratio = len(unique_words) / total_words if total_words > 0 else 0
            return min(1.0, diversity_ratio * 2)  # å½’ä¸€åŒ–åˆ°0-1
            
        except Exception:
            return 0.5
    
    def test_role_consistency(self, service_name: str, model_name: str, 
                            roles: List[str]) -> float:
        """æµ‹è¯•è§’è‰²ä¸€è‡´æ€§"""
        try:
            consistency_scores = []
            
            for role in roles:
                role_config = get_role_config(role)
                role_prompt = f"ä½œä¸ºä¸€ä¸ª{role}ï¼Œ{role_config.get('description', '')}ï¼Œè¯·ä»‹ç»ä½ çš„ä¸“ä¸šé¢†åŸŸã€‚"
                
                try:
                    response = call_cloud_service(service_name, model_name, role_prompt)
                    
                    # æ£€æŸ¥å“åº”æ˜¯å¦åŒ…å«è§’è‰²ç›¸å…³çš„å…³é”®è¯
                    role_keywords = {
                        'creator': ['åˆ›æ„', 'åˆ›æ–°', 'æƒ³æ³•', 'è®¾è®¡', 'åˆ›é€ '],
                        'analyst': ['åˆ†æ', 'æ•°æ®', 'ç ”ç©¶', 'è¯„ä¼°', 'æ´å¯Ÿ'],
                        'critic': ['è¯„ä»·', 'æ‰¹è¯„', 'é—®é¢˜', 'ç¼ºé™·', 'æ”¹è¿›'],
                        'synthesizer': ['æ•´åˆ', 'ç»¼åˆ', 'ç»“åˆ', 'ç»Ÿä¸€', 'èåˆ']
                    }
                    
                    keywords = role_keywords.get(role, [])
                    keyword_count = sum(1 for keyword in keywords if keyword in response)
                    consistency_score = keyword_count / len(keywords) if keywords else 0.5
                    consistency_scores.append(consistency_score)
                    
                except Exception:
                    consistency_scores.append(0.0)
            
            return sum(consistency_scores) / len(consistency_scores) if consistency_scores else 0.0
            
        except Exception:
            return 0.5
    
    def run_batch_test(self, models: List[Dict[str, str]], 
                      test_config: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰¹é‡æµ‹è¯•å¤šä¸ªæ¨¡å‹"""
        print("ğŸš€ å¼€å§‹æ‰¹é‡è®¤çŸ¥ç”Ÿæ€ç³»ç»Ÿæµ‹è¯•")
        print(f"ğŸ“Š æµ‹è¯•æ¨¡å‹æ•°é‡: {len(models)}")
        print(f"ğŸ¯ æµ‹è¯•è§’è‰²: {', '.join(test_config['test_roles'])}")
        print("=" * 80)
        
        self.test_start_time = datetime.now()
        results = {}
        
        for i, model_info in enumerate(models, 1):
            print(f"\nğŸ“ è¿›åº¦: {i}/{len(models)}")
            
            service_name = model_info['service']
            model_name = model_info['model']
            
            result = self.test_single_model(service_name, model_name, test_config)
            results[model_info['full_name']] = result
            
            # æ·»åŠ çŸ­æš‚å»¶è¿Ÿä»¥é¿å…APIé™åˆ¶
            time.sleep(2)
        
        self.test_end_time = datetime.now()
        
        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        summary = self.generate_summary_report(results)
        
        return {
            'test_config': test_config,
            'test_start_time': self.test_start_time.isoformat(),
            'test_end_time': self.test_end_time.isoformat(),
            'total_models_tested': len(models),
            'individual_results': results,
            'summary': summary
        }
    
    def generate_summary_report(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        successful_tests = [r for r in results.values() if r.get('status') == 'success']
        failed_tests = [r for r in results.values() if r.get('status') == 'failed']
        
        if not successful_tests:
            return {
                'total_tests': len(results),
                'successful_tests': 0,
                'failed_tests': len(failed_tests),
                'success_rate': 0.0
            }
        
        # è®¡ç®—å¹³å‡åˆ†æ•°
        avg_hallucination_resistance = sum(
            r.get('hallucination_tests', {}).get('resistance_score', 0) 
            for r in successful_tests
        ) / len(successful_tests)
        
        avg_diversity_score = sum(
            r.get('diversity_tests', {}).get('diversity_score', 0) 
            for r in successful_tests
        ) / len(successful_tests)
        
        avg_consistency_score = sum(
            r.get('consistency_tests', {}).get('consistency_score', 0) 
            for r in successful_tests
        ) / len(successful_tests)
        
        # æ‰¾å‡ºæœ€ä½³è¡¨ç°çš„æ¨¡å‹
        best_model = max(successful_tests, key=lambda x: (
            x.get('hallucination_tests', {}).get('resistance_score', 0) +
            x.get('diversity_tests', {}).get('diversity_score', 0) +
            x.get('consistency_tests', {}).get('consistency_score', 0)
        ))
        
        return {
            'total_tests': len(results),
            'successful_tests': len(successful_tests),
            'failed_tests': len(failed_tests),
            'success_rate': len(successful_tests) / len(results),
            'average_scores': {
                'hallucination_resistance': avg_hallucination_resistance,
                'cognitive_diversity': avg_diversity_score,
                'role_consistency': avg_consistency_score
            },
            'best_performing_model': {
                'name': best_model.get('model_name'),
                'scores': {
                    'hallucination_resistance': best_model.get('hallucination_tests', {}).get('resistance_score', 0),
                    'cognitive_diversity': best_model.get('diversity_tests', {}).get('diversity_score', 0),
                    'role_consistency': best_model.get('consistency_tests', {}).get('consistency_score', 0)
                }
            }
        }
    
    def save_results(self, results: Dict[str, Any], filename: str = None):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"cognitive_ecosystem_test_results_{timestamp}.json"
        
        results_dir = Path("test_results")
        results_dir.mkdir(exist_ok=True)
        
        filepath = results_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
        return filepath

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§  è®¤çŸ¥ç”Ÿæ€ç³»ç»Ÿäº‘æ¨¡å‹æµ‹è¯•å™¨")
    print("=" * 50)
    
    parser = argparse.ArgumentParser(description="Cloud Model Cognitive Ecosystem Test Runner.")
    parser.add_argument("--model", type=str, help="Specify a particular cloud model to test (e.g., 'service_name/model_name'). If not provided, all available models will be tested.")
    parser.add_argument("--intensity", type=str, choices=['light', 'medium', 'heavy'], help="Set the test intensity (light, medium, heavy). Defaults to medium.")
    args = parser.parse_args()

    tester = CognitiveEcosystemCloudTester()
    available_models = tester.get_available_models()
    
    if not available_models:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„äº‘æ¨¡å‹")
        return
    
    print(f"ğŸ“‹ å‘ç° {len(available_models)} ä¸ªå¯ç”¨æ¨¡å‹:")
    for model in available_models:
        print(f"  - {model['full_name']} ({model['service_display_name']})")
    
    models_to_test = []
    if args.model:
        # Check if the specified model is available
        specified_model_found = False
        for model_info in available_models:
            if model_info['full_name'] == args.model:
                models_to_test.append(model_info)
                specified_model_found = True
                break
        if not specified_model_found:
            print(f"âŒ æŒ‡å®šçš„æ¨¡å‹ '{args.model}' æœªæ‰¾åˆ°ã€‚è¯·æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®ã€‚")
            return
        print(f"\nå°†ä»…æµ‹è¯•æŒ‡å®šçš„æ¨¡å‹: {args.model}")
    else:
        # If no model is specified, prompt the user
        confirm = input("\næ˜¯å¦å¼€å§‹å¯¹æ‰€æœ‰å¯ç”¨æ¨¡å‹è¿›è¡Œæµ‹è¯•ï¼Ÿ(y/N): ").strip().lower()
        if confirm not in ['y', 'yes', 'æ˜¯']:
            print("æµ‹è¯•å·²å–æ¶ˆã€‚")
            return
        models_to_test = available_models

    # Determine test intensity
    intensity = args.intensity if args.intensity else "medium"
    if not args.model: # Only prompt for intensity if not specifying a model
        print("\nè¯·é€‰æ‹©æµ‹è¯•å¼ºåº¦:")
        print("  1. è½»é‡çº§æµ‹è¯• (2ä¸ªè§’è‰²ï¼Œå¿«é€Ÿ)")
        print("  2. æ ‡å‡†æµ‹è¯• (4ä¸ªè§’è‰²ï¼Œä¸­ç­‰)")
        print("  3. å®Œæ•´æµ‹è¯• (4ä¸ªè§’è‰²ï¼Œè¯¦ç»†)")
        choice = input("è¯·é€‰æ‹© (1-3ï¼Œé»˜è®¤2): ").strip() or "2"
        intensity_map = {"1": "light", "2": "medium", "3": "heavy"}
        intensity = intensity_map.get(choice, "medium")
    
    # åˆ›å»ºæµ‹è¯•é…ç½®
    test_config = tester.create_test_config(intensity)
    
    print(f"\nğŸ”§ æµ‹è¯•é…ç½®:")
    print(f"  - æµ‹è¯•å¼ºåº¦: {intensity}")
    print(f"  - æµ‹è¯•è§’è‰²: {', '.join(test_config['test_roles'])}")
    
    if models_to_test:
        results = tester.run_batch_test(models_to_test, test_config)
        tester.save_results(results)
    else:
        print("æ²¡æœ‰æ¨¡å‹å¯ä¾›æµ‹è¯•ã€‚")

if __name__ == "__main__":
    main()
