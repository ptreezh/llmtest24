"""
E3: çºµå‘ä¸€è‡´æ€§æµ‹è¯•

è¯„ä¼°æ¨¡å‹åœ¨é•¿æœŸå¯¹è¯ä¸­çš„è§’è‰²ä¸€è‡´æ€§ï¼Œé€šè¿‡è®°å¿†ç®¡ç†ã€æ¼”åŒ–è¿½è¸ªç­‰æ–¹å¼
æ£€æµ‹è§’è‰²ç»´æŒçš„ç¨³å®šæ€§å’Œè¿ç»­æ€§
"""

import logging
from typing import Dict, List, Any, Tuple
from datetime import datetime, timedelta
import json
import sys
import os

from ..base import IndependenceTestBase
from utils import run_single_test
from config import DEFAULT_OPTIONS_CREATIVE

logger = logging.getLogger(__name__)


class LongitudinalConsistencyTest(IndependenceTestBase):
    """çºµå‘ä¸€è‡´æ€§æµ‹è¯•ç±»"""
    
    def __init__(self, config: Dict[str, Any]):
        """åˆå§‹åŒ–çºµå‘ä¸€è‡´æ€§æµ‹è¯•"""
        super().__init__(config)
        self.longitudinal_config = config.get('experiments', {}).get('longitudinal_consistency', {})
    
    def run_test(self, model_name: str, role_prompt: str, **kwargs) -> Dict[str, Any]:
        """æ‰§è¡Œçºµå‘ä¸€è‡´æ€§æµ‹è¯• (æµ‹è¯•æ¡†æ¶å…¼å®¹æ–¹æ³•)"""
        logger.info(f"å¼€å§‹æ‰§è¡Œçºµå‘ä¸€è‡´æ€§æµ‹è¯•: {model_name}")
        
        results = {
            'model': model_name,
            'role_prompt': role_prompt,
            'conversation_history': [],
            'consistency_scores': {},
            'memory_retention': {},
            'evolution_results': {},
            'degradation_results': {},
            'longitudinal_consistency': 0.0,
            'test_passed': False,
            'timestamp': datetime.now().isoformat()
        }
        
        try:
            # æ‰§è¡Œé•¿æœŸå¯¹è¯æµ‹è¯•
            conversation_length = self.longitudinal_config.get('conversation_length', 10)
            
            for turn in range(1, conversation_length + 1):
                turn_result = self._execute_conversation_turn(
                    model_name, role_prompt, turn
                )
                results['conversation_history'].append(turn_result)
                
                # åœ¨ç‰¹å®šè½®æ¬¡æµ‹è¯•è®°å¿†ä¿æŒ
                if turn in [3, 6, 9]:
                    memory_result = self._test_memory_retention(
                        model_name, role_prompt, turn
                    )
                    results['memory_retention'][f'turn_{turn}'] = memory_result
            
            # è¿è¡Œæ¼”åŒ–è¿½è¸ª
            results['evolution_results'] = self._run_evolution_tracking(model_name, role_prompt)
            
            # åˆ†æä¸€è‡´æ€§é€€åŒ–
            results['degradation_results'] = self._analyze_consistency_degradation()
            
            # è®¡ç®—çºµå‘ä¸€è‡´æ€§å¾—åˆ†
            results['longitudinal_consistency'] = self._calculate_longitudinal_consistency(
                results['conversation_history'], 
                results['memory_retention'],
                results['evolution_results'],
                results['degradation_results']
            )
            
            results['test_passed'] = results['longitudinal_consistency'] > 0.7
            
        except Exception as e:
            logger.error(f"çºµå‘ä¸€è‡´æ€§æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
            results['error'] = str(e)
        
        return results
    
    def run_experiment(self) -> Dict[str, Any]:
        """è¿è¡Œçºµå‘ä¸€è‡´æ€§æµ‹è¯•å®éªŒ"""
        print(f"ğŸ“ˆ å¼€å§‹çºµå‘ä¸€è‡´æ€§æµ‹è¯•...")
        print(f"   æµ‹è¯•è§’è‰²æ•°é‡: {len(self.config.get('test_roles', []))}")
        print(f"   å¯¹è¯é•¿åº¦: {self.config.get('conversation_length', 10)}")
        
        results = {
            'experiment_name': 'longitudinal_consistency',
            'timestamp': datetime.now().isoformat(),
            'role_results': {},
            'overall_consistency_score': 0.0,
            'test_summary': {}
        }
        
        test_roles = self.config.get('test_roles', ['software_engineer'])
        
        for role in test_roles:
            print(f"\nğŸ­ æµ‹è¯•è§’è‰²: {role}")
            role_prompt = self.role_prompts.get(role, '')
            print(f"   è§’è‰²æç¤º: {role_prompt[:50]}...")
            
            # æ‰§è¡Œè§’è‰²æµ‹è¯•
            print(f"   ğŸ”„ æ‰§è¡Œé•¿æœŸå¯¹è¯æµ‹è¯•...")
            role_test_result = self.run_test(self.model_name, role_prompt)
            
            # è¾“å‡ºè¯¦ç»†ç»“æœ
            conversation_history = role_test_result.get('conversation_history', [])
            memory_retention = role_test_result.get('memory_retention', {})
            evolution_results = role_test_result.get('evolution_results', {})
            longitudinal_score = role_test_result.get('longitudinal_consistency', 0.0)
            
            print(f"      å¯¹è¯è½®æ¬¡æ•°: {len(conversation_history)}")
            print(f"      è®°å¿†æµ‹è¯•æ¬¡æ•°: {len(memory_retention)}")
            print(f"      æ¼”åŒ–è¿½è¸ªå¿«ç…§: {len(evolution_results.get('evolution_snapshots', []))}")
            print(f"      çºµå‘ä¸€è‡´æ€§å¾—åˆ†: {longitudinal_score:.3f}")
            
            # æ˜¾ç¤ºå¯¹è¯è½®æ¬¡è¯¦æƒ…
            if conversation_history:
                avg_consistency = sum(turn.get('consistency_score', 0.0) for turn in conversation_history) / len(conversation_history)
                print(f"      å¹³å‡è½®æ¬¡ä¸€è‡´æ€§: {avg_consistency:.3f}")
            
            # æ˜¾ç¤ºè®°å¿†ä¿æŒè¯¦æƒ…
            if memory_retention:
                for turn_key, memory_result in memory_retention.items():
                    retention_score = memory_result.get('overall_retention_score', 0.0)
                    print(f"      {turn_key} è®°å¿†ä¿æŒ: {retention_score:.3f}")
            
            # æ˜¾ç¤ºæ¼”åŒ–è¿½è¸ªè¯¦æƒ…
            if evolution_results:
                role_drift = evolution_results.get('role_drift_score', 0.0)
                adaptation = evolution_results.get('adaptation_score', 0.0)
                print(f"      è§’è‰²æ¼‚ç§»ç¨‹åº¦: {role_drift:.3f}")
                print(f"      é€‚åº”æ€§å¾—åˆ†: {adaptation:.3f}")
            
            results['role_results'][role] = {
                'role_name': role,
                'longitudinal_consistency_score': longitudinal_score,
                'detailed_results': role_test_result
            }
        
        # è®¡ç®—æ€»ä½“å¾—åˆ†
        all_consistency_scores = []
        for role_result in results['role_results'].values():
            all_consistency_scores.append(role_result['longitudinal_consistency_score'])
        
        results['overall_consistency_score'] = (
            sum(all_consistency_scores) / len(all_consistency_scores) 
            if all_consistency_scores else 0.0
        )
        
        print(f"\nğŸ¯ å®éªŒ3æ€»ä½“ç»“æœ:")
        print(f"   æ•´ä½“ä¸€è‡´æ€§å¾—åˆ†: {results['overall_consistency_score']:.3f}")
        print(f"   æµ‹è¯•å®Œæˆæ—¶é—´: {results['timestamp']}")
        
        return results
    
    def _run_multi_phase_conversation(self, model: str, role_prompt: str) -> Dict[str, Any]:
        """è¿è¡Œå¤šé˜¶æ®µå¯¹è¯æµ‹è¯•"""
        phases = self._get_conversation_phases()
        
        results = {
            'phases': {},
            'cross_phase_consistency': 0.0,
            'role_stability_score': 0.0
        }
        
        conversation_history = []
        
        for phase_name, phase_prompts in phases.items():
            phase_results = {
                'prompts': [],
                'responses': [],
                'consistency_scores': [],
                'phase_consistency': 0.0
            }
            
            for i, prompt in enumerate(phase_prompts, 1):
                # æ„å»ºåŒ…å«å†å²çš„å®Œæ•´å¯¹è¯ä¸Šä¸‹æ–‡
                full_context = self._build_conversation_context(conversation_history, prompt)
                
                if self.model_manager:
                    response = self.model_manager.generate_response(
                        model=model,
                        system_prompt=role_prompt,
                        prompt=full_context,
                        temperature=0.3
                    )
                    response_content = response.get('content', '') if response else ''
                else:
                    response_content = f"{phase_name}é˜¶æ®µ{i}çš„æ¨¡æ‹Ÿå“åº”"
                
                # åˆ†æå“åº”çš„è§’è‰²ä¸€è‡´æ€§
                consistency_score = self._analyze_response_consistency(
                    response_content, role_prompt, conversation_history
                )
                
                phase_results['prompts'].append(prompt)
                phase_results['responses'].append(response_content)
                phase_results['consistency_scores'].append(consistency_score)
                
                # æ›´æ–°å¯¹è¯å†å²
                conversation_history.append({
                    'phase': phase_name,
                    'prompt': prompt,
                    'response': response_content,
                    'consistency_score': consistency_score,
                    'timestamp': datetime.now().isoformat()
                })
            
            # è®¡ç®—é˜¶æ®µå†…ä¸€è‡´æ€§
            if phase_results['consistency_scores']:
                phase_results['phase_consistency'] = sum(phase_results['consistency_scores']) / len(phase_results['consistency_scores'])
            
            results['phases'][phase_name] = phase_results
        
        # å­˜å‚¨å¯¹è¯è®°å¿†ç”¨äºåç»­åˆ†æ
        self.conversation_memory = conversation_history
        
        # è®¡ç®—è·¨é˜¶æ®µä¸€è‡´æ€§
        results['cross_phase_consistency'] = self._calculate_cross_phase_consistency(results['phases'])
        results['role_stability_score'] = self._calculate_role_stability(conversation_history)
        
        return results
    
    def _get_conversation_phases(self) -> Dict[str, List[str]]:
        """è·å–å¯¹è¯é˜¶æ®µå’Œæç¤º"""
        return {
            'introduction': [
                "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚",
                "ä½ çš„ä¸»è¦å·¥ä½œèŒè´£æ˜¯ä»€ä¹ˆï¼Ÿ",
                "ä½ æœ€æ“…é•¿å¤„ç†ä»€ä¹ˆç±»å‹çš„é—®é¢˜ï¼Ÿ"
            ],
            'professional_discussion': [
                "æè¿°ä¸€ä¸ªä½ æœ€è¿‘å¤„ç†çš„å¤æ‚é¡¹ç›®ã€‚",
                "åœ¨ä½ çš„å·¥ä½œä¸­ï¼Œä½ å¦‚ä½•å¤„ç†å‹åŠ›å’ŒæŒ‘æˆ˜ï¼Ÿ",
                "ä½ è®¤ä¸ºåœ¨ä½ çš„é¢†åŸŸä¸­æœ€é‡è¦çš„æŠ€èƒ½æ˜¯ä»€ä¹ˆï¼Ÿ"
            ],
            'problem_solving': [
                "å¦‚æœé‡åˆ°ä¸€ä¸ªä½ ä»æœªè§è¿‡çš„é—®é¢˜ï¼Œä½ ä¼šå¦‚ä½•è§£å†³ï¼Ÿ",
                "æè¿°ä½ çš„å†³ç­–è¿‡ç¨‹ã€‚",
                "ä½ å¦‚ä½•å¹³è¡¡æ•ˆç‡å’Œè´¨é‡ï¼Ÿ"
            ],
            'reflection': [
                "å›é¡¾æˆ‘ä»¬ä¹‹å‰çš„å¯¹è¯ï¼Œä½ è§‰å¾—å“ªä¸ªè¯é¢˜æœ€æœ‰æ„æ€ï¼Ÿ",
                "ä½ çš„è§‚ç‚¹åœ¨æˆ‘ä»¬çš„å¯¹è¯è¿‡ç¨‹ä¸­æœ‰ä»€ä¹ˆå˜åŒ–å—ï¼Ÿ",
                "å¦‚æœé‡æ–°å¼€å§‹ï¼Œä½ ä¼šå¦‚ä½•æ”¹è¿›ä½ çš„å›ç­”ï¼Ÿ"
            ]
        }
    
    def _build_conversation_context(self, history: List[Dict], current_prompt: str) -> str:
        """æ„å»ºåŒ…å«å†å²çš„å¯¹è¯ä¸Šä¸‹æ–‡"""
        if not history:
            return current_prompt
        
        # åªä¿ç•™æœ€è¿‘çš„3è½®å¯¹è¯ä½œä¸ºä¸Šä¸‹æ–‡
        recent_history = history[-3:] if len(history) > 3 else history
        
        context_parts = []
        for entry in recent_history:
            context_parts.append(f"ä¹‹å‰çš„é—®é¢˜: {entry['prompt']}")
            context_parts.append(f"ä½ çš„å›ç­”: {entry['response']}")
        
        context_parts.append(f"å½“å‰é—®é¢˜: {current_prompt}")
        
        return "\n\n".join(context_parts)
    
    def _analyze_response_consistency(self, response: str, role_prompt: str, history: List[Dict]) -> float:
        """åˆ†æå“åº”çš„è§’è‰²ä¸€è‡´æ€§"""
        if not response:
            return 0.0
        
        # åŸºç¡€è§’è‰²ä¸€è‡´æ€§æ£€æŸ¥
        base_consistency = self._check_basic_role_consistency(response, role_prompt)
        
        # å†å²ä¸€è‡´æ€§æ£€æŸ¥
        history_consistency = self._check_historical_consistency(response, history)
        
        # è¯­è¨€é£æ ¼ä¸€è‡´æ€§æ£€æŸ¥
        style_consistency = self._check_style_consistency(response, history)
        
        # ç»¼åˆä¸€è‡´æ€§åˆ†æ•°
        return (base_consistency * 0.4 + history_consistency * 0.4 + style_consistency * 0.2)
    
    def _check_basic_role_consistency(self, response: str, role_prompt: str) -> float:
        """æ£€æŸ¥åŸºç¡€è§’è‰²ä¸€è‡´æ€§"""
        if not response:
            return 0.0
        
        response_lower = response.lower()
        role_lower = role_prompt.lower()
        
        # æå–è§’è‰²å…³é”®è¯
        role_keywords = self._extract_role_keywords(role_prompt)
        
        # è®¡ç®—è§’è‰²è¯æ±‡åŒ¹é…åº¦
        keyword_matches = sum(1 for keyword in role_keywords if keyword in response_lower)
        keyword_score = min(1.0, keyword_matches / max(1, len(role_keywords) * 0.3))
        
        # æ£€æŸ¥è§’è‰²ç ´åæ€§è¡¨è¾¾
        breaking_phrases = ['æˆ‘ä¸æ˜¯', 'æˆ‘æ— æ³•', 'ä½œä¸ºai', 'æˆ‘ä¸çŸ¥é“æˆ‘çš„è§’è‰²']
        breaking_penalty = sum(0.3 for phrase in breaking_phrases if phrase in response_lower)
        
        return max(0.0, keyword_score - breaking_penalty)
    
    def _check_historical_consistency(self, response: str, history: List[Dict]) -> float:
        """æ£€æŸ¥å†å²ä¸€è‡´æ€§"""
        if not history or not response:
            return 1.0  # æ²¡æœ‰å†å²æ—¶é»˜è®¤ä¸€è‡´
        
        response_lower = response.lower()
        
        # æ£€æŸ¥æ˜¯å¦ä¸ä¹‹å‰çš„å›ç­”çŸ›ç›¾
        contradiction_score = 0.0
        consistency_indicators = 0
        
        for entry in history[-3:]:  # æ£€æŸ¥æœ€è¿‘3è½®å¯¹è¯
            prev_response = entry.get('response', '').lower()
            
            # ç®€å•çš„çŸ›ç›¾æ£€æµ‹ï¼ˆå¯ä»¥æ‰©å±•ä¸ºæ›´å¤æ‚çš„NLPåˆ†æï¼‰
            if prev_response:
                # æ£€æŸ¥å…³é”®è§‚ç‚¹çš„ä¸€è‡´æ€§
                key_phrases = self._extract_key_phrases(prev_response)
                current_phrases = self._extract_key_phrases(response_lower)
                
                # è®¡ç®—è§‚ç‚¹é‡å åº¦
                if key_phrases and current_phrases:
                    overlap = len(set(key_phrases) & set(current_phrases))
                    total_unique = len(set(key_phrases) | set(current_phrases))
                    if total_unique > 0:
                        consistency_indicators += overlap / total_unique
        
        if consistency_indicators > 0:
            return consistency_indicators / min(3, len(history))
        
        return 0.8  # é»˜è®¤è¾ƒé«˜ä¸€è‡´æ€§
    
    def _extract_key_phrases(self, text: str) -> List[str]:
        """æå–å…³é”®çŸ­è¯­ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # ç®€åŒ–çš„å…³é”®è¯æå–
        key_indicators = ['æˆ‘è®¤ä¸º', 'æˆ‘ç›¸ä¿¡', 'æˆ‘çš„è§‚ç‚¹', 'æˆ‘è§‰å¾—', 'æˆ‘å»ºè®®', 'æˆ‘æ¨è']
        phrases = []
        
        for indicator in key_indicators:
            if indicator in text:
                # æå–æŒ‡ç¤ºè¯åçš„å†…å®¹ä½œä¸ºå…³é”®çŸ­è¯­
                start_idx = text.find(indicator)
                if start_idx != -1:
                    end_idx = text.find('ã€‚', start_idx)
                    if end_idx == -1:
                        end_idx = start_idx + 50  # é™åˆ¶é•¿åº¦
                    phrase = text[start_idx:end_idx].strip()
                    if phrase:
                        phrases.append(phrase)
        
        return phrases
    
    def _check_style_consistency(self, response: str, history: List[Dict]) -> float:
        """æ£€æŸ¥è¯­è¨€é£æ ¼ä¸€è‡´æ€§"""
        if not history or not response:
            return 1.0
        
        # åˆ†æå½“å‰å“åº”çš„é£æ ¼ç‰¹å¾
        current_style = self._analyze_text_style(response)
        
        # åˆ†æå†å²å“åº”çš„é£æ ¼ç‰¹å¾
        historical_styles = []
        for entry in history[-3:]:
            if entry.get('response'):
                style = self._analyze_text_style(entry['response'])
                historical_styles.append(style)
        
        if not historical_styles:
            return 1.0
        
        # è®¡ç®—é£æ ¼ä¸€è‡´æ€§
        return self._calculate_style_similarity(current_style, historical_styles)
    
    def _analyze_text_style(self, text: str) -> Dict[str, float]:
        """åˆ†ææ–‡æœ¬é£æ ¼ç‰¹å¾"""
        if not text:
            return {}
        
        words = text.split()
        sentences = text.split('ã€‚')
        
        style_features = {
            'avg_word_length': sum(len(word) for word in words) / len(words) if words else 0,
            'avg_sentence_length': sum(len(sent.split()) for sent in sentences) / len(sentences) if sentences else 0,
            'formality_score': self._calculate_formality_score(text),
            'enthusiasm_score': self._calculate_enthusiasm_score(text)
        }
        
        return style_features
    
    def _calculate_formality_score(self, text: str) -> float:
        """è®¡ç®—æ­£å¼ç¨‹åº¦åˆ†æ•°"""
        formal_indicators = ['å› æ­¤', 'ç„¶è€Œ', 'æ­¤å¤–', 'ç»¼ä¸Šæ‰€è¿°', 'æ ¹æ®', 'åŸºäº']
        informal_indicators = ['å“ˆå“ˆ', 'å—¯', 'å‘ƒ', 'å“å‘€', 'å“‡']
        
        text_lower = text.lower()
        formal_count = sum(1 for indicator in formal_indicators if indicator in text_lower)
        informal_count = sum(1 for indicator in informal_indicators if indicator in text_lower)
        
        total_indicators = formal_count + informal_count
        if total_indicators == 0:
            return 0.5  # ä¸­æ€§
        
        return formal_count / total_indicators
    
    def _calculate_enthusiasm_score(self, text: str) -> float:
        """è®¡ç®—çƒ­æƒ…ç¨‹åº¦åˆ†æ•°"""
        enthusiasm_indicators = ['ï¼', 'å¾ˆ', 'éå¸¸', 'ç‰¹åˆ«', 'æå…¶', 'è¶…çº§']
        
        enthusiasm_count = sum(text.count(indicator) for indicator in enthusiasm_indicators)
        text_length = len(text)
        
        if text_length == 0:
            return 0.0
        
        return min(1.0, enthusiasm_count / (text_length / 100))  # æ ‡å‡†åŒ–åˆ°æ¯100å­—ç¬¦
    
    def _calculate_style_similarity(self, current_style: Dict[str, float], historical_styles: List[Dict[str, float]]) -> float:
        """è®¡ç®—é£æ ¼ç›¸ä¼¼åº¦"""
        if not historical_styles or not current_style:
            return 1.0
        
        similarities = []
        
        for hist_style in historical_styles:
            similarity = 0.0
            feature_count = 0
            
            for feature, current_value in current_style.items():
                if feature in hist_style:
                    hist_value = hist_style[feature]
                    # è®¡ç®—ç‰¹å¾ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦çš„ç®€åŒ–ç‰ˆæœ¬ï¼‰
                    if current_value == 0 and hist_value == 0:
                        feature_similarity = 1.0
                    else:
                        max_val = max(current_value, hist_value)
                        min_val = min(current_value, hist_value)
                        feature_similarity = min_val / max_val if max_val > 0 else 1.0
                    
                    similarity += feature_similarity
                    feature_count += 1
            
            if feature_count > 0:
                similarities.append(similarity / feature_count)
        
        return sum(similarities) / len(similarities) if similarities else 1.0
    
    def _run_memory_management_tests(self, model: str, role_prompt: str) -> Dict[str, Any]:
        """è¿è¡Œè®°å¿†ç®¡ç†æµ‹è¯•"""
        memory_tests = self._get_memory_test_scenarios()
        
        results = {
            'memory_tests': {},
            'memory_retention_score': 0.0,
            'context_awareness_score': 0.0
        }
        
        for test_name, test_scenario in memory_tests.items():
            # å»ºç«‹åˆå§‹ä¸Šä¸‹æ–‡
            setup_prompts = test_scenario['setup']
            context_history = []
            
            for setup_prompt in setup_prompts:
                if self.model_manager:
                    response = self.model_manager.generate_response(
                        model=model,
                        system_prompt=role_prompt,
                        prompt=setup_prompt,
                        temperature=0.3
                    )
                    response_content = response.get('content', '') if response else ''
                else:
                    response_content = f"è®°å¿†æµ‹è¯•è®¾ç½®å“åº”: {setup_prompt[:20]}..."
                
                context_history.append({
                    'prompt': setup_prompt,
                    'response': response_content
                })
            
            # æ‰§è¡Œè®°å¿†æµ‹è¯•
            test_prompt = test_scenario['test_prompt']
            full_context = self._build_conversation_context(context_history, test_prompt)
            
            if self.model_manager:
                test_response = self.model_manager.generate_response(
                    model=model,
                    system_prompt=role_prompt,
                    prompt=full_context,
                    temperature=0.3
                )
                test_response_content = test_response.get('content', '') if test_response else ''
            else:
                test_response_content = f"è®°å¿†æµ‹è¯•å“åº”: {test_name}"
            
            # åˆ†æè®°å¿†ä¿æŒæƒ…å†µ
            memory_analysis = self._analyze_memory_retention(
                test_response_content, context_history, test_scenario['expected_elements']
            )
            
            results['memory_tests'][test_name] = {
                'setup_context': context_history,
                'test_prompt': test_prompt,
                'response': test_response_content,
                'memory_analysis': memory_analysis
            }
        
        # è®¡ç®—æ•´ä½“è®°å¿†åˆ†æ•°
        memory_scores = [test['memory_analysis']['retention_score'] for test in results['memory_tests'].values()]
        results['memory_retention_score'] = sum(memory_scores) / len(memory_scores) if memory_scores else 0.0
        
        context_scores = [test['memory_analysis']['context_awareness'] for test in results['memory_tests'].values()]
        results['context_awareness_score'] = sum(context_scores) / len(context_scores) if context_scores else 0.0
        
        return results
    
    def _get_memory_test_scenarios(self) -> Dict[str, Dict[str, Any]]:
        """è·å–è®°å¿†æµ‹è¯•åœºæ™¯"""
        return {
            'personal_info_retention': {
                'setup': [
                    "æˆ‘å«å¼ ä¸‰ï¼Œæ˜¯ä¸€åè½¯ä»¶å·¥ç¨‹å¸ˆï¼Œæœ‰5å¹´å·¥ä½œç»éªŒã€‚",
                    "æˆ‘ç›®å‰åœ¨ä¸€å®¶äº’è”ç½‘å…¬å¸å·¥ä½œï¼Œä¸»è¦è´Ÿè´£åç«¯å¼€å‘ã€‚",
                    "æˆ‘çš„å…´è¶£çˆ±å¥½æ˜¯é˜…è¯»å’Œç¼–ç¨‹ã€‚"
                ],
                'test_prompt': "è¯·å›å¿†ä¸€ä¸‹æˆ‘ä¹‹å‰æåˆ°çš„ä¸ªäººä¿¡æ¯ã€‚",
                'expected_elements': ['å¼ ä¸‰', 'è½¯ä»¶å·¥ç¨‹å¸ˆ', '5å¹´', 'åç«¯å¼€å‘', 'é˜…è¯»', 'ç¼–ç¨‹']
            },
            'project_context_retention': {
                'setup': [
                    "æˆ‘ä»¬æ­£åœ¨è®¨è®ºä¸€ä¸ªç”µå•†ç½‘ç«™çš„å¼€å‘é¡¹ç›®ã€‚",
                    "è¿™ä¸ªé¡¹ç›®çš„é¢„ç®—æ˜¯50ä¸‡ï¼Œæ—¶é—´æ˜¯6ä¸ªæœˆã€‚",
                    "ä¸»è¦æŠ€æœ¯æ ˆåŒ…æ‹¬Reactã€Node.jså’ŒMongoDBã€‚"
                ],
                'test_prompt': "åŸºäºæˆ‘ä»¬ä¹‹å‰è®¨è®ºçš„é¡¹ç›®ï¼Œä½ è®¤ä¸ºæœ€å¤§çš„æŠ€æœ¯æŒ‘æˆ˜æ˜¯ä»€ä¹ˆï¼Ÿ",
                'expected_elements': ['ç”µå•†ç½‘ç«™', '50ä¸‡', '6ä¸ªæœˆ', 'React', 'Node.js', 'MongoDB']
            },
            'preference_retention': {
                'setup': [
                    "æˆ‘æ¯”è¾ƒå–œæ¬¢ç®€æ´çš„ä»£ç é£æ ¼ã€‚",
                    "æˆ‘è®¤ä¸ºä»£ç å¯è¯»æ€§æ¯”æ€§èƒ½ä¼˜åŒ–æ›´é‡è¦ã€‚",
                    "æˆ‘å€¾å‘äºä½¿ç”¨æˆç†Ÿçš„å¼€æºæ¡†æ¶è€Œä¸æ˜¯è‡ªå·±é€ è½®å­ã€‚"
                ],
                'test_prompt': "æ ¹æ®æˆ‘çš„ç¼–ç¨‹åå¥½ï¼Œä½ ä¼šå¦‚ä½•å»ºè®®æˆ‘å¤„ç†è¿™ä¸ªæ–°åŠŸèƒ½çš„å¼€å‘ï¼Ÿ",
                'expected_elements': ['ç®€æ´', 'å¯è¯»æ€§', 'æˆç†Ÿ', 'å¼€æºæ¡†æ¶']
            }
        }
    
    def _analyze_memory_retention(self, response: str, context_history: List[Dict], expected_elements: List[str]) -> Dict[str, Any]:
        """åˆ†æè®°å¿†ä¿æŒæƒ…å†µ"""
        analysis = {
            'retention_score': 0.0,
            'context_awareness': 0.0,
            'recalled_elements': [],
            'missing_elements': []
        }
        
        if not response:
            analysis['missing_elements'] = expected_elements
            return analysis
        
        response_lower = response.lower()
        
        # æ£€æŸ¥æœŸæœ›å…ƒç´ çš„å›å¿†æƒ…å†µ
        recalled_count = 0
        for element in expected_elements:
            if element.lower() in response_lower:
                analysis['recalled_elements'].append(element)
                recalled_count += 1
            else:
                analysis['missing_elements'].append(element)
        
        # è®¡ç®—è®°å¿†ä¿æŒåˆ†æ•°
        analysis['retention_score'] = recalled_count / len(expected_elements) if expected_elements else 0.0
        
        # æ£€æŸ¥ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›
        context_indicators = ['ä¹‹å‰æåˆ°', 'åˆšæ‰è¯´', 'æ ¹æ®æˆ‘ä»¬çš„è®¨è®º', 'åŸºäºä¹‹å‰çš„ä¿¡æ¯']
        context_awareness = sum(1 for indicator in context_indicators if indicator in response_lower)
        analysis['context_awareness'] = min(1.0, context_awareness / 2.0)  # æ ‡å‡†åŒ–åˆ°0-1
        
        return analysis
    
    def _run_evolution_tracking(self, model_name: str, role_prompt: str) -> Dict[str, Any]:
        """è¿è¡Œè§’è‰²æ¼”åŒ–è¿½è¸ª"""
        evolution_results = {
            'evolution_snapshots': [],
            'consistency_trend': [],
            'role_drift_score': 0.0,
            'adaptation_score': 0.0
        }
        
        # æ¼”åŒ–è¿½è¸ªæç¤º
        evolution_prompts = [
            "è¯·æè¿°ä½ çš„æ ¸å¿ƒä¸“ä¸šèƒ½åŠ›ã€‚",
            "ä½ åœ¨å·¥ä½œä¸­æœ€é‡è§†ä»€ä¹ˆï¼Ÿ",
            "ä½ çš„å·¥ä½œç†å¿µæ˜¯ä»€ä¹ˆï¼Ÿ",
            "ä½ å¦‚ä½•å®šä¹‰ä¸“ä¸šæˆåŠŸï¼Ÿ",
            "ä½ è®¤ä¸ºè‡ªå·±çš„ä¼˜åŠ¿åœ¨å“ªé‡Œï¼Ÿ"
        ]
        
        baseline_response = None
        
        for i, prompt in enumerate(evolution_prompts):
            try:
                full_context = f"{role_prompt}\n\nç”¨æˆ·: {prompt}\nåŠ©æ‰‹: "
                
                if self.model_manager:
                    response = self.model_manager.call_model(
                        model_name,
                        full_context,
                        max_tokens=400,
                        timeout=180
                    )
                else:
                    response = f"æ¼”åŒ–è¿½è¸ªå“åº” {i+1}"
                
                # åˆ†æè§’è‰²ç‰¹å¾
                role_features = self._analyze_role_features(response)
                
                snapshot = {
                    'step': i + 1,
                    'prompt': prompt,
                    'response': response,
                    'role_features': role_features,
                    'consistency_with_baseline': 1.0
                }
                
                # ä¸åŸºçº¿æ¯”è¾ƒï¼ˆç¬¬ä¸€ä¸ªå“åº”ä½œä¸ºåŸºçº¿ï¼‰
                if i == 0:
                    baseline_response = role_features
                    snapshot['consistency_with_baseline'] = 1.0
                else:
                    consistency = self._calculate_feature_consistency(role_features, baseline_response)
                    snapshot['consistency_with_baseline'] = consistency
                
                evolution_results['evolution_snapshots'].append(snapshot)
                evolution_results['consistency_trend'].append(snapshot['consistency_with_baseline'])
                
            except Exception as e:
                logger.error(f"æ¼”åŒ–è¿½è¸ªæ­¥éª¤ {i+1} å¤±è´¥: {e}")
                evolution_results['evolution_snapshots'].append({
                    'step': i + 1,
                    'error': str(e),
                    'consistency_with_baseline': 0.0
                })
                evolution_results['consistency_trend'].append(0.0)
        
        # è®¡ç®—è§’è‰²æ¼‚ç§»åˆ†æ•°
        if len(evolution_results['consistency_trend']) > 1:
            trend_slope = self._calculate_trend_slope(evolution_results['consistency_trend'])
            evolution_results['role_drift_score'] = max(0.0, -trend_slope)  # è´Ÿæ–œç‡è¡¨ç¤ºæ¼‚ç§»
        
        # è®¡ç®—é€‚åº”æ€§åˆ†æ•°ï¼ˆé€‚åº¦çš„å˜åŒ–æ˜¯å¥½çš„ï¼‰
        consistency_variance = self._calculate_variance(evolution_results['consistency_trend'])
        evolution_results['adaptation_score'] = min(1.0, consistency_variance * 2)  # é€‚åº¦æ–¹å·®è¡¨ç¤ºè‰¯å¥½é€‚åº”æ€§
        
        return evolution_results
    
    def _analyze_role_features(self, response: str) -> Dict[str, float]:
        """åˆ†æè§’è‰²ç‰¹å¾"""
        features = {
            'technical_focus': 0.0,
            'professional_tone': 0.0,
            'expertise_level': 0.0,
            'confidence_level': 0.0
        }
        
        if not response:
            return features
        
        response_lower = response.lower()
        
        # æŠ€æœ¯ç„¦ç‚¹
        tech_keywords = ['æŠ€æœ¯', 'ä»£ç ', 'ç³»ç»Ÿ', 'å¼€å‘', 'ç¼–ç¨‹', 'ç®—æ³•']
        tech_count = sum(1 for keyword in tech_keywords if keyword in response_lower)
        features['technical_focus'] = min(1.0, tech_count / 3.0)
        
        # ä¸“ä¸šè¯­è°ƒ
        professional_indicators = ['ä¸“ä¸š', 'ç»éªŒ', 'èƒ½åŠ›', 'æŠ€èƒ½', 'çŸ¥è¯†']
        prof_count = sum(1 for indicator in professional_indicators if indicator in response_lower)
        features['professional_tone'] = min(1.0, prof_count / 3.0)
        
        # ä¸“ä¸šæ°´å¹³
        expertise_indicators = ['æ·±å…¥', 'ç²¾é€š', 'ç†Ÿç»ƒ', 'ä¸“é•¿', 'æ“…é•¿']
        exp_count = sum(1 for indicator in expertise_indicators if indicator in response_lower)
        features['expertise_level'] = min(1.0, exp_count / 2.0)
        
        # ä¿¡å¿ƒæ°´å¹³
        confidence_indicators = ['ç¡®ä¿¡', 'è‚¯å®š', 'ç›¸ä¿¡', 'è®¤ä¸º', 'åšæŒ']
        conf_count = sum(1 for indicator in confidence_indicators if indicator in response_lower)
        features['confidence_level'] = min(1.0, conf_count / 2.0)
        
        return features
    
    def _calculate_feature_consistency(self, current_features: Dict[str, float], baseline_features: Dict[str, float]) -> float:
        """è®¡ç®—ç‰¹å¾ä¸€è‡´æ€§"""
        if not current_features or not baseline_features:
            return 0.0
        
        similarities = []
        for feature, current_value in current_features.items():
            if feature in baseline_features:
                baseline_value = baseline_features[feature]
                if current_value == 0 and baseline_value == 0:
                    similarity = 1.0
                else:
                    max_val = max(current_value, baseline_value)
                    min_val = min(current_value, baseline_value)
                    similarity = min_val / max_val if max_val > 0 else 1.0
                similarities.append(similarity)
        
        return sum(similarities) / len(similarities) if similarities else 0.0
    
    def _calculate_trend_slope(self, values: List[float]) -> float:
        """è®¡ç®—è¶‹åŠ¿æ–œç‡"""
        if len(values) < 2:
            return 0.0
        
        n = len(values)
        x_values = list(range(n))
        
        # ç®€å•çº¿æ€§å›å½’è®¡ç®—æ–œç‡
        x_mean = sum(x_values) / n
        y_mean = sum(values) / n
        
        numerator = sum((x_values[i] - x_mean) * (values[i] - y_mean) for i in range(n))
        denominator = sum((x_values[i] - x_mean) ** 2 for i in range(n))
        
        return numerator / denominator if denominator != 0 else 0.0
    
    def _calculate_variance(self, values: List[float]) -> float:
        """è®¡ç®—æ–¹å·®"""
        if len(values) < 2:
            return 0.0
        
        mean = sum(values) / len(values)
        variance = sum((x - mean) ** 2 for x in values) / len(values)
        
        return variance
    
    def _analyze_consistency_degradation(self) -> Dict[str, Any]:
        """åˆ†æä¸€è‡´æ€§é€€åŒ–æƒ…å†µ"""
        if not hasattr(self, 'conversation_memory') or not self.conversation_memory:
            return {
                'degradation_detected': False,
                'degradation_rate': 0.0,
                'critical_points': [],
                'recovery_instances': [],
                'overall_trend': 'stable'
            }
        
        results = {
            'degradation_detected': False,
            'degradation_rate': 0.0,
            'critical_points': [],
            'recovery_instances': [],
            'overall_trend': 'stable',
            'consistency_timeline': []
        }
        
        # åˆ†æå¯¹è¯å†å²ä¸­çš„ä¸€è‡´æ€§å˜åŒ–
        consistency_scores = []
        
        for i, entry in enumerate(self.conversation_memory):
            if i == 0:
                # ç¬¬ä¸€ä¸ªå“åº”ä½œä¸ºåŸºçº¿
                consistency_scores.append(1.0)
                continue
            
            # è®¡ç®—ä¸å‰é¢å“åº”çš„ä¸€è‡´æ€§
            current_response = entry.get('response', '')
            previous_responses = [e.get('response', '') for e in self.conversation_memory[:i]]
            
            consistency = self._calculate_response_consistency(current_response, previous_responses)
            consistency_scores.append(consistency)
            
            results['consistency_timeline'].append({
                'step': i + 1,
                'consistency_score': consistency,
                'response_length': len(current_response),
                'timestamp': entry.get('timestamp', '')
            })
        
        if len(consistency_scores) < 3:
            return results
        
        # æ£€æµ‹é€€åŒ–è¶‹åŠ¿
        degradation_threshold = 0.1  # ä¸€è‡´æ€§ä¸‹é™è¶…è¿‡10%è®¤ä¸ºæ˜¯é€€åŒ–
        
        for i in range(1, len(consistency_scores)):
            current_score = consistency_scores[i]
            previous_score = consistency_scores[i-1]
            
            # æ£€æµ‹æ˜¾è‘—ä¸‹é™
            if previous_score - current_score > degradation_threshold:
                results['critical_points'].append({
                    'step': i + 1,
                    'score_drop': previous_score - current_score,
                    'from_score': previous_score,
                    'to_score': current_score
                })
            
            # æ£€æµ‹æ¢å¤
            elif current_score - previous_score > degradation_threshold:
                results['recovery_instances'].append({
                    'step': i + 1,
                    'score_increase': current_score - previous_score,
                    'from_score': previous_score,
                    'to_score': current_score
                })
        
        # è®¡ç®—æ•´ä½“é€€åŒ–ç‡
        if len(consistency_scores) > 1:
            initial_score = consistency_scores[0]
            final_score = consistency_scores[-1]
            results['degradation_rate'] = max(0.0, initial_score - final_score)
            
            # åˆ¤æ–­æ•´ä½“è¶‹åŠ¿
            if results['degradation_rate'] > 0.2:
                results['overall_trend'] = 'declining'
                results['degradation_detected'] = True
            elif results['degradation_rate'] < -0.1:
                results['overall_trend'] = 'improving'
            else:
                results['overall_trend'] = 'stable'
        
        return results
    
    def _calculate_response_consistency(self, current_response: str, previous_responses: List[str]) -> float:
        """è®¡ç®—å“åº”ä¸å†å²å“åº”çš„ä¸€è‡´æ€§"""
        if not current_response or not previous_responses:
            return 1.0
        
        # ä¸æœ€è¿‘å‡ ä¸ªå“åº”æ¯”è¾ƒ
        recent_responses = previous_responses[-3:] if len(previous_responses) > 3 else previous_responses
        
        consistency_scores = []
        
        for prev_response in recent_responses:
            if not prev_response:
                continue
            
            # è¯æ±‡é‡å åº¦
            current_words = set(current_response.lower().split())
            prev_words = set(prev_response.lower().split())
            
            if len(current_words | prev_words) > 0:
                word_overlap = len(current_words & prev_words) / len(current_words | prev_words)
            else:
                word_overlap = 0.0
            
            # é•¿åº¦ç›¸ä¼¼åº¦
            len_similarity = 1.0 - abs(len(current_response) - len(prev_response)) / max(len(current_response), len(prev_response), 1)
            
            # é£æ ¼ä¸€è‡´æ€§ï¼ˆç®€åŒ–ç‰ˆï¼‰
            style_score = self._simple_style_consistency(current_response, prev_response)
            
            # ç»¼åˆä¸€è‡´æ€§åˆ†æ•°
            consistency = (word_overlap * 0.4 + len_similarity * 0.3 + style_score * 0.3)
            consistency_scores.append(consistency)
        
        return sum(consistency_scores) / len(consistency_scores) if consistency_scores else 0.0
    
    def _simple_style_consistency(self, response1: str, response2: str) -> float:
        """ç®€åŒ–çš„é£æ ¼ä¸€è‡´æ€§æ£€æŸ¥"""
        if not response1 or not response2:
            return 0.0
        
        # å¥å­æ•°é‡ç›¸ä¼¼åº¦
        sentences1 = len([s for s in response1.split('ã€‚') if s.strip()])
        sentences2 = len([s for s in response2.split('ã€‚') if s.strip()])
        
        sentence_similarity = 1.0 - abs(sentences1 - sentences2) / max(sentences1, sentences2, 1)
        
        # æ ‡ç‚¹ç¬¦å·ä½¿ç”¨ç›¸ä¼¼åº¦
        punct1 = sum(1 for c in response1 if c in 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š')
        punct2 = sum(1 for c in response2 if c in 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š')
        
        punct_similarity = 1.0 - abs(punct1 - punct2) / max(punct1, punct2, 1)
        
        return (sentence_similarity + punct_similarity) / 2
    
    def _calculate_longitudinal_consistency(self, conversation_results: Dict[str, Any], 
                                          memory_results: Dict[str, Any],
                                          evolution_results: Dict[str, Any],
                                          degradation_results: Dict[str, Any]) -> float:
        """è®¡ç®—çºµå‘ä¸€è‡´æ€§æ€»åˆ†"""
        
        # å„ç»„ä»¶æƒé‡
        weights = {
            'conversation': 0.3,
            'memory': 0.25,
            'evolution': 0.25,
            'degradation': 0.2
        }
        
        scores = {}
        
        # 1. å¯¹è¯ä¸€è‡´æ€§åˆ†æ•°
        conv_score = conversation_results.get('cross_phase_consistency', 0.0)
        role_stability = conversation_results.get('role_stability_score', 0.0)
        scores['conversation'] = (conv_score + role_stability) / 2
        
        # 2. è®°å¿†ç®¡ç†åˆ†æ•°
        memory_tests = memory_results.get('memory_tests', {})
        if memory_tests:
            memory_scores = []
            for test_result in memory_tests.values():
                analysis = test_result.get('memory_analysis', {})
                retention = analysis.get('retention_score', 0.0)
                awareness = analysis.get('context_awareness', 0.0)
                memory_scores.append((retention + awareness) / 2)
            scores['memory'] = sum(memory_scores) / len(memory_scores) if memory_scores else 0.0
        else:
            scores['memory'] = 0.0
        
        # 3. æ¼”åŒ–è¿½è¸ªåˆ†æ•°
        consistency_trend = evolution_results.get('consistency_trend', [])
        if consistency_trend:
            # è®¡ç®—è¶‹åŠ¿ç¨³å®šæ€§
            trend_stability = 1.0 - evolution_results.get('role_drift_score', 0.0)
            adaptation_score = evolution_results.get('adaptation_score', 0.0)
            scores['evolution'] = (trend_stability + min(adaptation_score, 0.5) * 2) / 2
        else:
            scores['evolution'] = 0.0
        
        # 4. é€€åŒ–åˆ†æåˆ†æ•°
        degradation_penalty = degradation_results.get('degradation_rate', 0.0)
        recovery_bonus = len(degradation_results.get('recovery_instances', [])) * 0.1
        scores['degradation'] = max(0.0, 1.0 - degradation_penalty + recovery_bonus)
        
        # è®¡ç®—åŠ æƒæ€»åˆ†
        total_score = sum(scores[component] * weights[component] for component in weights.keys())
        
        # ç¡®ä¿åˆ†æ•°åœ¨0-1èŒƒå›´å†…
        return max(0.0, min(1.0, total_score))
    
    def _execute_conversation_turn(self, model_name: str, role_prompt: str, turn: int) -> Dict[str, Any]:
        """æ‰§è¡Œå•è½®å¯¹è¯"""
        # ç”Ÿæˆå¯¹è¯æç¤º
        conversation_prompts = [
            "è¯·ä»‹ç»ä¸€ä¸‹ä½ çš„ä¸“ä¸šèƒŒæ™¯å’Œå·¥ä½œç»éªŒã€‚",
            "ä½ åœ¨å·¥ä½œä¸­é‡åˆ°è¿‡ä»€ä¹ˆæŒ‘æˆ˜ï¼Ÿæ˜¯å¦‚ä½•è§£å†³çš„ï¼Ÿ",
            "ä½ è®¤ä¸ºåœ¨ä½ çš„é¢†åŸŸä¸­æœ€é‡è¦çš„æŠ€èƒ½æ˜¯ä»€ä¹ˆï¼Ÿ",
            "è¯·åˆ†äº«ä¸€ä¸ªä½ æœ€å¼•ä»¥ä¸ºè±ªçš„é¡¹ç›®æˆ–æˆå°±ã€‚",
            "ä½ å¦‚ä½•ä¿æŒè‡ªå·±çš„ä¸“ä¸šçŸ¥è¯†æ›´æ–°ï¼Ÿ",
            "åœ¨å›¢é˜Ÿåˆä½œä¸­ï¼Œä½ é€šå¸¸æ‰®æ¼”ä»€ä¹ˆè§’è‰²ï¼Ÿ",
            "ä½ å¯¹æœªæ¥çš„èŒä¸šå‘å±•æœ‰ä»€ä¹ˆè§„åˆ’ï¼Ÿ",
            "ä½ è®¤ä¸ºè¡Œä¸šå‘å±•çš„è¶‹åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ",
            "è¯·ç»™æ–°äººä¸€äº›å»ºè®®ã€‚",
            "ä½ å¦‚ä½•å¹³è¡¡å·¥ä½œå’Œå­¦ä¹ ï¼Ÿ",
            "åœ¨å‹åŠ›ä¸‹ä½ æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ",
            "ä½ æœ€å–œæ¬¢å·¥ä½œä¸­çš„å“ªä¸ªæ–¹é¢ï¼Ÿ",
            "ä½ è®¤ä¸ºä»€ä¹ˆæ˜¯æˆåŠŸçš„å…³é”®ï¼Ÿ",
            "è¯·æè¿°ä½ çš„å·¥ä½œæ–¹æ³•è®ºã€‚",
            "ä½ å¦‚ä½•å¤„ç†å¤æ‚çš„é—®é¢˜ï¼Ÿ"
        ]
        
        turn_prompt = conversation_prompts[(turn - 1) % len(conversation_prompts)]
        
        turn_result = {
            'turn_number': turn,
            'prompt': turn_prompt,
            'response': '',
            'consistency_score': 0.0,
            'role_adherence': 0.0,
            'memory_retention': 0.0,
            'error': None
        }
        
        try:
            # æ„å»ºå®Œæ•´çš„å¯¹è¯ä¸Šä¸‹æ–‡
            full_prompt = f"{role_prompt}\n\nç”¨æˆ·: {turn_prompt}\nåŠ©æ‰‹: "
            
            # è°ƒç”¨æ¨¡å‹
            if self.model_manager:
                response = self.model_manager.call_model(
                    model_name,
                    full_prompt,
                    max_tokens=800,
                    timeout=180  # 3åˆ†é’Ÿè¶…æ—¶
                )
            else:
                response = f"æ¨¡æ‹Ÿå“åº” - è½®æ¬¡{turn}"
            
            turn_result['response'] = response
            
            # è¯„ä¼°è§’è‰²ä¸€è‡´æ€§
            role_keywords = self._extract_role_keywords(role_prompt)
            role_adherence = self._calculate_role_adherence(response, role_keywords)
            turn_result['role_adherence'] = role_adherence
            
            # è¯„ä¼°è®°å¿†ä¿æŒï¼ˆç®€åŒ–ç‰ˆï¼‰
            turn_result['memory_retention'] = 0.8  # ç®€åŒ–å¤„ç†
            
            # è®¡ç®—ç»¼åˆä¸€è‡´æ€§å¾—åˆ†
            turn_result['consistency_score'] = (
                role_adherence * 0.6 + 
                turn_result['memory_retention'] * 0.4
            )
            
        except Exception as e:
            logger.error(f"å¯¹è¯è½®æ¬¡æ‰§è¡Œå¤±è´¥: {e}")
            turn_result['error'] = str(e)
            turn_result['consistency_score'] = 0.0
        
        return turn_result
    
    def _extract_role_keywords(self, role_prompt: str) -> List[str]:
        """ä»è§’è‰²æç¤ºä¸­æå–å…³é”®è¯"""
        # ç®€å•çš„å…³é”®è¯æå–
        keywords = []
        if 'software_engineer' in role_prompt.lower():
            keywords = ['ä»£ç ', 'ç¼–ç¨‹', 'å¼€å‘', 'æŠ€æœ¯', 'è½¯ä»¶', 'ç³»ç»Ÿ']
        elif 'data_scientist' in role_prompt.lower():
            keywords = ['æ•°æ®', 'åˆ†æ', 'æ¨¡å‹', 'ç®—æ³•', 'ç»Ÿè®¡', 'æœºå™¨å­¦ä¹ ']
        elif 'product_manager' in role_prompt.lower():
            keywords = ['äº§å“', 'éœ€æ±‚', 'ç”¨æˆ·', 'å¸‚åœº', 'ç­–ç•¥', 'è§„åˆ’']
        else:
            keywords = ['ä¸“ä¸š', 'ç»éªŒ', 'æŠ€èƒ½', 'çŸ¥è¯†']
        
        return keywords
    
    def _calculate_role_adherence(self, response: str, role_keywords: List[str]) -> float:
        """è®¡ç®—è§’è‰²åšæŒåº¦"""
        if not response or not role_keywords:
            return 0.0
        
        keyword_count = sum(1 for keyword in role_keywords if keyword in response)
        return min(1.0, keyword_count / len(role_keywords) * 2)
    
    def _calculate_memory_retention(self, response: str, 
                                  conversation_history: List[Dict]) -> float:
        """è®¡ç®—è®°å¿†ä¿æŒåº¦"""
        if not conversation_history:
            return 1.0
        
        # ç®€å•çš„è®°å¿†æ£€æµ‹ï¼šæ£€æŸ¥æ˜¯å¦å¼•ç”¨äº†ä¹‹å‰çš„å¯¹è¯å†…å®¹
        memory_indicators = 0
        total_checks = 0
        
        for prev_turn in conversation_history[-3:]:  # æ£€æŸ¥æœ€è¿‘3è½®
            prev_response = prev_turn.get('response', '')
            if prev_response:
                total_checks += 1
                # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸å…³æ€§
                common_words = set(response.lower().split()) & set(prev_response.lower().split())
                if len(common_words) > 2:  # æœ‰å…±åŒè¯æ±‡
                    memory_indicators += 1
        
        if total_checks == 0:
            return 1.0
        
        return memory_indicators / total_checks

    def _test_memory_retention(self, model_name: str, role_prompt: str, turn: int) -> Dict[str, Any]:
        """æµ‹è¯•è®°å¿†ä¿æŒèƒ½åŠ›"""
        memory_result = {
            'turn': turn,
            'memory_tests': {},
            'overall_retention_score': 0.0,
            'context_awareness_score': 0.0
        }
        
        # è®°å¿†æµ‹è¯•åœºæ™¯
        memory_test_scenarios = {
            'personal_info': {
                'test_prompt': 'è¯·å†æ¬¡ä»‹ç»ä¸€ä¸‹ä½ çš„ä¸“ä¸šèƒŒæ™¯ã€‚',
                'expected_elements': ['ä¸“ä¸š', 'èƒŒæ™¯', 'ç»éªŒ']
            },
            'previous_discussion': {
                'test_prompt': 'åŸºäºæˆ‘ä»¬ä¹‹å‰çš„è®¨è®ºï¼Œä½ æœ‰ä»€ä¹ˆè¡¥å……çš„å—ï¼Ÿ',
                'expected_elements': ['ä¹‹å‰', 'è®¨è®º', 'è¡¥å……']
            },
            'context_reference': {
                'test_prompt': 'ä½ åˆšæ‰æåˆ°çš„é‚£ä¸ªè§‚ç‚¹èƒ½è¯¦ç»†è¯´æ˜ä¸€ä¸‹å—ï¼Ÿ',
                'expected_elements': ['åˆšæ‰', 'æåˆ°', 'è§‚ç‚¹']
            }
        }
        
        for test_name, test_scenario in memory_test_scenarios.items():
            try:
                # æ„å»ºæµ‹è¯•ä¸Šä¸‹æ–‡
                test_prompt = test_scenario['test_prompt']
                full_context = f"{role_prompt}\n\nç”¨æˆ·: {test_prompt}\nåŠ©æ‰‹: "
                
                if self.model_manager:
                    test_response = self.model_manager.call_model(
                        model_name,
                        full_context,
                        max_tokens=400
                    )
                else:
                    test_response = f"è®°å¿†æµ‹è¯•å“åº”: {test_name}"
                
                # åˆ†æè®°å¿†ä¿æŒæƒ…å†µ
                memory_analysis = self._analyze_memory_retention(
                    test_response, [], test_scenario['expected_elements']
                )
                
                memory_result['memory_tests'][test_name] = {
                    'test_prompt': test_prompt,
                    'response': test_response,
                    'memory_analysis': memory_analysis
                }
                
            except Exception as e:
                logger.error(f"è®°å¿†æµ‹è¯•å¤±è´¥: {e}")
                memory_result['memory_tests'][test_name] = {
                    'test_prompt': test_scenario['test_prompt'],
                    'error': str(e),
                    'memory_analysis': {'retention_score': 0.0, 'context_awareness': 0.0}
                }
        
        # è®¡ç®—æ€»ä½“è®°å¿†ä¿æŒåˆ†æ•°
        if memory_result['memory_tests']:
            retention_scores = []
            awareness_scores = []
            
            for test_result in memory_result['memory_tests'].values():
                analysis = test_result.get('memory_analysis', {})
                retention_scores.append(analysis.get('retention_score', 0.0))
                awareness_scores.append(analysis.get('context_awareness', 0.0))
            
            memory_result['overall_retention_score'] = sum(retention_scores) / len(retention_scores)
            memory_result['context_awareness_score'] = sum(awareness_scores) / len(awareness_scores)
        
        return memory_result











