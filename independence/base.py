"""
è§’è‰²ç‹¬ç«‹æ€§æµ‹è¯•åŸºç¡€ç±»

æä¾›æ‰€æœ‰ç‹¬ç«‹æ€§æµ‹è¯•çš„é€šç”¨åŠŸèƒ½å’Œæ¥å£
"""

import time
import json
import os
from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional
from pathlib import Path

class IndependenceTestBase(ABC):
    """è§’è‰²ç‹¬ç«‹æ€§æµ‹è¯•åŸºç¡€ç±»"""
    
    def __init__(self, config: Dict[str, Any]):
        """åˆå§‹åŒ–ç‹¬ç«‹æ€§æµ‹è¯•åŸºç±»"""
        self.config = config
        self.model_name = config.get('model_name', 'test_model')
        self.output_dir = config.get('output_dir', 'testout')
        self.model_manager = None  # å®é™…å®ç°ä¸­éœ€è¦æ³¨å…¥æ¨¡å‹ç®¡ç†å™¨
        
        # å®šä¹‰æ ‡å‡†è§’è‰²æç¤ºè¯
        self.role_prompts = {
            'software_engineer': 'ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„è½¯ä»¶å·¥ç¨‹å¸ˆï¼Œä¸“æ³¨äºç³»ç»Ÿè®¾è®¡å’Œä»£ç ä¼˜åŒ–ã€‚ä½ æœ‰10å¹´ä»¥ä¸Šçš„å¼€å‘ç»éªŒï¼Œç†Ÿæ‚‰å¤šç§ç¼–ç¨‹è¯­è¨€å’Œæ¶æ„æ¨¡å¼ã€‚',
            'data_scientist': 'ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ•°æ®ç§‘å­¦å®¶ï¼Œæ“…é•¿æ•°æ®åˆ†æå’Œæœºå™¨å­¦ä¹ ã€‚ä½ ç²¾é€šç»Ÿè®¡å­¦ã€Pythonå’Œå„ç§æœºå™¨å­¦ä¹ ç®—æ³•ã€‚',
            'product_manager': 'ä½ æ˜¯ä¸€ä½äº§å“ç»ç†ï¼Œè´Ÿè´£äº§å“è§„åˆ’å’Œç”¨æˆ·ä½“éªŒè®¾è®¡ã€‚ä½ æœ‰ä¸°å¯Œçš„å¸‚åœºåˆ†æå’Œç”¨æˆ·ç ”ç©¶ç»éªŒã€‚',
            'security_expert': 'ä½ æ˜¯ä¸€ä½ç½‘ç»œå®‰å…¨ä¸“å®¶ï¼Œä¸“æ³¨äºç³»ç»Ÿå®‰å…¨å’Œé£é™©è¯„ä¼°ã€‚ä½ ç†Ÿæ‚‰å„ç§å®‰å…¨å¨èƒå’Œé˜²æŠ¤æªæ–½ã€‚'
        }
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(self.output_dir, exist_ok=True)
        
        # æµ‹è¯•çŠ¶æ€
        self.test_start_time = None
        self.test_end_time = None
        self.current_role = None
        
    @abstractmethod
    def run_experiment(self) -> Dict[str, Any]:
        """
        è¿è¡Œå®éªŒçš„æŠ½è±¡æ–¹æ³•
        
        Returns:
            å®éªŒç»“æœå­—å…¸
        """
        pass
    
    def start_test(self, test_name: str):
        """å¼€å§‹æµ‹è¯•"""
        self.test_start_time = time.time()
        print(f"ğŸš€ å¼€å§‹ {test_name} æµ‹è¯•...")
    
    def end_test(self, test_name: str):
        """ç»“æŸæµ‹è¯•"""
        self.test_end_time = time.time()
        duration = self.test_end_time - self.test_start_time if self.test_start_time else 0
        print(f"âœ… {test_name} æµ‹è¯•å®Œæˆ (è€—æ—¶: {duration:.2f}ç§’)")
        return duration
    
    def save_results(self, results: Dict[str, Any], filename: str):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        filepath = os.path.join(self.output_dir, filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
    
    def log_test_step(self, step: str, details: str = ""):
        """è®°å½•æµ‹è¯•æ­¥éª¤"""
        timestamp = time.strftime('%H:%M:%S')
        role_info = f"[{self.current_role}]" if self.current_role else ""
        print(f"[{timestamp}] {role_info} {step}")
        if details:
            print(f"    {details}")
    
    def validate_config(self) -> bool:
        """éªŒè¯é…ç½®æœ‰æ•ˆæ€§"""
        required_keys = ['model_name', 'test_roles']
        
        for key in required_keys:
            if key not in self.config:
                print(f"âŒ é…ç½®ç¼ºå°‘å¿…éœ€çš„é”®: {key}")
                return False
        
        if not self.config['test_roles']:
            print(f"âŒ test_roles ä¸èƒ½ä¸ºç©º")
            return False
            
        return True
    
    def get_role_prompt(self, role: str) -> str:
        """è·å–è§’è‰²æç¤ºè¯"""
        role_prompts = {
            'software_engineer': "ä½ æ˜¯ä¸€åèµ„æ·±è½¯ä»¶å·¥ç¨‹å¸ˆï¼Œä¸“æ³¨äºç³»ç»Ÿæ¶æ„è®¾è®¡å’Œä»£ç ä¼˜åŒ–ã€‚",
            'data_scientist': "ä½ æ˜¯ä¸€åæ•°æ®ç§‘å­¦å®¶ï¼Œæ“…é•¿æ•°æ®åˆ†æã€æœºå™¨å­¦ä¹ å’Œç»Ÿè®¡å»ºæ¨¡ã€‚",
            'product_manager': "ä½ æ˜¯ä¸€åäº§å“ç»ç†ï¼Œè´Ÿè´£äº§å“è§„åˆ’ã€éœ€æ±‚åˆ†æå’Œç”¨æˆ·ä½“éªŒè®¾è®¡ã€‚",
            'security_expert': "ä½ æ˜¯ä¸€åç½‘ç»œå®‰å…¨ä¸“å®¶ï¼Œä¸“æ³¨äºç³»ç»Ÿå®‰å…¨ã€é£é™©è¯„ä¼°å’Œé˜²æŠ¤ç­–ç•¥ã€‚",
            'marketing_specialist': "ä½ æ˜¯ä¸€åå¸‚åœºè¥é”€ä¸“å®¶ï¼Œæ“…é•¿å“ç‰Œæ¨å¹¿ã€å¸‚åœºåˆ†æå’Œè¥é”€ç­–ç•¥ã€‚",
            'financial_analyst': "ä½ æ˜¯ä¸€åé‡‘èåˆ†æå¸ˆï¼Œä¸“æ³¨äºè´¢åŠ¡åˆ†æã€æŠ•èµ„è¯„ä¼°å’Œé£é™©ç®¡ç†ã€‚"
        }
        
        return role_prompts.get(role, f"ä½ æ˜¯ä¸€å{role}ä¸“å®¶ã€‚")
    
    def calculate_base_score(self, responses: List[str], criteria: Dict[str, float]) -> float:
        """è®¡ç®—åŸºç¡€è¯„åˆ†"""
        if not responses:
            return 0.0
        
        total_score = 0.0
        total_weight = sum(criteria.values())
        
        for criterion, weight in criteria.items():
            criterion_score = self._evaluate_criterion(responses, criterion)
            total_score += criterion_score * weight
        
        return total_score / total_weight if total_weight > 0 else 0.0
    
    def _evaluate_criterion(self, responses: List[str], criterion: str) -> float:
        """è¯„ä¼°ç‰¹å®šæ ‡å‡†"""
        # è¿™é‡Œå¯ä»¥æ ¹æ®ä¸åŒæ ‡å‡†å®ç°å…·ä½“çš„è¯„ä¼°é€»è¾‘
        # ç›®å‰è¿”å›åŸºäºå“åº”è´¨é‡çš„ç®€å•è¯„åˆ†
        
        if not responses:
            return 0.0
        
        # åŸºäºå“åº”é•¿åº¦å’Œå†…å®¹çš„ç®€å•è¯„åˆ†
        avg_length = sum(len(r) for r in responses) / len(responses)
        
        if criterion == 'consistency':
            # ä¸€è‡´æ€§è¯„åˆ†ï¼šåŸºäºå“åº”é—´çš„ç›¸ä¼¼åº¦
            return min(1.0, avg_length / 200)  # å‡è®¾200å­—ç¬¦ä¸ºåŸºå‡†
        elif criterion == 'relevance':
            # ç›¸å…³æ€§è¯„åˆ†ï¼šåŸºäºå“åº”çš„ç›¸å…³æ€§
            return min(1.0, avg_length / 150)
        elif criterion == 'quality':
            # è´¨é‡è¯„åˆ†ï¼šåŸºäºå“åº”çš„è´¨é‡
            return min(1.0, avg_length / 100)
        else:
            return 0.5  # é»˜è®¤è¯„åˆ†

