#!/usr/bin/env python3
"""
è§’è‰²ç‹¬ç«‹æ€§æµ‹è¯•ç³»ç»Ÿæœ€ç»ˆéªŒè¯è„šæœ¬

æ‰§è¡Œå…¨é¢çš„ç³»ç»ŸéªŒè¯ï¼Œç¡®ä¿æ‰€æœ‰ç»„ä»¶æ­£å¸¸å·¥ä½œ
"""

import sys
import os
from pathlib import Path
import json
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

def final_system_validation():
    """æ‰§è¡Œæœ€ç»ˆç³»ç»ŸéªŒè¯"""
    print("ğŸš€ è§’è‰²ç‹¬ç«‹æ€§æµ‹è¯•ç³»ç»Ÿ - æœ€ç»ˆéªŒè¯")
    print("=" * 60)
    
    validation_results = {
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'tests': {},
        'overall_status': 'UNKNOWN'
    }
    
    # éªŒè¯æ­¥éª¤åˆ—è¡¨
    validation_steps = [
        ("æ¨¡å—å¯¼å…¥éªŒè¯", test_imports),
        ("é…ç½®ç³»ç»ŸéªŒè¯", test_configurations),
        ("æ ¸å¿ƒç»„ä»¶éªŒè¯", test_core_components),
        ("å®éªŒç³»ç»ŸéªŒè¯", test_experiment_systems),
        ("å·¥å…·å‡½æ•°éªŒè¯", test_utility_functions),
        ("é›†æˆæµ‹è¯•éªŒè¯", test_integration),
        ("è¿è¡Œè„šæœ¬éªŒè¯", test_run_scripts)
    ]
    
    passed_tests = 0
    total_tests = len(validation_steps)
    
    for step_name, test_func in validation_steps:
        print(f"\nğŸ“‹ {step_name}...")
        try:
            result = test_func()
            validation_results['tests'][step_name] = {
                'status': 'PASS' if result else 'FAIL',
                'details': 'Test completed successfully' if result else 'Test failed'
            }
            
            if result:
                print(f"âœ… {step_name} - é€šè¿‡")
                passed_tests += 1
            else:
                print(f"âŒ {step_name} - å¤±è´¥")
                
        except Exception as e:
            print(f"ğŸ’¥ {step_name} - å¼‚å¸¸: {e}")
            validation_results['tests'][step_name] = {
                'status': 'ERROR',
                'details': str(e)
            }
    
    # è®¡ç®—æ€»ä½“çŠ¶æ€
    success_rate = passed_tests / total_tests
    if success_rate >= 0.9:
        validation_results['overall_status'] = 'EXCELLENT'
        status_emoji = "ğŸ‰"
        status_desc = "ä¼˜ç§€ - ç³»ç»Ÿå®Œå…¨å°±ç»ª"
    elif success_rate >= 0.7:
        validation_results['overall_status'] = 'GOOD'
        status_emoji = "âœ…"
        status_desc = "è‰¯å¥½ - ç³»ç»ŸåŸºæœ¬å°±ç»ª"
    elif success_rate >= 0.5:
        validation_results['overall_status'] = 'FAIR'
        status_emoji = "âš ï¸"
        status_desc = "ä¸€èˆ¬ - ç³»ç»Ÿéƒ¨åˆ†å°±ç»ª"
    else:
        validation_results['overall_status'] = 'POOR'
        status_emoji = "âŒ"
        status_desc = "è¾ƒå·® - ç³»ç»Ÿæœªå°±ç»ª"
    
    # è¾“å‡ºæœ€ç»ˆç»“æœ
    print(f"\n{'='*60}")
    print(f"ğŸ“Š æœ€ç»ˆéªŒè¯ç»“æœ")
    print(f"{'='*60}")
    print(f"é€šè¿‡æµ‹è¯•: {passed_tests}/{total_tests}")
    print(f"æˆåŠŸç‡: {success_rate:.1%}")
    print(f"ç³»ç»ŸçŠ¶æ€: {status_emoji} {status_desc}")
    
    # ä¿å­˜éªŒè¯ç»“æœ
    try:
        output_file = project_root / 'testout' / 'final_validation_results.json'
        output_file.parent.mkdir(exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(validation_results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ éªŒè¯ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    except Exception as e:
        print(f"âš ï¸  ä¿å­˜éªŒè¯ç»“æœå¤±è´¥: {e}")
    
    return success_rate >= 0.7

def test_imports():
    """æµ‹è¯•æ¨¡å—å¯¼å…¥"""
    try:
        from independence.base import IndependenceTestBase
        from independence.experiments import BreakingStressTest, ImplicitCognitionTest, LongitudinalConsistencyTest
        from independence.utils import call_llm_api, calculate_text_similarity
        from independence.config import get_test_config, ROLE_DEFINITIONS
        from independence.metrics import IndependenceCalculator
        from tests.test_pillar_25_independence import run_independence_test
        return True
    except ImportError:
        return False

def test_configurations():
    """æµ‹è¯•é…ç½®ç³»ç»Ÿ"""
    try:
        from independence.config import get_test_config, validate_config, ROLE_DEFINITIONS
        from config.config import INDEPENDENCE_CONFIG
        
        # æµ‹è¯•é»˜è®¤é…ç½®
        default_config = get_test_config('default')
        if not validate_config(default_config):
            return False
        
        # æµ‹è¯•å¿«é€Ÿé…ç½®
        quick_config = get_test_config('quick')
        if not validate_config(quick_config):
            return False
        
        # æµ‹è¯•è§’è‰²å®šä¹‰
        if len(ROLE_DEFINITIONS) < 4:
            return False
        
        return True
    except Exception:
        return False

def test_core_components():
    """æµ‹è¯•æ ¸å¿ƒç»„ä»¶"""
    try:
        from independence.base import IndependenceTestBase
        from independence.config import get_test_config
        
        # åˆ›å»ºåŸºç¡€æµ‹è¯•å®ä¾‹
        config = get_test_config('quick')
        config['model_name'] = 'test_model'
        config['output_dir'] = 'testout'
        
        base_test = IndependenceTestBase(config)
        
        # éªŒè¯é…ç½®
        if not base_test.validate_config():
            return False
        
        # éªŒè¯åŸºç¡€æ–¹æ³•å­˜åœ¨
        required_methods = ['setup_test', 'run_experiment', 'analyze_results', 'generate_report']
        for method in required_methods:
            if not hasattr(base_test, method):
                return False
        
        return True
    except Exception:
        return False

def test_experiment_systems():
    """æµ‹è¯•å®éªŒç³»ç»Ÿ"""
    try:
        from independence.experiments import BreakingStressTest, ImplicitCognitionTest, LongitudinalConsistencyTest
        from independence.config import get_test_config
        
        config = get_test_config('quick')
        config['model_name'] = 'test_model'
        config['output_dir'] = 'testout'
        
        # åˆ›å»ºå®éªŒå®ä¾‹
        experiments = [
            BreakingStressTest(config),
            ImplicitCognitionTest(config),
            LongitudinalConsistencyTest(config)
        ]
        
        # éªŒè¯æ‰€æœ‰å®éªŒéƒ½èƒ½æ­£ç¡®åˆå§‹åŒ–å’Œé…ç½®éªŒè¯
        for exp in experiments:
            if not exp.validate_config():
                return False
        
        return True
    except Exception:
        return False

def test_utility_functions():
    """æµ‹è¯•å·¥å…·å‡½æ•°"""
    try:
        from independence.utils import (
            calculate_text_similarity,
            get_role_keywords,
            extract_professional_terms,
            analyze_response_style,
            detect_role_leakage,
            evaluate_role_consistency
        )
        
        # æµ‹è¯•æ–‡æœ¬ç›¸ä¼¼åº¦
        sim = calculate_text_similarity("æµ‹è¯•æ–‡æœ¬1", "æµ‹è¯•æ–‡æœ¬2")
        if not (0 <= sim <= 1):
            return False
        
        # æµ‹è¯•è§’è‰²å…³é”®è¯
        keywords = get_role_keywords('software_engineer')
        if not keywords or len(keywords) < 5:
            return False
        
        # æµ‹è¯•ä¸“ä¸šæœ¯è¯­æå–
        terms = extract_professional_terms("è¿™æ˜¯å…³äºæ¶æ„è®¾è®¡çš„è®¨è®º", keywords)
        if terms is None:
            return False
        
        # æµ‹è¯•å“åº”é£æ ¼åˆ†æ
        style = analyze_response_style(["å“åº”1", "å“åº”2"])
        if 'consistency_score' not in style:
            return False
        
        # æµ‹è¯•è§’è‰²æ³„éœ²æ£€æµ‹
        leakage = detect_role_leakage("æµ‹è¯•å“åº”", 'software_engineer', ['data_scientist'])
        if 'leakage_score' not in leakage:
            return False
        
        # æµ‹è¯•è§’è‰²ä¸€è‡´æ€§è¯„ä¼°
        consistency = evaluate_role_consistency(["å“åº”1", "å“åº”2"], 'software_engineer')
        if 'consistency_score' not in consistency:
            return False
        
        return True
    except Exception:
        return False

def test_integration():
    """æµ‹è¯•é›†æˆåŠŸèƒ½"""
    try:
        from tests.test_pillar_25_independence import validate_test_integration
        return validate_test_integration()
    except Exception:
        return False

def test_run_scripts():
    """æµ‹è¯•è¿è¡Œè„šæœ¬"""
    try:
        # æ£€æŸ¥è¿è¡Œè„šæœ¬æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        run_script = project_root / 'run_pillar_25_independence.py'
        if not run_script.exists():
            return False
        
        # å°è¯•å¯¼å…¥è¿è¡Œè„šæœ¬çš„ä¸»è¦å‡½æ•°
        sys.path.insert(0, str(project_root))
        from run_pillar_25_independence import main
        
        return True
    except Exception:
        return False

if __name__ == "__main__":
    success = final_system_validation()
    
    if success:
        print(f"\nğŸ‰ è§’è‰²ç‹¬ç«‹æ€§æµ‹è¯•ç³»ç»ŸéªŒè¯å®Œæˆï¼")
        print(f"âœ… ç³»ç»Ÿå·²å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥å¼€å§‹æµ‹è¯•")
        print(f"\nğŸš€ ä½¿ç”¨æ–¹æ³•:")
        print(f"  python run_pillar_25_independence.py")
        print(f"  python run_pillar_25_independence.py --quick")
        print(f"  python run_pillar_25_independence.py --validate-only")
        sys.exit(0)
    else:
        print(f"\nâŒ ç³»ç»ŸéªŒè¯æœªå®Œå…¨é€šè¿‡")
        print(f"ğŸ”§ è¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶ä¿®å¤é—®é¢˜åé‡æ–°éªŒè¯")
        sys.exit(1)
