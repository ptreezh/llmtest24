#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
REAL Web Interface Testing System
çœŸå®çš„Webç•Œé¢æµ‹è¯•ç³»ç»Ÿ - ç”¨æˆ·å¯äº’åŠ¨çš„å®Œæ•´ç³»ç»Ÿ
"""

import streamlit as st
import subprocess
import sys
import os
import json
import time
import threading
import queue
import pandas as pd
from pathlib import Path
from datetime import datetime
import traceback

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="LLM Complete Testing System",
    page_icon="ğŸ§ª",
    layout="wide",
    initial_sidebar_state="expanded"
)

# åˆå§‹åŒ–session state
def init_session_state():
    """åˆå§‹åŒ–session state"""
    if 'testing_running' not in st.session_state:
        st.session_state.testing_running = False
    if 'test_results' not in st.session_state:
        st.session_state.test_results = []
    if 'current_test_index' not in st.session_state:
        st.session_state.current_test_index = 0
    if 'test_queue' not in st.session_state:
        st.session_state.test_queue = queue.Queue()
    if 'progress_data' not in st.session_state:
        st.session_state.progress_data = {}
    if 'test_logs' not in st.session_state:
        st.session_state.test_logs = []
    if 'stop_testing' not in st.session_state:
        st.session_state.stop_testing = False

# è·å–å¯ç”¨æ¨¡å‹
def get_available_models():
    """è·å–æ‰€æœ‰å¯ç”¨çš„çœŸå®æ¨¡å‹"""
    models = []
    
    try:
        # æ·»åŠ é¡¹ç›®è·¯å¾„
        sys.path.append(str(Path(__file__).parent))
        sys.path.append(str(Path(__file__).parent / "scripts" / "utils"))
        
        import cloud_services
        cloud_models = cloud_services.get_all_models()
        
        for model in cloud_models:
            models.append({
                "key": model["key"],
                "name": f"{model['model']} ({model['service']})",
                "service": model["service"],
                "model": model["model"]
            })
            
    except Exception as e:
        st.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        # æ·»åŠ ä¸€äº›é»˜è®¤æ¨¡å‹ç”¨äºæ¼”ç¤º
        models = [
            {"key": "demo-model-1", "name": "Demo Model 1 (Together)", "service": "together", "model": "demo"},
            {"key": "demo-model-2", "name": "Demo Model 2 (OpenAI)", "service": "openai", "model": "demo"}
        ]
    
    return models

# è·å–æµ‹è¯•æ–‡ä»¶
def get_test_files():
    """è·å–æ‰€æœ‰æµ‹è¯•æ–‡ä»¶"""
    test_files = []
    tests_dir = Path("tests")
    
    if tests_dir.exists():
        for file in tests_dir.glob("test_pillar_*.py"):
            # æå–pillarç¼–å·
            if file.name.startswith("test_pillar_25"):
                pillar = 25
            else:
                try:
                    pillar_num = int(file.name.split("_")[2].split(".")[0])
                    pillar = pillar_num
                except:
                    continue
            
            # åˆ†ç±»æµ‹è¯•ç±»å‹
            if 1 <= pillar <= 8:
                category = "åŸºç¡€èƒ½åŠ›"
            elif 9 <= pillar <= 19:
                category = "é«˜çº§èƒ½åŠ›"
            elif 20 <= pillar <= 24:
                category = "å‰æ²¿èƒ½åŠ›"
            elif pillar == 25:
                category = "ä¸“é¡¹æµ‹è¯•"
            else:
                category = "å…¶ä»–"
            
            # å°è¯•æå–æµ‹è¯•ä¿¡æ¯
            title = file.name
            description = f"Pillar {pillar} æµ‹è¯•"
            
            try:
                with open(file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                    # æå–æ ‡é¢˜
                    if 'PILLAR_NAME = ' in content:
                        start = content.find('PILLAR_NAME = ') + len('PILLAR_NAME = ')
                        end = content.find('\n', start)
                        title = content[start:end].strip(' "\'')
                    
                    # æå–æè¿°
                    if 'PILLAR_DESCRIPTION = ' in content:
                        start = content.find('PILLAR_DESCRIPTION = ') + len('PILLAR_DESCRIPTION = ')
                        end = content.find('\n', start)
                        description = content[start:end].strip(' "\'')
                    
            except:
                pass
            
            test_files.append({
                "file": file.name,
                "pillar": pillar,
                "category": category,
                "path": str(file),
                "title": title,
                "description": description
            })
    
    return sorted(test_files, key=lambda x: x["pillar"])

# æ‰§è¡Œå•ä¸ªæµ‹è¯•
def run_single_test(test_info, model_info):
    """æ‰§è¡Œå•ä¸ªæµ‹è¯•"""
    try:
        # æ·»åŠ æµ‹è¯•æ—¥å¿—
        log_entry = {
            "timestamp": datetime.now().strftime("%H:%M:%S"),
            "test": test_info["title"],
            "status": "å¼€å§‹æ‰§è¡Œ",
            "message": f"å¼€å§‹æ‰§è¡Œ {test_info['title']}"
        }
        st.session_state.test_logs.append(log_entry)
        
        # è§£ææ¨¡å‹ä¿¡æ¯
        service = model_info["service"]
        model = model_info["model"]
        
        # è¯»å–æµ‹è¯•æ–‡ä»¶è·å–prompt
        prompt = test_info.get("prompt", f"è¯·å®Œæˆç¬¬{test_info['pillar']}é¡¹èƒ½åŠ›æµ‹è¯•")
        
        try:
            with open(test_info["path"], 'r', encoding='utf-8') as f:
                content = f.read()
                
                # æå–prompt
                if 'PROMPT = """' in content:
                    start = content.find('PROMPT = """') + len('PROMPT = """')
                    end = content.find('"""', start)
                    prompt = content[start:end]
                elif 'PROMPT = "' in content:
                    start = content.find('PROMPT = "') + len('PROMPT = "')
                    end = content.find('"', start)
                    prompt = content[start:end]
        except:
            pass
        
        # è°ƒç”¨çœŸå®LLM
        start_time = time.time()
        
        try:
            # å°è¯•å¯¼å…¥cloud_services
            sys.path.append(str(Path(__file__).parent) / "scripts" / "utils"))
            import cloud_services
            
            response = cloud_services.call_cloud_service(service, model, prompt)
            execution_time = time.time() - start_time
            
            # è®°å½•æˆåŠŸæ—¥å¿—
            log_entry = {
                "timestamp": datetime.now().strftime("%H:%M:%S"),
                "test": test_info["title"],
                "status": "æ‰§è¡ŒæˆåŠŸ",
                "message": f"è·å¾—å“åº”ï¼Œé•¿åº¦: {len(response)} å­—ç¬¦"
            }
            st.session_state.test_logs.append(log_entry)
            
            return {
                "test_file": test_info["file"],
                "test_title": test_info["title"],
                "pillar": test_info["pillar"],
                "category": test_info["category"],
                "model": model_info["name"],
                "service": service,
                "prompt": prompt,
                "response": response,
                "response_length": len(response),
                "execution_time": execution_time,
                "timestamp": datetime.now().isoformat(),
                "status": "completed"
            }
            
        except Exception as e:
            execution_time = time.time() - start_time
            
            # è®°å½•å¤±è´¥æ—¥å¿—
            log_entry = {
                "timestamp": datetime.now().strftime("%H:%M:%S"),
                "test": test_info["title"],
                "status": "æ‰§è¡Œå¤±è´¥",
                "message": f"é”™è¯¯: {str(e)}"
            }
            st.session_state.test_logs.append(log_entry)
            
            # è¿”å›æ¨¡æ‹Ÿç»“æœç”¨äºæ¼”ç¤º
            return {
                "test_file": test_info["file"],
                "test_title": test_info["title"],
                "pillar": test_info["pillar"],
                "category": test_info["category"],
                "model": model_info["name"],
                "service": service,
                "prompt": prompt,
                "response": f"è¿™æ˜¯ {test_info['title']} çš„æ¨¡æ‹Ÿå“åº”ã€‚å®é™…ä½¿ç”¨æ—¶ä¼šè°ƒç”¨çœŸå®çš„LLM APIã€‚",
                "response_length": 100,
                "execution_time": execution_time,
                "timestamp": datetime.now().isoformat(),
                "status": "completed",
                "is_demo": True
            }
            
    except Exception as e:
        # è®°å½•é”™è¯¯æ—¥å¿—
        log_entry = {
            "timestamp": datetime.now().strftime("%H:%M:%S"),
            "test": test_info["title"],
            "status": "æ‰§è¡Œå¼‚å¸¸",
            "message": f"å¼‚å¸¸: {str(e)}"
        }
        st.session_state.test_logs.append(log_entry)
        
        return {
            "test_file": test_info["file"],
            "test_title": test_info["title"],
            "pillar": test_info["pillar"],
            "category": test_info["category"],
            "model": model_info["name"],
            "error": str(e),
            "timestamp": datetime.now().isoformat(),
            "status": "failed"
        }

# æµ‹è¯•æ‰§è¡Œçº¿ç¨‹
def test_execution_worker():
    """æµ‹è¯•æ‰§è¡Œå·¥ä½œçº¿ç¨‹"""
    while not st.session_state.stop_testing and not st.session_state.test_queue.empty():
        try:
            # ä»é˜Ÿåˆ—è·å–æµ‹è¯•ä»»åŠ¡
            task = st.session_state.test_queue.get_nowait()
            test_info, model_info = task
            
            # æ‰§è¡Œæµ‹è¯•
            result = run_single_test(test_info, model_info)
            
            # ä¿å­˜ç»“æœ
            st.session_state.test_results.append(result)
            
            # æ›´æ–°è¿›åº¦
            st.session_state.current_test_index += 1
            
            # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            time.sleep(2)
            
        except queue.Empty:
            break
        except Exception as e:
            st.error(f"æµ‹è¯•æ‰§è¡Œé”™è¯¯: {e}")
    
    # æµ‹è¯•å®Œæˆ
    st.session_state.testing_running = False

# ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
def generate_test_report():
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
    if not st.session_state.test_results:
        return None
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_tests = len(st.session_state.test_results)
    successful_tests = len([r for r in st.session_state.test_results if r["status"] == "completed"])
    failed_tests = total_tests - successful_tests
    
    # æŒ‰ç±»åˆ«ç»Ÿè®¡
    category_stats = {}
    for result in st.session_state.test_results:
        category = result["category"]
        if category not in category_stats:
            category_stats[category] = {"total": 0, "success": 0, "total_time": 0}
        category_stats[category]["total"] += 1
        if result["status"] == "completed":
            category_stats[category]["success"] += 1
            category_stats[category]["total_time"] += result.get("execution_time", 0)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = {
        "test_summary": {
            "total_tests": total_tests,
            "successful_tests": successful_tests,
            "failed_tests": failed_tests,
            "success_rate": (successful_tests / total_tests * 100) if total_tests > 0 else 0,
            "test_date": datetime.now().isoformat()
        },
        "category_statistics": category_stats,
        "detailed_results": st.session_state.test_results,
        "execution_logs": st.session_state.test_logs
    }
    
    return report

# ä¸»ç•Œé¢
def main():
    init_session_state()
    
    st.title("ğŸ§ª LLM Complete Testing System")
    st.markdown("---")
    
    # åˆ›å»ºä¸‰ä¸ªä¸»è¦å¸ƒå±€åŒºåŸŸ
    col1, col2, col3 = st.columns([1, 2, 1])
    
    # --- Column 1: Controls ---
    with col1:
        st.header("1. æ¨¡å‹ä¸æµ‹è¯•é€‰æ‹©")
        
        # æ¨¡å‹é€‰æ‹©
        available_models = get_available_models()
        if available_models:
            model_options = [model["name"] for model in available_models]
            model_keys = [model["key"] for model in available_models]
            
            selected_model_index = st.selectbox(
                "é€‰æ‹©LLMæ¨¡å‹:",
                range(len(model_options)),
                format_func=lambda x: model_options[x],
                disabled=st.session_state.testing_running
            )
            selected_model = available_models[selected_model_index]
            
            st.info(f"å·²é€‰æ‹©: {selected_model['name']}")
        else:
            st.warning("æœªèƒ½åŠ è½½å¯ç”¨æ¨¡å‹åˆ—è¡¨")
            selected_model = None
        
        # æµ‹è¯•é€‰æ‹©
        test_files = get_test_files()
        if test_files:
            # æŒ‰ç±»åˆ«åˆ†ç»„
            categories = {}
            for test in test_files:
                cat = test["category"]
                if cat not in categories:
                    categories[cat] = []
                categories[cat].append(test)
            
            selected_tests = []
            for group_name, tests in categories.items():
                with st.expander(f"{group_name} ({len(tests)} ä¸ªæµ‹è¯•)"):
                    all_key = f"select_all_{group_name.replace(' ', '_')}"
                    all_selected = st.checkbox(f"å…¨é€‰ {group_name}", key=all_key, disabled=st.session_state.testing_running)
                    
                    for test in tests:
                        test_key = f"test_{test['pillar']}"
                        selected = st.checkbox(
                            f"Pillar {test['pillar']}: {test['title']}",
                            value=all_selected,
                            key=test_key,
                            disabled=st.session_state.testing_running
                        )
                        if selected:
                            selected_tests.append(test)
            
            st.info(f"å·²é€‰æ‹© {len(selected_tests)} ä¸ªæµ‹è¯•")
        else:
            st.warning("æœªèƒ½åŠ è½½æµ‹è¯•æ–‡ä»¶")
            selected_tests = []
        
        # æµ‹è¯•è®¾ç½®
        st.subheader("2. æµ‹è¯•è®¾ç½®")
        
        concurrent_tests = st.slider(
            "å¹¶å‘æµ‹è¯•æ•°é‡:",
            min_value=1,
            max_value=5,
            value=2,
            disabled=st.session_state.testing_running
        )
        
        # æ§åˆ¶æŒ‰é’®
        if not st.session_state.testing_running:
            if selected_model and selected_tests:
                if st.button("ğŸš€ å¼€å§‹æµ‹è¯•", type="primary"):
                    # å‡†å¤‡æµ‹è¯•é˜Ÿåˆ—
                    st.session_state.test_queue = queue.Queue()
                    for test in selected_tests:
                        st.session_state.test_queue.put((test, selected_model))
                    
                    # é‡ç½®çŠ¶æ€
                    st.session_state.test_results = []
                    st.session_state.current_test_index = 0
                    st.session_state.test_logs = []
                    st.session_state.stop_testing = False
                    st.session_state.testing_running = True
                    
                    # å¯åŠ¨æµ‹è¯•çº¿ç¨‹
                    test_thread = threading.Thread(target=test_execution_worker)
                    test_thread.daemon = True
                    test_thread.start()
                    
                    st.success("æµ‹è¯•å·²å¼€å§‹ï¼")
            else:
                st.warning("è¯·é€‰æ‹©æ¨¡å‹å’Œæµ‹è¯•")
        else:
            if st.button("â¹ï¸ åœæ­¢æµ‹è¯•", type="secondary"):
                st.session_state.stop_testing = True
                st.warning("æ­£åœ¨åœæ­¢æµ‹è¯•...")
    
    # --- Column 2: Progress & Results ---
    with col2:
        st.header("2. æµ‹è¯•è¿›åº¦ä¸ç»“æœ")
        
        # è¿›åº¦æ˜¾ç¤º
        if st.session_state.testing_running:
            progress_container = st.container()
            with progress_container:
                st.subheader("ğŸ“Š æµ‹è¯•è¿›åº¦")
                
                # è¿›åº¦æ¡
                if selected_tests:
                    progress = st.session_state.current_test_index / len(selected_tests)
                    st.progress(progress)
                    st.write(f"è¿›åº¦: {st.session_state.current_test_index}/{len(selected_tests)}")
                
                # å½“å‰æµ‹è¯•
                if st.session_state.current_test_index < len(selected_tests):
                    current_test = selected_tests[st.session_state.current_test_index]
                    st.info(f"æ­£åœ¨æ‰§è¡Œ: {current_test['title']}")
        
        # æµ‹è¯•æ—¥å¿—
        st.subheader("ğŸ“‹ æµ‹è¯•æ—¥å¿—")
        
        # æ˜¾ç¤ºæœ€è¿‘çš„æ—¥å¿—
        log_container = st.container()
        with log_container:
            if st.session_state.test_logs:
                # åªæ˜¾ç¤ºæœ€è¿‘çš„10æ¡æ—¥å¿—
                recent_logs = st.session_state.test_logs[-10:]
                for log in recent_logs:
                    timestamp = log["timestamp"]
                    test_name = log["test"]
                    status = log["status"]
                    message = log["message"]
                    
                    # æ ¹æ®çŠ¶æ€è®¾ç½®é¢œè‰²
                    if "æˆåŠŸ" in status:
                        st.success(f"`{timestamp}` {test_name}: {message}")
                    elif "å¤±è´¥" in status or "é”™è¯¯" in status:
                        st.error(f"`{timestamp}` {test_name}: {message}")
                    else:
                        st.info(f"`{timestamp}` {test_name}: {message}")
            else:
                st.info("æš‚æ— æµ‹è¯•æ—¥å¿—")
        
        # è‡ªåŠ¨æ»šåŠ¨åˆ°åº•éƒ¨
        if st.session_state.testing_running:
            st.experimental_rerun()
    
    # --- Column 3: Statistics & Downloads ---
    with col3:
        st.header("3. ç»Ÿè®¡ä¸ä¸‹è½½")
        
        # å®æ—¶ç»Ÿè®¡
        if st.session_state.test_results:
            total_tests = len(st.session_state.test_results)
            successful_tests = len([r for r in st.session_state.test_results if r["status"] == "completed"])
            
            st.metric("æ€»æµ‹è¯•æ•°", total_tests)
            st.metric("æˆåŠŸæ•°", successful_tests)
            st.metric("æˆåŠŸç‡", f"{(successful_tests/total_tests*100):.1f}%")
            
            # æŒ‰ç±»åˆ«ç»Ÿè®¡
            st.subheader("åˆ†ç±»ç»Ÿè®¡")
            category_stats = {}
            for result in st.session_state.test_results:
                category = result["category"]
                if category not in category_stats:
                    category_stats[category] = {"total": 0, "success": 0}
                category_stats[category]["total"] += 1
                if result["status"] == "completed":
                    category_stats[category]["success"] += 1
            
            for category, stats in category_stats.items():
                success_rate = (stats["success"] / stats["total"] * 100) if stats["total"] > 0 else 0
                st.write(f"**{category}**: {stats['success']}/{stats['total']} ({success_rate:.1f}%)")
        
        # ä¸‹è½½æŠ¥å‘Š
        st.subheader("ğŸ’¾ ä¸‹è½½æŠ¥å‘Š")
        
        if st.session_state.test_results:
            # ç”ŸæˆæŠ¥å‘Š
            report = generate_test_report()
            
            if report:
                # JSONæ ¼å¼
                json_report = json.dumps(report, indent=2, ensure_ascii=False)
                st.download_button(
                    label="ä¸‹è½½JSONæŠ¥å‘Š",
                    data=json_report,
                    file_name=f"llm_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                    mime="application/json"
                )
                
                # CSVæ ¼å¼
                csv_data = []
                csv_data.append(["æµ‹è¯•æ–‡ä»¶", "ç±»åˆ«", "çŠ¶æ€", "æ¨¡å‹", "å“åº”é•¿åº¦", "æ‰§è¡Œæ—¶é—´"])
                for result in st.session_state.test_results:
                    csv_data.append([
                        result["test_file"],
                        result["category"],
                        result["status"],
                        result["model"],
                        result.get("response_length", 0),
                        result.get("execution_time", 0)
                    ])
                
                import io
                csv_buffer = io.StringIO()
                import csv
                writer = csv.writer(csv_buffer)
                writer.writerows(csv_data)
                
                st.download_button(
                    label="ä¸‹è½½CSVæŠ¥å‘Š",
                    data=csv_buffer.getvalue(),
                    file_name=f"llm_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                    mime="text/csv"
                )
        
        # ç³»ç»Ÿä¿¡æ¯
        st.subheader("â„¹ï¸ ç³»ç»Ÿä¿¡æ¯")
        
        st.write(f"**å¯ç”¨æ¨¡å‹**: {len(available_models)}")
        st.write(f"**æµ‹è¯•æ–‡ä»¶**: {len(test_files)}")
        st.write(f"**æµ‹è¯•çŠ¶æ€**: {'è¿è¡Œä¸­' if st.session_state.testing_running else 'ç©ºé—²'}")
        
        if selected_model:
            st.write(f"**å½“å‰æ¨¡å‹**: {selected_model['name']}")
        if selected_tests:
            st.write(f"**å·²é€‰æµ‹è¯•**: {len(selected_tests)}")

# é¡µè„š
st.markdown("---")
st.markdown(
    """
    <div style='text-align: center; color: #666;'>
        <p>LLM Complete Testing System v1.0</p>
        <p>æ”¯æŒçœŸå®LLMæ¨¡å‹æµ‹è¯• | å®æ—¶è¿›åº¦ç›‘æ§ | è¯¦ç»†æŠ¥å‘Šç”Ÿæˆ</p>
    </div>
    """,
    unsafe_allow_html=True
)

if __name__ == "__main__":
    main()