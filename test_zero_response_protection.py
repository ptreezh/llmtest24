#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•é›¶å“åº”ä¿æŠ¤æœºåˆ¶
éªŒè¯å¼ºåŒ–çš„é‡è¯•ç­–ç•¥å’Œå¤‡ç”¨æç¤ºè¯
"""

import requests
import time

# é…ç½®
OLLAMA_API_URL = 'http://localhost:11434/api/chat'
ATLAS_MODEL = 'atlas/intersync-gemma-7b-instruct-function-calling:latest'
API_TIMEOUT = 300

def call_ollama_with_protection(model: str, prompt: str, max_retries: int = 10) -> str:
    """
    å¸¦å¼ºåŒ–ä¿æŠ¤çš„Ollamaè°ƒç”¨
    """
    print(f"    - Testing {model} with prompt: '{prompt[:50]}...' ({len(prompt)} chars)")
    
    system_prompt = "Detective. Analyze murder case. Summarize key evidence concisely."
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": prompt}
    ]
    
    for attempt in range(max_retries):
        # æ¸è¿›å¼å‚æ•°è°ƒæ•´ç­–ç•¥
        if attempt <= 2:
            # å‰3æ¬¡å°è¯•ï¼šæ ‡å‡†å‚æ•°
            temp = 0.6 + (attempt * 0.2)
            top_p = 0.95
            top_k = 60
        elif attempt <= 5:
            # ç¬¬4-6æ¬¡ï¼šæé«˜éšæœºæ€§
            temp = 0.9 + (attempt * 0.1)
            top_p = 0.98
            top_k = 80
        else:
            # ç¬¬7-10æ¬¡ï¼šæœ€å¤§éšæœºæ€§
            temp = 1.2 + (attempt * 0.1)
            top_p = 1.0
            top_k = 100
        
        options = {
            "temperature": min(temp, 2.0),
            "top_p": top_p,
            "top_k": top_k,
            "repeat_penalty": max(1.0, 1.05 - (attempt * 0.01)),
            "timeout": 40,
            "num_ctx": max(1024, 2048 - (attempt * 100)),
            "num_predict": 100 + (attempt * 10),
            "seed": -1,
            "mirostat": 2 if attempt > 3 else 0,
            "mirostat_tau": 5.0 if attempt > 3 else 5.0
        }
        
        payload = {
            "model": model,
            "messages": messages,
            "stream": False,
            "options": options
        }
        
        try:
            response = requests.post(OLLAMA_API_URL, json=payload, timeout=API_TIMEOUT)
            response.raise_for_status()
            data = response.json()
            content = data.get('message', {}).get('content', '')
            
            if content and content.strip():
                if attempt > 0:
                    print(f"    âœ… Success on attempt {attempt + 1}: {len(content)} chars")
                else:
                    print(f"    âœ… Success: {len(content)} chars")
                return content
            else:
                print(f"    âš ï¸ Zero response on attempt {attempt + 1}/{max_retries} (temp: {temp:.2f})")
                if attempt < max_retries - 1:
                    time.sleep(1)
                    continue
                    
        except Exception as e:
            print(f"    âŒ Error on attempt {attempt + 1}/{max_retries}: {str(e)[:50]}...")
            if attempt < max_retries - 1:
                time.sleep(1)
                continue
    
    print(f"    âŒ All {max_retries} attempts failed")
    return ""

def test_fallback_prompts(model: str, original_prompt: str, fallback_prompt: str, default_response: str) -> str:
    """æµ‹è¯•å¤‡ç”¨æç¤ºè¯æœºåˆ¶"""
    print(f"\nğŸ”„ Testing fallback mechanism...")
    
    # å°è¯•åŸå§‹æç¤ºè¯
    result = call_ollama_with_protection(model, original_prompt)
    
    if not result or result.strip() == "":
        print(f"    ğŸ”„ Original failed, trying fallback...")
        result = call_ollama_with_protection(model, fallback_prompt)
        
        if not result or result.strip() == "":
            print(f"    ğŸ†˜ Fallback failed, using default...")
            result = default_response
            print(f"    ğŸ“ Default response: {result}")
    
    return result

def test_zero_response_protection():
    """æµ‹è¯•é›¶å“åº”ä¿æŠ¤æœºåˆ¶"""
    print("ğŸ§ª Testing Zero Response Protection Mechanism")
    print("="*55)
    
    model = ATLAS_MODEL
    
    # æµ‹è¯•åœºæ™¯1ï¼šåˆå§‹æ‘˜è¦
    print(f"\n--- åœºæ™¯1ï¼šåˆå§‹æ‘˜è¦ä¿æŠ¤ ---")
    original_prompt1 = "S:Aï¼šæ˜¨æ™šå¬åˆ°å¥‡æ€ªå£°éŸ³ã€‚Bï¼šçœ‹åˆ°é»‘å½±ã€‚Cï¼šå¼ ä¸‰ä¸¢æ–§å¤´ã€‚Dï¼šè„šå°å¾ˆå¤§ã€‚"
    fallback_prompt1 = "Sum:Aå¬éŸ³ï¼ŒBçœ‹é»‘å½±ï¼ŒCè¯´å¼ ä¸‰ä¸¢æ–§å¤´ï¼ŒDè„šå°å¤§"
    default_response1 = "Evidence found, investigation continues."
    
    result1 = test_fallback_prompts(model, original_prompt1, fallback_prompt1, default_response1)
    print(f"âœ… åœºæ™¯1ç»“æœ: '{result1[:50]}...' ({len(result1)} chars)")
    
    # æµ‹è¯•åœºæ™¯2ï¼šæ›´æ–°æ‘˜è¦
    print(f"\n--- åœºæ™¯2ï¼šæ›´æ–°æ‘˜è¦ä¿æŠ¤ ---")
    original_prompt2 = "E:æ˜¨æ™šå¬åˆ°å£°éŸ³ï¼Œçœ‹åˆ°é»‘å½± N:æå››æ·±å¤œå¤–å‡ºï¼Œèº«é«˜ä¸€ç±³å…« U:"
    fallback_prompt2 = "Update:Eï¼šå¬éŸ³çœ‹å½±ï¼ŒNï¼šæå››å¯ç–‘é«˜1.8"
    default_response2 = "Previous evidence [continued]"
    
    result2 = test_fallback_prompts(model, original_prompt2, fallback_prompt2, default_response2)
    print(f"âœ… åœºæ™¯2ç»“æœ: '{result2[:50]}...' ({len(result2)} chars)")
    
    # æµ‹è¯•åœºæ™¯3ï¼šæœ€ç»ˆæ¨ç†
    print(f"\n--- åœºæ™¯3ï¼šæœ€ç»ˆæ¨ç†ä¿æŠ¤ ---")
    original_prompt3 = "E:æ˜¨æ™šå¬åˆ°å£°éŸ³ï¼Œçœ‹åˆ°é»‘å½±ï¼Œæå››æ·±å¤œå¤–å‡ºï¼Œèº«é«˜ä¸€ç±³å…«ï¼Œæœ‰çŸ›ç›¾ K?"
    fallback_prompt3 = "Who killed? æå››å¤œå‡ºï¼Œé«˜1.8Mï¼Œæœ‰çŸ›ç›¾"
    default_response3 = "Based evidence: æå››æ·±å¤œå¤–å‡ºï¼Œèº«é«˜ä¸€ç±³å…«ï¼Œæœ‰çŸ›ç›¾, further investigation needed to determine the killer."
    
    result3 = test_fallback_prompts(model, original_prompt3, fallback_prompt3, default_response3)
    print(f"âœ… åœºæ™¯3ç»“æœ: '{result3[:50]}...' ({len(result3)} chars)")
    
    # æ±‡æ€»æŠ¥å‘Š
    print(f"\n" + "="*55)
    print(f"ğŸ“‹ Zero Response Protection Test Summary")
    print(f"="*55)
    
    success_count = sum([
        1 if result1 and result1.strip() else 0,
        1 if result2 and result2.strip() else 0,
        1 if result3 and result3.strip() else 0
    ])
    
    print(f"Protection mechanisms:")
    print(f"  1. æ¸è¿›å¼å‚æ•°è°ƒæ•´ (10æ¬¡é‡è¯•)")
    print(f"  2. å¤‡ç”¨ç®€åŒ–æç¤ºè¯")
    print(f"  3. é»˜è®¤å“åº”ä¿åº•")
    print(f"")
    print(f"Test results:")
    print(f"  åœºæ™¯1 (åˆå§‹æ‘˜è¦): {'âœ… SUCCESS' if result1 and result1.strip() else 'âŒ FAILED'}")
    print(f"  åœºæ™¯2 (æ›´æ–°æ‘˜è¦): {'âœ… SUCCESS' if result2 and result2.strip() else 'âŒ FAILED'}")
    print(f"  åœºæ™¯3 (æœ€ç»ˆæ¨ç†): {'âœ… SUCCESS' if result3 and result3.strip() else 'âŒ FAILED'}")
    print(f"")
    print(f"Overall success rate: {success_count}/3 ({success_count/3*100:.1f}%)")
    
    if success_count == 3:
        print(f"ğŸ¯ Zero response protection successful!")
        print(f"âœ… æ‰€æœ‰åœºæ™¯éƒ½è·å¾—äº†å“åº”")
        print(f"âœ… å¤šå±‚ä¿æŠ¤æœºåˆ¶æœ‰æ•ˆ")
        print(f"âœ… ç¡®ä¿æµ‹è¯•æµç¨‹ä¸ä¸­æ–­")
    else:
        print(f"âš ï¸ Some scenarios still failed, need stronger protection")
    
    # ä¿å­˜ç»“æœ
    with open('zero_response_protection_test.txt', 'w', encoding='utf-8') as f:
        f.write("Zero Response Protection Test Results\n")
        f.write("="*40 + "\n\n")
        f.write(f"Model: {model}\n")
        f.write(f"Success rate: {success_count}/3 ({success_count/3*100:.1f}%)\n\n")
        f.write("Results:\n")
        f.write(f"åœºæ™¯1: {result1}\n\n")
        f.write(f"åœºæ™¯2: {result2}\n\n")
        f.write(f"åœºæ™¯3: {result3}\n\n")
    
    print(f"ğŸ’¾ Test results saved to: zero_response_protection_test.txt")

if __name__ == "__main__":
    # æ£€æŸ¥æ¨¡å‹å¯ç”¨æ€§
    print(f"ğŸ” Checking model availability: {ATLAS_MODEL}")
    
    try:
        test_response = call_ollama_with_protection(ATLAS_MODEL, "Hello", max_retries=3)
        if test_response:
            print(f"âœ… Model {ATLAS_MODEL} is available")
            test_zero_response_protection()
        else:
            print(f"âŒ Model {ATLAS_MODEL} is not responding")
    except Exception as e:
        print(f"âŒ Failed to connect: {e}")
