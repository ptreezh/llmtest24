#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•atlasæ¨¡å‹çš„æç®€æç¤ºè¯é…ç½®
éªŒè¯æ˜¯å¦èƒ½å‡å°‘é›¶å“åº”é—®é¢˜
"""

def get_prompt(prompt_type: str, context: dict = {}, model: str = "") -> str:
    """å¤åˆ¶TestLLM.pyä¸­çš„æç®€get_promptå‡½æ•°"""
    # é’ˆå¯¹atlas/intersync-gemmaæ¨¡å‹çš„æç®€æç¤ºè¯ï¼ˆæ€»é•¿åº¦<300å­—ç¬¦ï¼‰
    # ç³»ç»Ÿæç¤ºè¯(65) + ç”¨æˆ·æç¤ºè¯(<235) = 300å­—ç¬¦ï¼Œæ€»ç»“å†…å®¹ä¸¥æ ¼é™åˆ¶åœ¨150å­—ç¬¦
    if "atlas/intersync-gemma" in model:
        if prompt_type == "intermediate":
            if context.get('summary_so_far', '').strip() and context.get('summary_so_far', '').strip() != 'None':
                # æç®€æ›´æ–°æç¤ºè¯ï¼Œæ€»è®¡<235å­—ç¬¦ï¼ˆç³»ç»Ÿæç¤ºè¯65å­—ç¬¦ï¼‰
                summary = context['summary_so_far'][:120]  # 120å­—ç¬¦æ¦‚æ‹¬
                new_content = context['new_dialogue_chunk'][:80]  # 80å­—ç¬¦æ–°å†…å®¹
                return f"Evidence:{summary} New:{new_content} Update:"
            else:
                # æç®€åˆå§‹æç¤ºè¯
                content = context['new_dialogue_chunk'][:200]  # 200å­—ç¬¦å†…å®¹
                return f"Summarize:{content}"
        elif prompt_type == "final":
            # æç®€æœ€ç»ˆæ¨ç†
            facts = context.get('summary_so_far', '')[:200]  # 200å­—ç¬¦äº‹å®
            return f"Evidence:{facts} Killer?"
    
    return "Standard prompt for other models"

def test_minimal_prompts():
    """æµ‹è¯•æç®€æç¤ºè¯é…ç½®"""
    print("ğŸ§ª Testing Atlas Model Minimal Prompts")
    print("="*50)
    
    model = "atlas/intersync-gemma-7b-instruct-function-calling:latest"
    system_prompt = "Detective. Analyze murder case. Summarize key evidence concisely."
    
    print(f"System prompt: '{system_prompt}' ({len(system_prompt)} chars)")
    
    # æµ‹è¯•åœºæ™¯1ï¼šåˆå§‹æ‘˜è¦
    test_dialogue = "Aï¼šæ˜¨æ™šæˆ‘å¬åˆ°äº†å¥‡æ€ªçš„å£°éŸ³ï¼Œå¥½åƒæ˜¯ä»æ£®æ—é‚£è¾¹ä¼ æ¥çš„ï¼Œå£°éŸ³å¾ˆè§„å¾‹ï¼Œä¸åƒæ˜¯é‡å…½å‘å‡ºçš„ã€‚Bï¼šæˆ‘ä¹Ÿå¬åˆ°äº†ï¼Œè€Œä¸”æˆ‘è¿˜çœ‹åˆ°äº†ä¸€ä¸ªé»‘å½±åœ¨è€æ©¡æ ‘é™„è¿‘ç§»åŠ¨ï¼Œé‚£ä¸ªäººåŠ¨ä½œå¾ˆå¿«ã€‚Cï¼šè¿™ç¡®å®å¾ˆå¥‡æ€ªï¼Œä¼šä¸ä¼šæ˜¯æœ‰äººåœ¨é‚£é‡Œåšä»€ä¹ˆåäº‹ï¼Ÿæ˜¨æ™šé‚£ä¹ˆæ™šäº†è¿˜æœ‰äººåœ¨å¤–é¢ç¡®å®ä¸æ­£å¸¸ã€‚Dï¼šæˆ‘è§‰å¾—å¯èƒ½æ˜¯ä¼æœ¨å·¥åœ¨å·¥ä½œï¼Œä½†è¿™ä¹ˆæ™šäº†è¿˜å·¥ä½œç¡®å®å¾ˆå¥‡æ€ªã€‚Eï¼šè€Œä¸”æˆ‘å¬è¯´å¼ ä¸‰æ˜¨å¤©ä¸¢äº†ä¸€æŠŠæ–§å¤´ï¼Œè¿™ä¼šä¸ä¼šæœ‰å…³ç³»ï¼Ÿ"
    
    context1 = {
        "new_dialogue_chunk": test_dialogue
    }
    
    prompt1 = get_prompt("intermediate", context1, model)
    total1 = len(system_prompt) + len(prompt1)
    
    print(f"\n--- åœºæ™¯1ï¼šåˆå§‹æ‘˜è¦ ---")
    print(f"User prompt: '{prompt1}' ({len(prompt1)} chars)")
    print(f"Total length: {total1} chars")
    print(f"âœ… Within 300 limit: {total1 <= 300}")
    
    # æµ‹è¯•åœºæ™¯2ï¼šæ›´æ–°æ‘˜è¦ï¼ˆ120å­—ç¬¦æ‘˜è¦ï¼‰
    previous_summary = "æ˜¨æ™šå¬åˆ°è§„å¾‹å£°éŸ³æ¥è‡ªæ£®æ—ï¼Œçœ‹åˆ°é»‘å½±åœ¨è€æ©¡æ ‘é™„è¿‘ç§»åŠ¨ï¼Œæ€€ç–‘æœ‰äººåšåäº‹ã€‚å¼ ä¸‰ä¸¢äº†æ–§å¤´å¯èƒ½æœ‰å…³ã€‚å¯èƒ½æ˜¯ä¼æœ¨å·¥ä½†æ—¶é—´ä¸æ­£å¸¸ã€‚éœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥èº«é«˜ä¸€ç±³å…«çš„å¯ç–‘äººå‘˜ã€‚"  # çº¦80å­—ç¬¦
    
    context2 = {
        "summary_so_far": previous_summary,
        "new_dialogue_chunk": "Fï¼šæˆ‘æƒ³èµ·æ¥äº†ï¼Œæ˜¨å¤©çœ‹åˆ°æå››å¾ˆæ™šè¿˜åœ¨å¤–é¢ã€‚Gï¼šæå››ï¼Ÿä»–ä¸æ˜¯ä¼æœ¨å·¥å•Šã€‚Hï¼šä½†æ˜¯ä»–èº«é«˜ç¡®å®æœ‰ä¸€ç±³å…«ã€‚Iï¼šè€Œä¸”ä»–æœ€è¿‘å’Œå¼ ä¸‰æœ‰çŸ›ç›¾ã€‚"
    }
    
    prompt2 = get_prompt("intermediate", context2, model)
    total2 = len(system_prompt) + len(prompt2)
    
    print(f"\n--- åœºæ™¯2ï¼šæ›´æ–°æ‘˜è¦ ---")
    print(f"Previous summary: '{previous_summary}' ({len(previous_summary)} chars)")
    print(f"User prompt: '{prompt2}' ({len(prompt2)} chars)")
    print(f"Total length: {total2} chars")
    print(f"âœ… Within 300 limit: {total2 <= 300}")
    print(f"âœ… Summary within 150 limit: {len(previous_summary) <= 150}")
    
    # æµ‹è¯•åœºæ™¯3ï¼šæœ€ç»ˆæ¨ç†
    final_summary = "ç»è¿‡å¤šè½®åˆ†æï¼šæ˜¨æ™šæ£®æ—ä¼ æ¥è§„å¾‹å£°éŸ³ï¼Œè€æ©¡æ ‘é™„è¿‘å‘ç°é»‘å½±ç§»åŠ¨ã€‚å¼ ä¸‰ä¸¢å¤±æ–§å¤´ï¼Œåœ°é¢æœ‰å¤§é´å­è„šå°ã€‚é»‘å½±èº«é«˜ä¸€ç±³å…«å·¦å³ã€‚æå››æ·±å¤œå¤–å‡ºï¼Œä¸å¼ ä¸‰æœ‰çŸ›ç›¾ï¼Œèº«é«˜ç¬¦åˆã€‚å·¥å…·ç®±æœ‰è¡€è¿¹ã€‚"  # çº¦90å­—ç¬¦
    
    context3 = {
        "summary_so_far": final_summary
    }
    
    prompt3 = get_prompt("final", context3, model)
    total3 = len(system_prompt) + len(prompt3)
    
    print(f"\n--- åœºæ™¯3ï¼šæœ€ç»ˆæ¨ç† ---")
    print(f"Final summary: '{final_summary}' ({len(final_summary)} chars)")
    print(f"User prompt: '{prompt3}' ({len(prompt3)} chars)")
    print(f"Total length: {total3} chars")
    print(f"âœ… Within 300 limit: {total3 <= 300}")
    print(f"âœ… Summary within 150 limit: {len(final_summary) <= 150}")
    
    # æ±‡æ€»æŠ¥å‘Š
    print(f"\n" + "="*50)
    print(f"ğŸ“‹ Atlas Model Minimal Prompts Test Summary")
    print(f"="*50)
    print(f"System prompt length: {len(system_prompt)} chars")
    print(f"Target total limit: 300 chars (reduced from 360)")
    print(f"Summary content limit: 150 chars")
    print(f"Segmentation: Every 2000 tokens")
    print(f"")
    print(f"Test results:")
    print(f"  åœºæ™¯1 (åˆå§‹æ‘˜è¦): {total1} chars - {'âœ… PASS' if total1 <= 300 else 'âŒ FAIL'}")
    print(f"  åœºæ™¯2 (æ›´æ–°æ‘˜è¦): {total2} chars - {'âœ… PASS' if total2 <= 300 else 'âŒ FAIL'}")
    print(f"  åœºæ™¯3 (æœ€ç»ˆæ¨ç†): {total3} chars - {'âœ… PASS' if total3 <= 300 else 'âŒ FAIL'}")
    
    all_pass = all([total1 <= 300, total2 <= 300, total3 <= 300])
    
    print(f"")
    print(f"Overall result: {'âœ… ALL TESTS PASS' if all_pass else 'âŒ SOME TESTS FAIL'}")
    
    if all_pass:
        print(f"ğŸ¯ Atlas model minimal prompts optimized!")
        print(f"âœ… æ€»æç¤ºè¯ â‰¤ 300å­—ç¬¦ï¼ˆé™ä½60å­—ç¬¦ï¼‰")
        print(f"âœ… æç®€åŒ–ç”¨æˆ·æç¤ºè¯æ ¼å¼")
        print(f"âœ… ä¿æŒ150å­—ç¬¦æ‘˜è¦é™åˆ¶")
        print(f"âœ… æ¯2000 tokensæ€»ç»“ä¸€æ¬¡")
        print(f"")
        print(f"ğŸ”§ Zero Response Mitigation:")
        print(f"â€¢ æ›´é«˜æ¸©åº¦å‚æ•° (0.5+)")
        print(f"â€¢ é™ä½é‡å¤æƒ©ç½š (1.02)")
        print(f"â€¢ å‡å°‘ä¸Šä¸‹æ–‡é•¿åº¦ (2048)")
        print(f"â€¢ éšæœºç§å­ (-1)")
        print(f"â€¢ æç®€æç¤ºè¯æ ¼å¼")
    else:
        print(f"âš ï¸ Some tests failed, need further optimization")

if __name__ == "__main__":
    test_minimal_prompts()
